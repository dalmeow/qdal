# Indexing categorical predictors {#sec-regression-index}

![](https://img.shields.io/badge/Area-R-green)

@sec-regression-cat showed you how to fit a regression model with a categorical predictor. We modelled reaction times as a function of word type (real or nonce). The categorical predictor `IsWord` (word type) was coded with so-called treatment contrasts using dummy coding: the model's intercept is the mean of the first level and the "slope" is the difference between the second level and the first.

Another way to include categorical predictors in a regression model is to use **indexing**: with indexing, separate regression coefficients are estimated for each level in the categorical predictor. This involves so-called "one-hot encoding" of categorical predictors into numbers and no contrasts (i.e. the coefficients do not represent differences between levels).

In this chapter we revisit the regression model from @sec-regression-cat using indexing.

## Indexing categorical predictors

When using treatment coding and contrasts, the model coefficients are set up this way:

$$
\begin{align}
RT_i & \sim Gaussian(\mu_i, \sigma)\\
\mu_i & = \beta_0 + \beta_1 \cdot w_i\\
\end{align}
$$

where $w$ is the indicator variable for `IsWord`. $\beta_0$ is the mean RT with real words and $\beta_1$ is the difference between the mean RT with nonce words and that with real words.

With indexing, the model mathematical specification is the following:

$$
\begin{align}
RT_i & \sim Gaussian(\mu_i, \sigma)\\
\mu_i & = \beta_1 \cdot W_{\text{T}[i]} + \beta_2 \cdot W_{\text{F}[i]}\\
\end{align}
$$

In this formula, $\text{W}_\text{T}$ and $\text{W}_\text{T}$ are indicator variables. The system these indicator variables use is known as "one-hot encoding": each level of the categorical predictor gets an indicator variable, which is $1$ if the level of the observation $i$ is the level encoded by that indicator variable, otherwise 0. @tbl-index shows the correspondence between levels of `IsWord` and the values of the indicator variables.

| `IsWord`       | $W_\text{T}$ | $W_\text{F}$ |
|----------------|:------------:|:------------:|
| IsWord = TRUE  |      1       |      0       |
| IsWord = FALSE |      0       |      1       |

: Indicator variables of the categorical predictor `IsWord` when using indexing instead of contrasts. {#tbl-index}

The regression formula has two coefficients to be estimated: $\beta_1$ and $\beta_2$. $\beta_1$ is the mean RT when `IsWord` is `TRUE` ($\mu_\text{T}$) and $\beta_2$ is the mean RT when `IsWord` is `FALSE` ($\mu_\text{F}$), as shown below by substituting the indicator variables with the values $0$ or $1$ depending on the level of `IsWord`.

$$
\begin{align}
\mu_\text{T} & = \beta_1 \cdot 1 + \beta_2 \cdot 0\\
& = \beta_1\\
\mu_\text{F} & = \beta_1 \cdot 0 + \beta_2 \cdot 1\\
& = \beta_2\\
\end{align}
$$

Since treatment contrasts are the default in R, the formula `RT ~ IsWord` gives us treatment contrasts. So how do you instruct R to use indexing instead?

The syntax for indexing is not particularly intuitive: `RT ~ 0 + IsWord`. Why `0 +`? That is the way to tell R to remove the intercept term: remember that the R formula includes a `1 +` by default (even when not explicitly written out) and that `1` is the constant or intercept term? Removing the intercept term changes the implied mathematical formula for $\mu$ from $\beta_0 + \beta_1 \cdot w_i$ to $\beta_1 \cdot W_{\text{T}[i]} + \beta_2 \cdot W_{\text{F}[i]}$.

## A model of RTs with indexing

Let's read the MALD data.

```{r}
#| label: mald
#| message: false

library(tidyverse)

mald <- readRDS("data/tucker2019/mald_1_1.rds")
```

Now let's fit a Gaussian regression model of RTs depending on the lexical status of the target word (real, `IsWord` = `TRUE`, or nonce, `IsWord` = `FALSE`). As usual, we set a seed and save the model output to a file for reproducibility.

```{r}
#| label: rt-idx
#| message: false

library(brms)

rt_idx <- brm(
  # Remove the intercept term with `0 +`
  RT ~ 0 + IsWord,
  family = gaussian,
  data = mald,
  seed = 6725,
  file = "cache/ch-regression-index-rt_idx"
)
```

Now we can inspect the model summary. Pay particular attention to the `Regression Coefficients`.

```{r}
#| label: rt-idx-summ

summary(rt_idx)
```

```{r}
#| label: rt-idx-ef
#| include: false

rt_idx_ef <- fixef(rt_idx)

```

The regression coefficients are also reported in @tbl-index.

```{r}
#| label: tbl-rt-idx-ef
#| tbl-cap: "Regression coefficients of the `rt_idx` model."
#| code-fold: true

rt_idx_ef |> knitr::kable(digits = 0)

```

-   `IsWordTRUE` is $\beta_1$, in other words $\mu_\text{T}$, or the mean RT with real words. According to our data and model, there is 95% probability that the mean RT with real words is between `{r} round(rt_idx_ef[1,3])` and `{r} round(rt_idx_ef[1,4])` ms.

-   `IsWordFALSE` is $\beta_2$, in other words $\mu_\text{F}$, or the mean RT with nonce words. According to the data and model, we can be 95% confident that the mean RT with nonce words is between `{r} round(rt_idx_ef[2,3])` and `{r} round(rt_idx_ef[2,4])` ms.

While in the model we fitted in @sec-regression-cat one of the regression coefficients gave us the *difference* in mean RT between the two word conditions, in the model we just fitted we don't get this information from the summary. Instead, you will have to calculate the difference between the two levels of `IsWord` by wrangling the MCMC draws of the model. This will be covered in @sec-regression-draws, so for now focus on understanding the concepts and application of treatment contrasts vs indexing.

## Why indexing?

You might be asking yourself: if I am interested in the difference between real and nonce words, why should I use indexing instead of treatment contrasts?

In the simple case of a model with one categorical predictor with two levels, it might seem a overhead to have to further process the model to get the difference of interest when with treatment contrasts the difference is directly estimated by the model. But in most cases you will have more than two levels and more than one predictor! In those models, it is not possible to get all of the logical "contrasts" be estimated by the model and you will still have to process the model's MCMC draws to get the comparisons you need. When the model estimates the means of different levels, it is more straightforward to process the draws. You will see examples in later chapters.

Another advantage of using indexing is related to specifying prior probability distributions. So far, we have used the default priors set by brms and we will continue to do so. However, in real research contexts it is important to think about priors and to specify custom priors, tailored for the specific context. It is much easier to think about prior probability distributions of means than of differences between means: with treatment contrasts, you would have to specify a prior probability distribution for the mean of the first level and a prior probability distribution for the difference between the second and first level.

The next chapter, @sec-regression-more, will illustrate models with one categorical predictor that has three levels. The chapter will show you both a model that uses treatment contrasts and the same model but with indexing of the categorical predictor. It is important to be familiar with both ways of including categorical predictors, but we will drop treatment contrasts after that and stick with indexing.

## Calculate posterior predictions from the draws

In @sec-regression-index, you learned how to use indexing of categorical predictors to estimate the mean of the outcome variable for each level in the categorical predictor. You might be wondering: with contrasts you get the mean of the reference level and the difference between the second level and the reference level; with indexing you get the mean at each level; however, all of these estimands (the mean of all levels and the difference between the levels) are important for inference. So, should you fit the model twice to obtain all estimands? Of course, that is not necessary because independent from the way you code categorical predictors, the model will have all the information needed to calculate all of the estimands of interest. This information is in the MCMC draws. In this section, you will learn how to obtain the estimated mean RT when the word is a nonce word from the model that used contrasts.

Let's revise the formula of the mean $\mu$ from the model formula above: $\mu_i = \beta_0 + \beta_1 \cdot w_i$. The mean depends on the value of $w$ (that's what the subscript $i$ is for). We can substitute $w_i$ with 0 for `IsWord` = `TRUE` and with 1 for `IsWord` = `FALSE`. So we get $\mu_T = \beta_0$ for `TRUE` and $\mu_F = \beta_0 + \beta_1$. Look again at `rt_bm_1_draws`. Recall that `b_Intercept` is $\beta_0$ and `b_IsWordFALSE` is $\beta_1$. Can you figure out what you need to do to get the posterior probability distribution of $\mu_F$ (i.e. the mean RT when `IsWord` is `FALSE`)? You *sum* the columns `b_Intercept` and `b_IsWordFALSE`! It goes without saying that the posterior probability distribution of the mean RT when `IsWord` is `TRUE` is the probability density of `b_Intercept`.

```{r}
#| label: hnr-bm-draws-draws

rt_bm_1_draws <- rt_bm_1_draws |> 
  mutate(
    # predicted RT for real words
    real = b_Intercept,
    # predicted HNR for polite attitude
    nonce = b_Intercept + b_IsWordFALSE
  )

rt_bm_1_draws |> select(real, nonce)
```

The sum operation for `nonce` is applied row-wise, to each draw (remember, each row in the draw tibble is the draw from one iteration of the MCMC chains). So, for `nonce`, the value of `b_Intercept` at draw 1 is added to the value of `b_IsWordFALSE` at draw 1, and so on. You end up with a list of sums that has the same length as the initial draws (here, 4000, i.e. 1000 per chain!). Then you can summarise and plot this new list as you did with the `b_` coefficients earlier. To make further processing of the draws easier, we can reshape the tibble so that instead of having two columns `real` and `nonce` with the drawn values in them, we end up with a tibble with one column `IsWord` (where we store the value `real` or `nonce`) and a column named `value` with the drawn value. Reshaping tibbles is called *pivoting* in tidyverse parlance. We won't go into the details of all pivoting operations, so I strongly recommend you to look through the [vignette on pivoting](https://tidyr.tidyverse.org/articles/pivot.html) (a vignette is a short tutorial that some R packages provide for users to learn basic operations with the functions in the package). The operation we need to get the shape we want is pivoting from a "wider" format to a "longer" format: this is achieved with `pivot_longer()` from tidyr. Make sure you read the section on `pivot_longer()` in the vignette to understand what the following code does.

```{r}
#| label: rt-bm-1-draws-long

rt_bm_1_draws_long <- rt_bm_1_draws |> 
  select(.chain, .iteration, .draw, real, nonce) |> 
  pivot_longer(real:nonce, names_to = "IsWord", values_to = "value")

rt_bm_1_draws_long
```

Check the resulting `rt_bm_1_draws_long` tibble. The first two rows are draw 1 of chain 1: one row is for the case when the word is a real word, and the other row is for the case when the word is a nonce word. There are a total of 8000 rows: 4000 draws (1000 draws times 4 chains) times 2 (two levels of `IsWord`). Now let's make a violin plot of the predicted HNR in informal and polite attitude!

```{r}
#| label: fig-rt-bm-pred-dens
#| fig-cap: "Density plot of predicted mean RT for real and nonce words."
#| echo: false

rt_bm_1_draws_long |> 
  ggplot(aes(IsWord, value)) +
  geom_violin(width = 0.2) +
  labs(
    x = "Word type", y = "Predicted RT (ms)"
  )
```

We can use the ggdist package to plot something a bit fancier, for example a "half eye" geometry (which can be obtained with the "halfeye" statistic, `stat_halfeye()`). Check the `?stat_halfeye` documentation to learn about it.

```{r}
#| label: fig-rt-bm-pred-halfeye
#| fig-cap: "Half-eye plot of predicted mean RT for real and nonce words."

library(ggdist)

rt_bm_1_draws_long |> 
  ggplot(aes(value, IsWord)) +
  stat_halfeye() +
  labs(x = "Predicted RT (ms)", y = "Word type")
```

Another useful ggplot2 statistic is `stat_interval()`.

```{r}
#| label: fig-rt-bm-pred-interval
#| fig-cap: "Credible Intervals of predicted mean RT for real and nonce words."

rt_bm_1_draws_long |> 
  ggplot(aes(IsWord, value)) +
  stat_interval() +
  labs(
    x = "Word type", y = "Predicted RT (ms)"
  )
```

When reporting the results, it is recommended to report the CrIs of the predicted outcome for each level in the categorical predictor. It is useful to include tables like @tbl-rt-cris. To make the generation of the table more straightforward, we can use a couple of tricks when creating the `rt_bm_cris` below. The `glue()` function from the tidyverse package [glue](https://glue.tidyverse.org) allows you to specify strings based on the output of R code computation. Code is included between curly braces `{}`.

```{r}
#| label: rt-bm-1-pred-summ

library(glue)

rt_bm_1_cris <- rt_bm_1_draws_long |> 
  mutate(IsWord = factor(IsWord, levels = c("real", "nonce"))) |> 
  group_by(IsWord) |> 
  summarise(
    mean = mean(value), sd = sd(value),
    ci_90 = glue("[{round(quantile2(value, 0.05))}, {round(quantile2(value, 0.95))}]"),
    ci_80 = glue("[{round(quantile2(value, 0.1))}, {round(quantile2(value, 0.9))}]"),
    ci_60 = glue("[{round(quantile2(value, 0.2))}, {round(quantile2(value, 0.8))}]"),
  )
  
```

```{r}
#| label: tbl-rt-cris
#| tbl-cap: "Summary measures of predicted RTs by word type (in milliseconds)."
#| 
knitr::kable(rt_bm_1_cris, col.names = c("Type", "Mean RT", "SD", "90% CrI", "80% CrI", "60% CrI"), digits = 0)
  
```

In the main text, you could report the results like so:

> According to the model, the predicted RTs when the word is real word is between 1059 and 1080 ms at 90% confidence ($\beta$ = 1069, SD = 6). When the word is a nonce word, the predicted RTs are between 943 and 963 ms ($\beta$ = 953, SD = 6).
