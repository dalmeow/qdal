# Research cycle

![](https://img.shields.io/badge/Area-Research_methods-blue)

In @sec-res-meth, you learned about the research process, which includes the research context, data acquisition, data analysis and communication. A different perspective on the research process that highlights the temporal succession of the process steps is the [research cycle]{.smallcaps}, represented in an idealised form in @fig-res-cycle.

![The research cycle](img/res-process-cycle.png){#fig-res-cycle fig-align="center" width="500"}

The cycle starts with the development of **research questions and hypotheses**. This step involves a thorough literature review and the identification of the topic, research problem, goal, questions and, possibly, hypotheses (as described in @sec-res-context). Once the research questions and hypotheses have been determined, the researcher proceeds with the **design of the study** which sets out to answer the research questions and assess the research hypotheses. The study design process includes determining a large number of interconnected aspects, like materials, procedures, data management and data analysis plans, target population, sampling method and so on. At times the study design process reveals shortcomings or unforeseen aspects of the research questions/hypotheses which can be updated accordingly.

Once the study design has been finalised, one proceeds with **acquiring data** based on the protocols detailed in their plan. After the completion of data acquisition, researchers **analyse data** and **interpret the results** in light of the research questions and hypotheses. Finally, the outcomes of the study are **published** in some form and the **next study cycle** begins once again.

This sounds all very reasonable, but in reality, the researchers' practice is quite different. This chapter introduces the concept of "researcher's degrees of freedom" and describes the so-called Questionable Research Practices (QRPs). We will review literature that shows the grim reality of how common QRPs are. In @sec-open-research, you will learn about principles and tools that are designed to help minimise the presence and impact of QRPs in one's own research.

## Researcher's degrees of freedom

::: {.callout-important appearance="simple"}
This section is reproduced from @coretta2023 (CC-BY-NC) with minor edits.
:::

Data analysis involves many decisions, such as how to operationalise and measure a given phenomenon or behaviour, which data to submit to statistical modelling and which to exclude in the final analysis, or which inferential approach to employ. This "freedom" can be problematic because humans show cognitive biases that can lead to erroneous inferences [@tversky1974]. For example, humans are prone to see coherent patterns even in the absence of them [@brugger2001], convince themselves of the validity of prior expectations by cherry-picking evidence (aka confirmation bias, "I knew it"; @nickerson1998), and perceive events as being plausible in hindsight ("I knew it all along"; @fischhoff1975). In conjunction with an academic incentive system that rewards certain discovery processes more than others [@koole2012; @sterling1959], we often find ourselves exploring many possible analytic pathways but reporting only a selected few depending on the quality of the narrative that we can achieve with them.

This issue is particularly amplified in fields in which the raw data lend themselves to many possible ways of being measured [@roettger2019]. Combined with a wide variety of conceptual and methodological traditions as well as varying levels of quantitative training across sub-fields, the inherent flexibility of data analysis might lead to a vast plurality of analytic approaches that can itself lead to different scientific conclusions [@roettger2019a]. Analytic flexibility has been widely discussed from a conceptual point of view [@nosek2014; @simmons2011; @wagenmakers2012] and in regard to its application in individual scientific fields (e.g., @charles2019; @roettger2019a; @wicherts2016). This notwithstanding, there are still many unknowns regarding the extent of analytic plurality in practice.

Consequently, a substantial body of published articles likely present overconfident interpretations of data and statistical results based on idiosyncratic analytic strategies (e.g., @gelman2014a; @simmons2011). These interpretations, and the conclusions that derive from them, are thus associated with an unknown degree of uncertainty (dependent on the strength of evidence provided) and with an unknown degree of generalizability (dependent on the chosen analysis). Moreover, the same data could lead to very different conclusions depending on the analytic path taken by the researcher. However, instead of being critically evaluated, scientific results often remain unchallenged in the publication record.

## Questionable Research Practices

::: {.callout-important appearance="simple"}
This section contains text from @coretta2020b (CC-BY-NC) .
:::

![The research cycle and questionable research practices](img/res-process-cycle-qrps.png){#fig-res-cycle-qrps fig-align="center" width="500"}

[Questionable research practices]{.smallcaps} are practices, whether intentional or not, that undermine the robustness of research [@simmons2011; @morin2015; @flake2020]. Questionable research practices are practices that negatively affect the research enterprise, but that are employed (most of the time unintentionally) by a surprisingly high number of researchers [@john2012]. For each step in the research cycle, questionable practices are available to researchers. These are part of the researcher's degrees of freedom, introduced in the previous section. In this section, we will briefly review some of the most common questionable research practices identified in the literature.

@makel2012 looked at the publication history of 100 psychological journals since 1900. They found that only 1.07% of the papers (that is, 1 in 100 papers) were replications of previous studies. This means that the vast majority of studies are run only once and the field moves on. As @tukey1969 [p. 84] said, "Confirmation comes from repetition. Any attempt to avoid this statement leads to failure and more probably to destruction". This lack of replication attempts is problematic, given than we can't be certain the results obtain from the one study would replicate if the study is run again. While the study in @makel2012 focused on psychology, @kobrock2023 find that linguistics shows a more dire situation: only 0.08% of experimental articles contains an independent direct replication (1 in 1250).

Another issue that affects modern research regards study design, including aspects related to sample size. Several studies have found that most research employs study designs that grant a 50% probability of being able to find effects of medium size [@cohen1962; @sedlmeier1992; @bezeau2001]. @gaeta2020 find a similar scenario in speech, language and hearing research: the majority of studies they screened did not have an adequate sample size to be able to detect medium-sized effects.
