# Regression models

![](https://img.shields.io/badge/Area-R-green)

In @sec-reg-intro you were introduced to regression models. Regression is a statistical model based on the equation of a straight line, with added error.

$$
y = \beta_0 + \beta_1 \cdot x + \epsilon
$$

$\beta_0$ is the regression line's intercept and $\beta_1$ is the slope of the line. We have seen that $\epsilon$ is assumed to be from a Gaussian distribution with mean 0 and standard deviation $\sigma$.

$$
\begin{align}
y & \sim Gaussian(\mu, \sigma)\\
\mu & = \beta_0 + \beta_1 \cdot x\\
\end{align}
$$

From now on, we will use the latter way of expressing regression models, because it makes it clear which distribution we assume the variable $y$ to be generated by (here, a Gaussian distribution). Note that in the wild, variables very rarely are generated by Gaussian distributions. It is just pedagogically convenient to start with Gaussian regression models (i.e. regression models with a Gaussian distribution as the distribution of the outcome variable $y$) because the parameters of the Gaussian distribution, $\mu$ and $\sigma$ can be interpreted straightforwardly on the same scale as the outcome variable $y$: so for example if $y$ is in centimetres, then the mean and standard deviation are in centimetres, if $y$ is in Hz, then the mean and SD are in Hz, and so on. Similarly, the regression $\beta$ coefficients will be on the same scale as the outcome variable $y$. You will be introduced later to regression models with distributions other than the Gaussian, where the regression parameters are estimated on a different scale than that of the outcome variable $y$.

The goal of the Gaussian regression model expressed in the formulae above is to estimate $\beta_0$, $\beta_1$ and $\sigma$ from observed data. Now, since truly Gaussian data is difficult to come by, especially in linguistics, for the sake of pedagogical simplicity we will start the learning journey on fitting regression models using data for which a Gaussian regression is generally not appropriate. You will learn in later chapters more appropriate distribution families for this data.

## Vowel duration in Italian: the data

We will analyse the duration of vowels in Italian from @coretta2019k and how speech rate affects vowel duration. The expectation we might have is that vowels get shorter with increasing speech rate. You will notice how this is a very vague hypothesis: how shorter do they get? Is the shortening the same across all speech rates, or does it get weaker with higher speech rates? Our expectation/hypothesis simply states that vowels get shorter with increasing speech rate. Maybe we could do better and use what we know from speech production and come up with something more precise, but this type of vague hypothesis are very common, if not standard, in language research, so we will stick to it for practical and pedagogical reasons. Remember, however, that robust research should strive for precision.

Let's load the R data file `coretta2018a/ita_egg.rda`. It contains several phonetic measurements obtained from audio and electroglottographic recordings. You can find the information on the data here: [Electroglottographic data on Italian](https://uoelel.github.io/qml-data/data/coretta2018a/ita_egg.html).

```{r}
#| label: ita-egg

library(tidyverse)
load("data/coretta2018a/ita_egg.rda")

ita_egg
```

Let's plot vowel duration and speech rate in a scatter plot. The relevant columns in the tibble are `v1_duration` and `speech_rate`. TODO: check that geom_smooth was done earlier. @fig-vow-rate shows speech rate on the *x*-axis and vowel duration on the *y*-axis. The points in the plot are the individual observations (measurements) of vowels in the 19 speakers of Italian. The plot also includes a regression line, generated by `geom_smooth(method = "lm")`. By glancing at the individual points, we can see a negative relationship between speech rate and vowel duration: vowels get shorter with greater speech rate. This is reflected by the regression line, which has a negative slope.

```{r}
#| label: fig-vow-rate
#| fig-cap: "Relationship between speech rate (as number of syllables per second) and vowel duration (in milliseconds) in 19 Italian speakers."

ita_egg |> 
  ggplot(aes(speech_rate, v1_duration)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(
    x = "Speech rate (syllables per second)",
    y = "Vowel duration (ms)"
  )
```

You might have noticed a warning about missing values. This is because some observations of vowel duration (`v1_duration`) in the data are missing (i.e. they are `NA`, "Not Available"). Let's drop them from the tibble with `drop_na()`. We will use `ita_egg_clean` for the rest of the tutorial.

```{r}
#| label: ita-egg-clean

ita_egg_clean <- ita_egg |> 
  drop_na(v1_duration)
```

### The model

Let's move on onto fitting a Gaussian regression model to vowel duration as the outcome variable and speech rate as the predictor. We are assuming that vowel duration follows a Gaussian distribution (although as mentioned above this is not the case, but it will do for now). Here is the model we will fit in mathematical notation.

$$
\begin{align}
\text{vdur} & \sim Gaussian(\mu, \sigma)\\
\mu & = \beta_0 + \beta_1 \cdot \text{sr}\\
\end{align}
$$

-   Vowel duration ($\text{vdur}$) is distributed ($\sim$) according to a Gaussian distribution ($Gaussian(\mu, \sigma)$).

-   The mean $\mu$ is equal to the sum of $\beta_0$ (the intercept) and the product of $\beta_1$ and speech rate ($\beta_1 \cdot \text{sr}$). The formula of $\mu$ is the equation of a straight line, aka the *linear equation*. This is why regression models are also called linear models.

The regression model estimates the parameters in the mathematical formulae: parameters to be estimated in regression models are usually represented with Greek letters (hence why we adopted this notation for the linear equation). Since $\mu$ is the sum of terms with parameters, the model estimates those parameters directly. So in total, the regression model represented in the formulae above has to estimate the following three parameters:

-   The *regression coefficients* $\beta_0$ and $\beta_1$.

-   The standard deviation of the Gaussian distribution, $\sigma$.

Before that though, create a folder called `cache/` in the `data/` folder of the RStudio project of the course. We will use this folder to save the output of model fitting so that you don't have to refit the model every time. This is useful because as models get more and more complex, they can take quite a while to fit.

```{r}
#| label: vow-bm

library(brms)

vow_bm <- brm(
  v1_duration ~ speech_rate,
  family = gaussian,
  data = ita_egg_clean,
  cores = 4,
  seed = 20912,
  file = "cache/vow_bm"
)
```

The model will be fitted and saved in `cache/` with the file name `vow_bm.rds`. If you now re-run the same code again, you will notice that `brm()` does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works! Check the contents of `cache/` to see for yourself.).

::: callout-warning
When you save the model fit to a file, R does not keep track of changes in the model specification, so if you make changes to the formula or data, you need to **delete the saved model file** before re-running the code for the changes to have effect!
:::

## Interpret the model summary

To obtain a summary of the model, use the `summary()` function.

```{r}
#| label: vow-bm-summary

summary(vow_bm)
```

Let's focus on the "Regression Coefficients" table of the summary. To understand what they are, just remember the equation of line and the model formula above.

-   `Intercept` is $\beta_0$: this is the mean vowel duration, **when speech rate is 0**.

-   `speech_rate` is $\beta_1$: this is the change in vowel duration **for each unit increase of speech rate**.

This should make sense, if you understand the equation of a line: $y = \beta_0 + \beta_1 x$. If you are still uncertain, play around with the [web app](https://stefanocoretta.shinyapps.io/lines/).

Recall that the `Estimate` and `Est.Error` column are simply the **mean and standard deviation of the posterior probability distributions** of the estimate of `Intercept` and `speech_rate` respectively.

Looking at the 95% Credible Intervals (CrIs), we can say that based on the model and data:

-   The mean vowel duration, when speech rate is 0 syl/s, is between 192 and 205 ms, at 95% confidence.

-   We can be 95% confident that, for each unit increase of speech rate (i.e. for each increase of one syllable per second), the duration of the vowel decreases by 20.5-23 ms.

To see what the posterior probability densities of `\beta_0`, `\beta_1` and `\sigma` look like, you can quickly plot them with the `plot()` function.

```{r}
#| label: vow-bm-plot

plot(vow_bm)
```

If you prefer to see density plots instead of histograms, you can specify the `combo` argument.

```{r}
#| label: vow-bm-plot-2
plot(vow_bm, combo = c("dens", "trace"))
```

## Plot the model predictions

You should always also plot the model predictions, i.e. the predicted values of vowel duration based on the model predictors (here just `speech_rate`).

You will learn more advanced methods later on, but for now you can use `conditional_effects()` from the brms package.

```{r}
#| label: vow-bm-cond

conditional_effects(vow_bm, effects = "speech_rate")
```

If you wish to include the raw data in the plot, you can wrap `conditional_effects()` in `plot()` and specify `points = TRUE`. Any argument that needs to be passed to `geom_point()` (these are all ggplot2 plots!) can be specified in a list as the argument `point_args`. Here we are making the points transparent.

```{r}
#| label: vow-bm-cond-1

plot(conditional_effects(vow_bm, effects = "speech_rate"), points = TRUE, point_args = list(alpha = 0.1))
```

## Simulating Gaussian data

To make things a little bit more worldly, we will simulate data of human adult weight and height. We assume that height is distributed according to a Gaussian distribution with $\mu = 165$ cm and $\sigma = 8$. Based on height, we simulate weight as $0.4 * height$ plus Gaussian error with mean 0 and standard deviation 2.

$$
\begin{align}
w & \sim Gaussian(\mu, \sigma)\\
\mu & = 0.4 * h\\
\sigma & = 2
\end{align}
$$

```{r}
#| label: weight
#| message: false

library(tidyverse)

set.seed(62854)

h <- round(rnorm(200, 165, 8))
w <- 0.4 * h + rnorm(200, 0, 2)

weight <- tibble(h, w)
```

The code uses the `rnorm()` function which generates a random sample of values from a Gaussian distribution. The function takes three arguments:

-   The number of values to sample, here 200.
-   The mean of the Gaussian distribution, here 165.
-   The standard deviation of the Gaussian distribution, here 8.

::: callout-note
The name of the `rnorm()` function is composed of:

-   `r` for random.
-   `norm` for "normal", the Gaussian distribution.
:::

We use the `round()` function to round the values generated by `rnorm()` to the nearest integer.

The following plot shows the density curve of the simulated height data. The purple vertical line is the mean.

```{r}
#| label: height-plot

weight |> 
  ggplot(aes(h)) +
  geom_density(fill = "darkgreen", alpha = 0.2) +
  geom_rug() +
  geom_vline(aes(xintercept = mean(h)), colour = "purple", linewidth = 1)
```

## Regression models in R

R offers several options for fitting regression models. In this course you will be introduced to a more modern and robust approach to regression modelling using the R package [brms](http://paulbuerkner.com/brms/).

brms fits Bayesian regression models and it is a very flexible package that allows you to model a lot of different types of variables. You don't really need to understand a lot of technical details to be able to effectively use the package and interpret the results, so in the course we will focus on how to use the package in the context of research, but if you are interested in the inner workings, feel free to read the documentation.

One useful thing to know is that brms is a bridge between R and the statistical programming software [Stan](https://mc-stan.org). Stan is a powerful piece of software that can fit any type of Bayesian models, not only regression models. What brms does is that it allows you to write regression models in R, which are translated into Stan models and run with Stan under the hood.

You can safely use brms without learning Stan, but if you are interested in the computational aspects of Stan, you can check the [Stan documentation](https://mc-stan.org).

The following sections will guide you through the steps to fit, interpret and plot your first Bayesian regression model!

## Fitting a regression model with brms

The first thing to do is of course to attach the brms package.

```{r}
#| label: brms
library(brms)
```

You can then fit a regression model with the `brm()` function, short for Bayesian Regression Model.

The minimal arguments you need are a model formula, a distribution family, and the data you want to fit the formula to.

-   `h ~ 1` and `family = gaussian` simply tell brms to model `h` with a Gaussian distribution. This means that a mean and a standard deviation will be estimated from the data.
-   We specify the data with `data = height`.

This model correspond to the following mathematical formula:

$$
\text{height} \sim Gaussian(\mu, \sigma)
$$

The last argument you see in the code below, `chain = 1`, is related to the algorithm that Stan uses to fit the model and obtain estimates for the model's parameters (in this model, these are the mean and the standard deviation of height). Stan uses the Markov Chain Monte Carlo (MCMC) algorithm. The algorithm is run by default four times; in technical terms, *four MCMC chains* are run. For this example we will just run one chain to speed things up, but in normal circumstance you would want to run four (you will learn how to run them in parallel in later tutorials).

If you want to learn more about the MCMC algorithm, check [Finding posteriors through sampling](https://elizabethpankratz.github.io/bayes_stat/day1/mcmc.html) by Elizabeth Pankratz and the resources linked there.

Now, here is the code to fit this model with brms. When you run the code, text will be printed in the R Console or below the code if you are using a Quarto document. You can find it below the code here too. The messages in the text are related to Stan and the MCMC algorithm. `Compiling Stan program...` tells you that brms is instructing Stan to compile the model specified in R, and `Start sampling` tells us that the MCMC algorith has started. The rest of the messages are about the MCMC chains themselves. Since we are only running one chain, you see info for that chain only.

```{r}
#| label: h-1
h_1 <- brm(
  h ~ 1,
  family = gaussian,
  data = height,
  chain = 1
)
```

Fantastic! When the model has finished running, `h_1` will have the model output for you to inspect.

## Model summary

The natural way to inspect a model object like `h_1`, is to use the `summary()` function, which prints the model summary.

Let's inspect the summary of the model now.

```{r}
#| label: h-1-summary
summary(h_1)
```

Let's break that down bit by bit:

-   The first few lines are a reminder of the model we fitted:

    -   `Family` is the chosen distribution for the outcome variable, a Gaussian distribution.

    -   `Links` lists the link functions.

    -   `Formula` is the model formula.

    -   `Data` reports the name of the data and the number of observations.

    -   Finally, `Draws` has information about the MCMC draws.

-   Then, the `Regression Coefficients` are listed as a table. In this model we only have one regression coefficient, `Intercept` which corresponds to $\mu$ from the formula above. You will learn more about the `Regression Coefficients` table below.

-   Then `Further distributional parameters` are tabled. Here we only have `sigma` which is $\sigma$ from the formula above. The table has the same columns as the `Regression Coefficients` table.

## Posterior probability distributions

The `Regression Coefficients` table is the most important part of the model summary, the part that gives you information on the estimates of the model parameters.

The main characteristics of Bayesian regressions is that they don't just provide you with a single numeric estimate for the model parameters. Rather, the model estimates a *full probability distribution* for each parameter/coefficient. These probability distributions are called **posterior probability distributions** (or posteriors for short).

They are called *posterior* because they are derived from the data (and the prior probability distributions, you will learn more about these later).

The `Regression Coefficients` table reports a few summary measures of these posterior distributions. Here, we only have the summary measures of one posterior: the posterior of the model's mean $\mu$. The table also has three diagnostic measures, which you can ignore for now.

Here's a break down of the table's columns:

-   `Estimate`, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient).

-   `Est.error`, the error of the mean estimate (i.e. the standard deviation of the posterior distribution of the coefficient).

-   `l-95% CI` and `u-95% CI`, the lower and upper limits of the 95% Bayesian Credible Interval (more on these below).

-   `Rhat`, `Bulk_ESS`, `Tail_ESS` are diagnostics of the MCMC chains.

## Plotting the posterior distributions

While the model summary reports *summaries* of the posterior distributions, it is always helpful to *plot* the posteriors.

```{r}
#| label: h-1-plot
plot(h_1, combo = c("dens", "trace"))
```

The plot above shows the posterior distribution of the estimated coefficient `b_Intercept` and `sigma`, which correspond to the $\mu$ and $\sigma$ of the formula above, respectively.

For the `b_Intercept` coefficient, the posterior probability encompasses values between 163 and 167 cm, approximately. But some values are more probable then others: the values in the centre of the distribution have a higher probability density then the values on the sides. In other words, values around 165 cm are more probable than values below 164 and above 166, for example.

However, looking at a full probability distribution like that is not super straightforward. Credible Intervals (CrIs) help summarise the posterior distributions so that interpretation is more straightforward.

## Interpreting Credible Intervals

The model summary reports the Bayesian Credible Intervals (CrIs) of the posterior distributions.

Another way of obtaining summaries of the coefficients is to use the `posterior_summary()` function. Ignore the last three lines.

```{r}
#| label: h1-post-summary
posterior_summary(h_1)
```

The 95% CrI of the `b_Intercept` is between `{r} round(brms::fixef(h_1)[3])` and `{r} round(brms::fixef(h_1)[4])`. This means there is a 95% probability, or that we can be 95% confident, that the `Intercept` value is within that range.

For `sigma`, the 95% CrI is between `{r} round(brms::posterior_summary(h_1)["sigma", "Q2.5"], 1)` and `{r} round(brms::posterior_summary(h_1)["sigma", "Q97.5"], 1)`. So, again, there is a 95% probability that the `sigma` value is between those values.

So, to summarise, **a 95% CrI tells us that we can be 95% confident, or in other words that there is a 95% probability, that the value of the coefficient is between the values of the CrI.**

Here you have the results from the regression model. Really, the results of the model are the full posterior probabilitis, but it makes things easier to focus on the CrI.

The model has "recovered" the values we used to simulate the height data quite well: above, we used a mean of 165 and a standard deviation of 8 to simulate data. The model is suggesting that the mean and SD of `h` is between 164-166 cm and 7-9 cm respectively: quite neat!

Regression models work.

## Reporting

What about reporting the model in writing? We could report the model and the results like this.[^ch-regression-1]

[^ch-regression-1]: To know how to add a citation for any R package, simply run `citation("package")` in the R Console, where `"package"` is the package name between double quotes.

::: callout-tip
We fitted a Bayesian regression using the brms package (Bürkner 2017) in R (R Core Team 2024). The outcome variable was height and we did not include any predictor, to estimate the overall mean and standard deviation of height.

Based on the model results, there is a 95% probability that the mean is between `{r} round(brms::fixef(h_1)[3])` and `{r} round(brms::fixef(h_1)[4])` cm and that the standard deviation is between `{r} round(brms::posterior_summary(h_1)["sigma", "Q2.5"], 1)` and `{r} round(brms::posterior_summary(h_1)["sigma", "Q97.5"], 1)` cm.
:::

## What's next

In this post you have learnt the very basics of Bayesian regression models. As mentioned above, regression models with brms are very flexible and you can easily fit very complex models with a variety of distribution families (for a list, see `?brmsfamily`; you can even define your own distributions!).

The perk of using brms is that you can just learn the basics of one package and one approach and use it to fit a large variety of regression models.

This is different from the standard frequentis approach, where different models require different packages or functions, with their different syntax and quirks.

In the folllowing posts, you will build your understanding of Bayesian regression models, which will enable you to approach even the most complex models! However, due to time limits you won't learn everything there is to learn.

Developing conceptual and practical skills in quantitative methods is a long-term process and unfortunately one semester will not be enough. So be prepared to continue your learning journey for years to come!
