[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Data Analysis for Linguists in R",
    "section": "",
    "text": "Welcome!\nHello! Welcome to the Quantitative Data Analysis for Linguists in R textbook! This is the textbook for the Quantitative Methods in Linguistics and English Language course at the University of Edinburgh, but the textbook is open to all. Please, read the Preface to familiarise yourself with the pedagogical background and structure of the book.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Audience\nThis textbook is specifically designed for the students taking the Quantitative Methods in Linguistics and English Language (QML) course at the University of Edinburgh (UoE), but you don’t have to be enrolled in the course to work through it. This book is an introduction to quantitative methods and statistics in R for absolute beginners in the field of linguistics. No prior familiarity with quantitative methods, statistics or knowledge of R is expected, just a sense of adventure! Of course, the book can be helpful to researchers who have experience with statistics but want to learn about a more modern approach to statistical analysis like Bayesian statistics. Independent of your background, you will be exposed to several aspects of quantitative data analysis and R skills that can be applied and extended to many cases of data analysis in linguistics and related fields.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#justification-and-pedagogical-background",
    "href": "preface.html#justification-and-pedagogical-background",
    "title": "Preface",
    "section": "Justification and pedagogical background",
    "text": "Justification and pedagogical background\nThis book is a response to the need for a structured textbook that can fit into a one semester course and yet cover enough materials for an absolute beginner to be able to complete at least basic quantitative analyses. While there are many good books out there, they tend to focus on one aspect or the other, rather than covering all the necessary topics without assuming some prior knowledge. Examples of excellent textbooks are R for Data Science (R4DS, Wickham, Çetinkaya-Rundel, and Grolemund 2023) which covers the basics of R and data processing and visualisation; Statistics for linguists: an introduction using R (Winter 2020), Statistical rethinking: a Bayesian course with examples in R and Stan (McElreath 2020), and Introduction to Bayesian Data Analysis for Cognitive Science (Nicenboim, Schad, and Vasishth 2025), among many others, that focus on statistics with R. In fact, this textbook has taken a lot of inspiration from those books, and I am forever grateful to the authors for their fantastic work.\nHowever, the QML course in the Linguistics and English Language department at UoE is the only quantitative methods course in the department and the majority of students start as absolute beginners. The course must cover basic principles of research methods, some aspects of the philosophy of “science”, statistical concepts, and practical skills in R to run appropriate quantitative analyses. It is a lot to cover and with 9 weeks of teaching available, only the surface can be scratched, but scratched enough that by the end of the course you will feel comfortable taking a further step outside your comfort zone. So this textbook is not in any way meant to be exhaustive and it lives within the constraints of the specifics of the course it is intended to serve. However, where relevant, pointers to other resources will be given so that each reader can choose to focus on some aspects over others.\nAnother important point about this book is that, like the textbooks mentioned above, it moves away from the “traditional” (perhaps old fashion) way of doing statistics and instead it adopts a fresh take on quantitative data analysis which some have called the “New Statistics” (Cumming 2013; Kruschke and Liddell 2018). All of you view will be familiar with research papers in linguistics (whether you read them for a class or as part of your job as a researcher) and you will surely have encountered the (in)famous p-value and statements like “statistically significant”. These concepts belong to a particular way of doing statistics, called frequentist statistics, which has become ritualised into a set of cookbook recipes that we started to blindly follow (the “Null Ritual,” Gigerenzer 2004, 2018; Gigerenzer, Krauss, and Vitouch 2004). While (good) frequentist statistics is not bad in itself, the so-called Null Ritual has done a lot of damage, as you will learn in later chapters of this book. Because of these and other reasons, this book adopts a Bayesian approach to statistics, where instead of chasing after “significant p-values” we focus on a robust estimation of effects and patterns in the data. This is a bit of an oversimplification, but it should give you enough sense for where the textbook comes from. From a practical perspective, Bayesian statistics just works, even in those cases where the traditional way of doing statistics fails for one reason or the other. By learning a few building blocks of Bayesian statistics, you will be able to extend your skills to develop expertise in more advanced techniques, all within a coherent framework. You will of course learn about p-values and how (not) to interpret them, since a lot of current research is still carried out under the Null Ritualistic approach.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#book-structure",
    "href": "preface.html#book-structure",
    "title": "Preface",
    "section": "Book structure",
    "text": "Book structure\nThe book is structured according to the schedule of the QML course. Chapters are divided into “weeks” and the course will cover those topics weekly. If you are reading this book without being enrolled in the course, you are free to go through the chapters at your own pace! Not however that the chapters are written so that there is a certain progression of topics, and later chapters build on previous ones, so I recommend to start from the first chapter and if need be maybe read through the first chapters quickly if they cover things you are already familiar with, and start reading more intently when you hit a chapter that covers something new.\nEach chapter has “badges” that indicate the major topic area of the chapter. While some chapters focus on a specific area, others focus on more than one. These are the badges:\n\n is for chapters on research methods more generally, including research practices and philosophy.\n is for chapters that introduce statistical concepts without going necessarily into the details of how to do that in R.\n is for chapters that teach you how to use R to complete a particular task, like reading or plotting data, using statistical models or transform data.\n\nThe book also uses different types of “call-out boxes” to present specific content. Here are examples.\n\n\n\n\n\n\nTipDefinition or hint\n\n\n\nA green box contains definitions or hints to solve exercises.\n\n\n\n\n\n\n\n\nWarningExercise or activity\n\n\n\nOrange boxes are for exercises or more general activities.\n\n\n\n\n\n\n\n\nNoteQuiz, examples and summaries\n\n\n\nBlue boxes contain quizzes, examples or summaries. The title in the box will specify what type of content.\n\n\n\n\n\n\n\n\nImportantR Note, Spotlight or solutions\n\n\n\nRed boxes called “R Note” explain something about R or contain R tips that don’t quite fit in the main text. Spotlight” boxes focus on statistical concepts, historical context or philosophy. Red boxes called “Solutions” are solutions to the exercises.\n\n\nThe textbook will teach how to use R. R is a programming language, meaning that you will have to write code which is executed and the results are returned to (as output in the R Console, as plots, tables, so on). R code in-text will look like this: \"this is R code\", while longer code chunks will look like this:\n# Sum two numbers!\n\na &lt;- 1 + 2\nprint(a)\nSometimes, when the R code is not that important, for example for certain plots, you will see a little grey triangle next to Code and if you click on it the code will be shown, like in the following example.\n\n\nCode\nlibrary(tidyverse)\nlibrary(glue)\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nrt_mean &lt;- mean(mald$RT)\nrt_sd &lt;- sd(mald$RT)\nrt_mean_text &lt;- glue(\"mean: {round(rt_mean)} ms\")\nrt_sd_text &lt;- glue(\"SD: {round(rt_sd)} ms\")\nx_int &lt;- 2000\n\nggplot(data = tibble(x = 0:300), aes(x)) +\n  geom_density(data = mald, aes(RT), colour = \"grey\", fill = \"grey\", alpha = 0.2) +\n  stat_function(fun = dnorm, n = 101, args = list(rt_mean, rt_sd), colour = \"#9970ab\", linewidth = 1.5) +\n  scale_x_continuous(n.breaks = 5) +\n  geom_vline(xintercept = rt_mean, colour = \"#1b7837\", linewidth = 1) +\n  geom_rug(data = mald, aes(RT), alpha = 0.1) +\n  annotate(\n    \"label\", x = rt_mean + 1, y = 0.0015,\n    label = rt_mean_text,\n    fill = \"#1b7837\", colour = \"white\"\n  ) +\n  annotate(\n    \"label\", x = x_int, y = 0.0015,\n    label = rt_sd_text,\n    fill = \"#8c510a\", colour = \"white\"\n  ) +\n  annotate(\n    \"label\", x = x_int, y = 0.001,\n    label = \"theoretical sample\\ndistribution\",\n    fill = \"#9970ab\", colour = \"white\"\n  ) +\n  annotate(\n    \"label\", x = x_int, y = 0.0003,\n    label = \"empirical sample\\ndistribution\",\n    fill = \"grey\", colour = \"white\"\n  ) +\n  labs(\n    title = \"Theoretical sample distribution of reaction times\",\n    subtitle = glue(\"Gaussian distribution: mean = {round(rt_mean)} ms, SD = {round(rt_sd)}\"),\n    x = \"RT (ms)\", y = \"Relative probability (density)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#highlighting-and-notes",
    "href": "preface.html#highlighting-and-notes",
    "title": "Preface",
    "section": "Highlighting and notes",
    "text": "Highlighting and notes\nHighlighting and notes are enabled in this book, thanks to the Hypothesis plugin. You can access the Hypothesis sidebar from the right side of the textbook (click on the &lt; button, above the eye button in the right-end side of the website).\nYou need to create an Hypothesis account and log in on the side bar to be able to use the plugin. The account is free. Highlighting and comments are private by default. Please do not make public highlights and notes.\n\n\n\n\n\n\n\n\n\nCumming, Geoff. 2013. “The New Statistics: Why and How.” Psychological Science 25 (1): 729. https://doi.org/10.1177/0956797613504966.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The Journal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\n———. 2018. “Statistical Rituals: The Replication Delusion and How We Got There.” Advances in Methods and Practices in Psychological Science 1 (2): 198218. https://doi.org/10.1177/2515245918771329.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual. What You Always Wanted to Know about Significance Testing but Were Afraid to Ask.” In, 391408.\n\n\nKruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin & Review 25 (1): 178206. https://doi.org/10.3758/s13423-016-1221-4.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press.\n\n\nNicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025. Introduction to Bayesian Data Analysis for Cognitive Science. https://bruno.nicenboim.me/bayescogsci/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science (2e). Second edition. https://r4ds.hadley.nz.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch-research-methods.html",
    "href": "ch-research-methods.html",
    "title": "1  Research methods",
    "section": "",
    "text": "1.1 Empirical research\nResearch Methods are about the theory, methods and practice of conducting research. I like to call the discipline that deals with research methods Hodotics, but it hasn’t caught on yet. One way of categorising different aspects of research methods is represented in Figure 1.1. You can think of research methods as the combination of:\nLet’s zoom in on the research process, as represented in Figure 1.2.\nEmpirical research is one approach to research. This type of research focusses on learning about the Universe through data and observation.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-methods.html#empirical-research",
    "href": "ch-research-methods.html#empirical-research",
    "title": "1  Research methods",
    "section": "",
    "text": "TipEmpirical research\n\n\n\nThe word empirical is related to experience, and in the context of research it basically means “based on experience (i.e. data and observation)”.\nLear more about the etymology of empirical here.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-methods.html#axes-of-research",
    "href": "ch-research-methods.html#axes-of-research",
    "title": "1  Research methods",
    "section": "1.2 Axes of research",
    "text": "1.2 Axes of research\n\n\n\n\n\n\nFigure 1.3: Research axes.\n\n\n\nThere are two main “axes” of empirical research types: exploratory vs corroboratory and descriptive vs explanatory.\n\n\n\n\n\n\nTipExploratory vs corroboratory research\n\n\n\n\nExploratory research is about exploring the data looking for patterns, associations, features and so on. This type of research is also known as “hypothesis generating” because exploration can lead to the formulation of new hypotheses.\nCorroboratory research (aka as confirmatory research) is about checking expectations against data. It is also known as “hypothesis testing” because it is about testing hypotheses using data.\n\n\n\nWhile there is still a lot of prejudice against exploratory research (typical sentiments are “it doesn’t have theory”) it is an important way of doing research, as recognised by important scholars. For example, Tukey (1980) stressed the importance of both approaches. The other axis of research is the descriptive/explanatory axis.\n\n\n\n\n\n\nTipDescriptive vs explanatory research\n\n\n\n\nDescriptive research is about describing facts through observation and collection of data. In other words, descriptive research is about the what.\nExplanatory research is about explaining facts, i.e. understanding why they are the way they are. In other words, explanatory research is about the why.\n\n\n\nSimilarly to the exploratory/corroboratory axis, there is still prejudice against descriptive research (again, typical sentiments are that “it doesn’t have theory”). Within linguistics, several scholars have shown that both descriptive and explanatory research are fundamental and that both need conceptual and methodological theories to function. Indeed, Dryer (2008) talks about descriptive theories and explanatory theories, granting both the same status.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-methods.html#research-objectives",
    "href": "ch-research-methods.html#research-objectives",
    "title": "1  Research methods",
    "section": "1.3 Research objectives",
    "text": "1.3 Research objectives\nOrthogonal to the two research axes from the previous section, we can classify research instances based on their objectives. There are in principle three types of research objectives: establishing facts, improving the fit of a framework to the facts, and comparing the fit of difference frameworks to the facts. Each has its merits and to improve our understanding of the Universe we need all three, although there is nothing wrong for any one study to focus just on one or two!\n\n\n\n\n\n\nTipEstablish facts\n\n\n\nResearch can establish facts and fill a gap in the knowledge of one or more phenomena.\nThe aim of establishing facts is to accumulate evidence of particular events, features, associations.\nExamples:\n\nWhat are the uses of the Sanskrit verb gam ‘to go’?\nWhat is the duration of vowels in Mawayana (Arawakan)?\nDo people interact with AI as with other people?\n\n\n\n\n\n\n\n\n\nTipImprove fit of framework to facts\n\n\n\nResearch can improve the fit of a specific framework to established facts. Usually this is done to fine-tune a framework in light of new evidence but it also just works when you want to test new expectations/hypotheses. When the facts do not match the expectations, researchers modify the framework to accommodate the results.\nIn some cases, a framework can be totally abandoned in light of the facts, or a new one could be developed.\nExamples:\n\nStrong exemplar-based models preclude the possibility of abstract representations, but certain categorisation tasks seem to involve abstract representations so these must be included in exemplar-based models.\n\n\n\n\n\n\n\n\n\nTipCompare fit of different frameworks to facts\n\n\n\nThis objective allows researcher two compare two or more frameworks in light of empirical results. The main prerequisite for this approach is that each framework must have different expectations in relation to the phenomenon at hand.\nWhen different frameworks entail different and exclusive hypotheses, one can test the hypotheses with data: the results might help excluding certain hypotheses and keep others. The frameworks that generate the excluded hypotheses have to be abandoned (unless they can be modified to fit the new results, see above, while still be different enough from other frameworks).\nExamples:\n\nThere are two possible models for the bilingual lexicon: Word association and concept mediation. Which one better describes and explains the data?\nA strict feed-forward architecture of grammar does not allow phonetic details to be sensitive to morphological structure, while some exemplar-based models allow that.\n\n\n\nEach of the three objectives are important in research, but note that in order to really advance our understanding of things the third objective is fundamental: it is only by directly comparing different frameworks that we can accumulate knowledge and weed out inaccurate explanations. Every time you read about a study, ask yourself which of these objectives the study is setting to address.\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nSelect the appropriate research types for the following study: Previous research showed that in several Euroasiatic languages, vowels followed by voiced consonants tend to be longer than vowels followed by voiceless consonants. We investigate this tendency in Quechua.\n\n Descriptive, exploratory. Descriptive, corroboratory. Explanatory, corroboratory.\n\nWhich of the following studies aims to improve the fit of a framework to the data? (Thanks to András Bárány for suggesting the second example)\n\n We set out to test whether gestural timing is affected by foot-structure (as per the foot-sensitivity hypothesis) or not (as per the segmental hypothesis). In particular, we expect V-to-V timing to be stable within but not across feet independent of intervening segments if the foot-sensitivity hypothesis holds, while the timing should be affected by intervening segments both within and across feet if the segmental hypothesis holds. According to one hypothesis, there is one operation, Agree, which assigns Case features to an argument (accusative to objects, in the example) and at the same time gets the argument's person, number, and gender features. This means that in an English sentence like John sees her, her gets accusative case from a functional head (v) and v in turn gets the object's features — these are not spelled out in English; there is never any object agreement. Other languages are different in this respect: for example, in Hungarian, all direct objects have accusative case and the verb can show object agreement but it doesn't always do so. So there are a few theoretical options: either in all of these languages Case and agreement happen but case is not always realised (in English, case is sometimes realised but agreement never is; in Hungarian, object case is always realised, but object agreement only sometimes is), or Agree is actually not both Case and feature-agreement at the same time.\n\n\n\n\n\n\n\n\nDryer, Matthew S. 2008. “Descriptive Theories, Explanatory Theories, and Basic Linguistic Theory.” In Catching Language: The Standing Challenge of Grammar Writing, edited by Felix K. Ameka, Alan Charles Dench, and Nicholas Evans. Vol. 167. Trends in Linguistics Studies and Monographs. Mouton De Gruyter.\n\n\nTukey, John W. 1980. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.2307/2682991.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html",
    "href": "ch-research-context.html",
    "title": "2  Research context",
    "section": "",
    "text": "2.1 Research questions\nFigure 1.2 shows the main steps that compose the research process. The first component is the research context. Ellis and Levy (2008) discuss the research context and propose a convenient break-down of the concept. Figure 2.1 is a schematic representation of different aspects of the research context, from the most general to the most specific. An example of each is also provided.\nThe following sections treat research questions and research hypotheses in more detail.\nResearch questions are questions whose answers directly address the research problem. They take the form of actual questions. For example:\nResearch questions are always necessary, independent of the type and objective of the research. While there is an undue pressure on researcher to come up with “novel” research questions all the time, it is perfectly fine to ask the same question multiple times.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html#research-questions",
    "href": "ch-research-context.html#research-questions",
    "title": "2  Research context",
    "section": "",
    "text": "What is the average speech rate of adolescents vs that of older adults?\nWhat happens to infants syntactic processing when they move from a monolingual to a multilingual environment?\nIs the morphological complexity of languages spoken by larger populations different from that of languages spoken by smaller populations?",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html#research-hypotheses",
    "href": "ch-research-context.html#research-hypotheses",
    "title": "2  Research context",
    "section": "2.2 Research hypotheses",
    "text": "2.2 Research hypotheses\nResearch questions can be further developed into research hypotheses. Research hypotheses are statements (not questions) about the research problem. Hypotheses are never true nor confirmed. We can only corroborate hypothesis, and it’s a long term process. The same hypothesis has to be tested again and again, by multiple researchers in multiple contexts. Research is not a one-off matter: knowledge can only be acquired slowly and with a lot of effort. This idea has been beautifully synthesised into the “Slow Science” movement (Slow Science Academy 2010): “[Researchers] need time to think. [Researchers] need time to read, and time to fail. [Researchers] do not always know what it might be at right now.”\nIt is however perfectly fine to run a study with only research questions, without a research hypothesis. As long as you clearly state whether you are talking about research questions or research hypotheses and you don’t mix them up, you are fine.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html#precision-and-testability",
    "href": "ch-research-context.html#precision-and-testability",
    "title": "2  Research context",
    "section": "2.3 Precision and testability",
    "text": "2.3 Precision and testability\nSolid research questions and hypotheses must have two main properties: they must be precise and testable. Precision is about the semantics of the words and phrases that make up the question or hypothesis. For example, in the question “Is the morphological complexity of languages spoken by larger populations different from that of languages spoken by smaller populations?” we need to clearly define the following: morphological complexity, larger population, smaller population. What do we mean by “morphological complexity”? How do we classify a population as large or small? For our research question to be a good research question, it is important that we think very hard about what we mean by those words. This is because depending on the specific meaning, we might obtain different outcomes and to be sure that the outcomes answer out specific research question we need to ensure that the question itself and the words within it are well defined.\nSecondly, research questions and hypotheses must be testable. Testability is about formulating research questions and hypotheses in an enough precise manner that naturally leads to a well defined, specific study design. For example, the testability of the hypothesis from Figure 2.1 would be compromised if we didn’t define “processing cost” precisely. For example, processing cost could be related to the cognitive load of processing the sentences, or to the number of “computational steps” needed to process the sentence, or to ease of the computational steps independent of their number. All of these aspects are strictly entangled with the researcher’s assumptions and favourite linguistic framework or model of sentence processing. Very often, “fast research” leads to hypotheses that look precise and testable on the surface, but they fail to hit the mark upon greater scrutiny. Lack of precision and testability undermines the robustness of research, as pointed out for example by Yarkoni (2022), Scheel (2022), Scheel et al. (2020), and Devezer et al. (2021).\n\n\n\n\n\n\nTipPrecision and testability\n\n\n\nResearch questions and hypotheses should be precise (all the components should be clearly defined) and testable (they clearly translate into a well-defined, specific study design).\n\n\nIt is difficult to come by precise and testable hypotheses in linguistics just because our current knowledge and understanding of Language and languages is limited. At best, we can normally come up with vague hypotheses that state whether a difference between two conditions is expected or not and, if we are lucky, the direction of the difference (i.e. “A is greater than B” or vice versa). This state of affairs makes testing hypotheses using statistical methods less straightforward, because of the non-straightforward mapping of (vague) hypotheses to statistical models.\n\n\n\n\n\n\nImportantSpotlight: Falsificationism and falsifiability\n\n\n\n\n\nFalsification is a procedure proposed by philosopher Karl Popper in relation to the “problem of induction”. Induction is based on observations. Imagine you observe several swans over a long time period in the United Kingdom and they are all white. You induce that “all swans are white” and expect that to be true because you have never observed a swan that was not white. However, black swans do exist (they are native of Australia and New Zealand). You can see that it doesn’t matter how many white swans you observe in the UK, you cannot be certain of the truthfulness of the statement “all swans are white”. On the other hand, you only need see one single black swan to know that “all swans are white” is false. In other words, a statement can only ever be shown to be false, never to be true.\nSo, induction does not necessarily lead us to true statements, but falsification (observing even one case that makes the statement false) surely tells us which statements are false. A falsifiable statement or hypothesis should prevent us from wrongly accepting a false statement (but we can never know if it is true). John Spacey defines statement falsificability in his blog post Seven examples of falsifiability:\n\nA statement is falsifiable if it can be contradicted by an observation. If such observation is impossible to make with current technology, falsifiability is not achieved.\n\nSome examples of falsifiable hypotheses:\n\n“Life only exists on Earth.” (it would be falsified by the observation of life somewhere else).\n“If there is a 1st person exclusive dual, then there is also a 1st person inclusive dual.” [Universal 1871] (it would be falsified by the observation of languages with a 1st person exclusive dual but without the inclusive alternative).\n“Infants start uttering full sentences only after their 12th month of life.” (it would be falsified by the observation of infants uttering full sentences before their 12th month of life).\n\nThe following are some examples of non-falsifiable hypotheses:\n\n“Life might exist outside of the Solar system.” (if we observe life outside the Solar system or we don’t, the statement is still true, because of the might exist).\n“Languages with a 1st person inclusive dual can have a 1st person exclusive dual.” (whether we observe a language with both 1st inclusive and exclusive dual or not, the statement is still true, because of the can have.)\n\nFalsification has become a tenet of a lost of modern quantitative research and has become what could be regarded as falsificationism, but falsification is not the only approach to quantitative research, as you have learned in this chapter: precision and testability are two other equally valid criteria to follow when formulating hypotheses.\nIf you are interested in the philosophy behind research and statistics (commonly known as “philosophy of science”) I recommend the following books (of increasing length and depth): Okasha (2016), Dienes (2008), Rosenberg and Mclntyre (2020).\n\n\n\n\n\n\n\nDevezer, Berna, Danielle J. Navarro, Joachim Vandekerckhove, and Erkan Ozge Buzbas. 2021. “The Case for Formal Methodology in Scientific Reform.” Royal Society Open Science 8 (3): rsos.200805, 200805. https://doi.org/10.1098/rsos.200805.\n\n\nDienes, Zoltan. 2008. Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Macmillan International Higher Education.\n\n\nEllis, J. Timothy, and Yair Levy. 2008. “Framework of Problem-Based Research: A Guide for Novice Researchers on the Development of a Research-Worthy Problem.” Informing Science: The International Journal of an Emerging Transdiscipline 11: 1733. https://doi.org/10.28945/438.\n\n\nOkasha, Samir. 2016. Philosophy of Science: Very Short Introduction. Oxford: Oxford University Press. https://doi.org/10.1093/actrade/9780192802835.001.0001.\n\n\nRosenberg, Alexander, and Lee Mclntyre. 2020. Philosophy of science: a contemporary introduction. Fourth edition. Routledge contemporary introductions to philosophy. New York London: Routledge.\n\n\nScheel, Anne M. 2022. “Why Most Psychological Research Findings Are Not Even Wrong.” Infant and Child Development 31 (1): e2295. https://doi.org/10.1002/icd.2295.\n\n\nScheel, Anne M., Leonid Tiokhin, Peder M. Isager, and Daniël Lakens. 2020. “Why Hypothesis Testers Should Spend Less Time Testing Hypotheses.” Perspectives on Psychological Science 16 (4): 744–55. https://doi.org/10.1177/1745691620966795.\n\n\nSlow Science Academy. 2010. “The Slow Science Manifesto.” http://slow-science.org.\n\n\nYarkoni, Tal. 2022. “The Generalizability Crisis.” Behavioral and Brain Sciences 45. https://doi.org/10.1017/s0140525x20001685.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html",
    "href": "ch-quantitative.html",
    "title": "3  Quantitative data analysis",
    "section": "",
    "text": "3.1 Quantitative data analysis\nData analysis is anything that relates to analysing data, whether you collected it yourself or you used pre-existing data.\nThere are two main approaches to data analysis:\nNote that while it is common to talk about “quantitative vs qualitative data” in fact in most cases data can be conceived as both quantitative and qualitative. It is really how we approach the data that can be quantitative and/or qualitative. Moreover, these two approaches to data analysis are not necessarily opposite to each other and there are some aspects of each in each other. This will become clearer at the end of the course this textbook is written for.\nThis textbook focuses on quantitative data analysis. The rest of this chapter introduces fundamental concepts of quantitative methods.\nQuantitative analyses are usually comprised of three parts (these are not strictly distinct and the boundaries are sometimes blurred):\nSummary measures are numbers that represent certain properties of the data: common summary measures are the mean and the standard deviation. You will have frequently seen these in published paper, either in text or as a table. You will learn about summary measures in Chapter 10.\nPlots, or graphs, are another common way to summarise data but they are based on visual representation rather than single numbers. As the saying goes, “a picture is worth a thousand words”. The aim of plots is to make explicit certain patterns in the data. Choosing and designing plots that are effective and captivating is more of an art and you will learn the basics and heuristics of good (and bad plots) in Chapter 14.\nStatistical models are mathematical representations of patterns and relationship in data. Statistical modelling is a powerful tool to learn from the data or to assess research hypotheses. This textbook introduces you to a specific type of statistical models: regression models. These are highly flexible model that can be used with a variety of data types. You will start learning about statistical models in Chapter 23.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html#quantitative-data-analysis",
    "href": "ch-quantitative.html#quantitative-data-analysis",
    "title": "3  Quantitative data analysis",
    "section": "",
    "text": "Summarise data with summary measures.\nVisualise data with plots.\nModel data with statistical models.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html#the-computational-workflow",
    "href": "ch-quantitative.html#the-computational-workflow",
    "title": "3  Quantitative data analysis",
    "section": "3.2 The computational workflow",
    "text": "3.2 The computational workflow\n\n\n\n\n\n\nFigure 3.1: An overview of the computational workflow of quantitative data analysis from Wickham, Çetinkaya-Rundel, and Grolemund (2023). CC BY-NC-ND 3.0\n\n\n\nAnother way to look at quantitative data analysis is through its computational workflow. Figure 3.1 shows a typical workflow (Wickham, Çetinkaya-Rundel, and Grolemund 2023): you import data, you tidy data up (i.e. you reshape the data so that it is easy to work with), you transform it (i.e., you filter observations, change existing columns or create new ones, obtain summary measures and join data together), visualise it, apply statistical models and the communicate what you learned. Very often, transforming, visualising and modelling data is done iteratively, that is why these steps are shown in a loop in Figure 3.1, and together they form the “understanding” part of the process. Through the transform-visualise-model cycle, you understand things about the data. All of the steps in Figure 3.1 are surrounded by program: this is “computational programming”, in other words using the computer to execute those steps.\nYou will learn the basics of how to import (aka read) data in Chapter 9, transform it in Chapter 11 and Chapter 12, visualise it in Chapter 14 and Chapter 15, and model it from Chapter 21 onwards. However, you will find bits from any of these steps in many other chapters, so that you won’t have to learn everything at once.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html#numbers-have-no-meaning",
    "href": "ch-quantitative.html#numbers-have-no-meaning",
    "title": "3  Quantitative data analysis",
    "section": "3.3 Numbers have no meaning",
    "text": "3.3 Numbers have no meaning\nFinally, I should mention a more philosophical aspect of quantitative data analysis. As said above, both qualitative and quantitative approaches are valid and necessary to improve our understanding of things. Crucially, even a very complex quantitative analysis will always contain some qualitative aspects to it.\n\n\n\n\n\n\nThe numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.\n—Nate Silver, The Signal and the Noise\n\n\n\nThere’s a lot of wisdom in that quote. Numbers do not mean anything by themselves. We need to interpret numbers, “imbue them with meaning”, based on many aspects of research and beyond, including our own identity and positionality (Jafar 2018; Darwin Holmes 2020). Gelman and Hennig (2017) highlight how we should move away from concepts of “objectivity” and “subjectivity” as applied to statistics, and instead propose a broader collection of “virtues”. They say: “Instead of debating over whether a given statistical method is subjective or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize attributes such as transparency and acknowledgement of multiple perspectives as complementary” (Gelman and Hennig 2017, 973). The philosophical backdrop of this textbook (and its author) very much embody this sentiment.\n\n\n\n\nDarwin Holmes, Andrew Gary. 2020. “Researcher Positionality: A Consideration of Its Influence and Place in Qualitative Researcha New Researcher Guide.” Shanlax International Journal of Education 8 (4): 110. https://doi.org/10.34293/education.v8i4.3232.\n\n\nGelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and Objective in Statistics.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 180 (4): 9671033. https://doi.org/10.1111/rssa.12276.\n\n\nJafar, Anisa J. N. 2018. “What Is Positionality and Should It Be Expressed in Quantitative Studies?” Emergency Medicine Journal. https://doi.org/10.1136/emermed-2017-207158.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science (2e). Second edition. https://r4ds.hadley.nz.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html",
    "href": "ch-r-rstudio.html",
    "title": "4  R basics",
    "section": "",
    "text": "4.1 Why R?\nR can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, map (GIS, Geographic Information System) data and even images.\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free for anyone to use!",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#r-vs-rstudio",
    "href": "ch-r-rstudio.html#r-vs-rstudio",
    "title": "4  R basics",
    "section": "4.2 R vs RStudio",
    "text": "4.2 R vs RStudio\nBeginners usually have trouble understanding the difference between R and RStudio. Let’s use a car analogy. What makes the car go is the engine and you can control the engine through the dashboard. You can think of R as an engine and RStudio as the dashboard.\n\n\n\n\n\n\n\nTipR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the related task is executed.\n\n\n\n\n\n\n\n\n\nTipRStudio\n\n\n\n\nRStudio is an Integrated Development Environment or IDE.\nIt helps you using R more efficiently.\nIt has a graphical user interface or GUI.\n\n\n\nThe next section will give you a tour of RStudio.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#rstudio-1",
    "href": "ch-r-rstudio.html#rstudio-1",
    "title": "4  R basics",
    "section": "4.3 RStudio",
    "text": "4.3 RStudio\nOpen RStudio on your computer and familiarise yourself with the different parts. When you open RStudio, you can see the window is divided into 3 panels:\n\nBlue (left): the Console.\nGreen (top-right): the Environment tab.\nPurple (bottom-right): the Files tab.\n\n\nThe Console is where R commands can be executed. Think of this as the interface to R. Now, try to run (execute) some R code in the console:\n\n\n\n\n\n\nWarningExercise 1\n\n\n\n\nWrite the following code in the Console:\nsum(3, 1, 4)\nPress ENTER/RETURN on your keyboard. This will run (i.e. execute) the code. The code sums the numbers 3, 1, and 4.\nThe output of the code is shown below it, in the Console. (Never mind the [1] for now).\n\n\n\nThe Environment tab lists the objects created with R, while in the Files tab you can navigate folders on your computer to get to files and open them in the file Editor.\n\n4.3.1 RStudio and Quarto projects\nRStudio is an IDE (see above) which allows you to work efficiently with R, all in one place. Note that files and data live in folders on your computer, outside of RStudio: do not think of RStudio as an app where you can save files in. All the files that you see in the Files tab are files on your computer and you can access them from the Finder or File Explorer as you would with any other file.\nIn principle, you can open RStudio and then navigate to any folder or file on your computer. However, there is a more efficient way of working with RStudio: RStudio and Quarto Projects.\n\n\n\n\n\n\nTipProjects\n\n\n\nAn RStudio Project is a folder on your computer that has an .Rproj file.\nA Quarto Project is an RStudio Project with a _quarto.yml file.\n\n\nYou can create as many Quarto Projects as you wish, and I recommend to create one per project (your dissertation, a research project, a course, etc…). We will create a Quarto Project for this course (meaning, you will create a folder for the course which will be the Quarto Project). You will have to use this project/folder throughout the semester.\nTo create a new Quarto Project, click on the button that looks like a transparent light blue box with a plus, in the top-left corner of RStudio. A window like the one below will pop up.\n\nClick on New Directory then Quarto Project.\n\nNow, this will create a new folder (aka directory) on your computer and will make that a Quarto Project (meaning, it will add a file with the .Rproj extension and a file called _quarto.yml to the folder; the name of the .Rproj file will be the name of the project/folder).\nGive a name to your new project, something like the name of the course and year (e.g. qml-2025).\nThen you need to specify where to create this new folder/Project. Click on Browse… and navigate to the folder you want to create the new folder/Project in. This could be your Documents folder, or the Desktop (we had issues with OneDrive in the past, so we recommend you save the project outside of OneDrive).\nWhen done, click on Create Project. RStudio will automatically open your new project.\n\n\n\n\n\n\n\nImportantImportant\n\n\n\nWhen working through this textbook, always make sure you are in the Quarto Project you just created for the course.\nYou know you are in an RStudio/Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light blue cube icon.\nIf you see Project (none) in the top-right corner, that means your are not in a Quarto Project.\nAn easy way to ensure that RStudio is opened from within a specific project, to open RStudio go to the project folder in File Explorer or Finder and double click on the .Rproj file. This will automatically open RStudio and set the project to that folder.\n\n\nThere are several ways of opening a Quarto Project:\n\nYou can go to the Quarto Project folder in Finder or File Explorer and double click on the .Rproj file.\nYou can click on File &gt; Open Project in the RStudio menu.\nYou can click on the project name in the top-right corner of RStudio, which will bring up a list of projects. Click on the desired project to open it.\n\n\n\n\n\n\n\nWarningExercise 2\n\n\n\n\nClose RStudio.\nNow re-open RStudio by double-clicking on the .Rproj file of the project you created.\n\n\n\n\n\n4.3.2 A few important settings\nBefore moving on, there are a few important settings that you need to change.\n\n\nOpen the RStudio preferences (Tools &gt; Global options...).\nUn-tick Restore .RData into workspace at startup.\n\nThis mean that every time you start RStudio you are working with a clean Environment. Not restoring the workspace ensures that the code you write is fully reproducible. When this setting is enabled, your environment is saved in a hidden file called .Rdata and loaded every time you start RStudio. This very frequently leads to errors where the wrong variables/data is read or used by the code without you noticing, so always make sure the setting is disabled.\n\nSelect Never in Save workspace to .RData on exit.\n\nSince we are not restoring the workspace at startup, we don’t need to save it. Remember that as long as you save the code, you will not lose any of your work! You will learn how to save code from next week.\n\nClick OK to confirm the changes.\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nTrue or false?\n\nRStudio executes the code. TRUEFALSE\nR is a programming language. TRUEFALSE\nAn IDE is necessary to run R. TRUEFALSE\nRStudio projects are folders with an .Rproj file. TRUEFALSE\nQuarto projects can’t be RStudio projects TRUEFALSE\nThe project name is shown in the top-right corner of RStudio. TRUEFALSE\nI have disabled Restore .RData and Save workspace in the settings. TRUEFALSE",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#r-basics",
    "href": "ch-r-rstudio.html#r-basics",
    "title": "4  R basics",
    "section": "4.4 R basics",
    "text": "4.4 R basics\nIn this part of the tutorial you will learn the very basics of R. If you have prior experience with programming, you should find all this familiar. If not, not to worry! Make sure you understand the concept highlighted in the green boxes and practice the related skills.\nIn the following sections, you should just run code directly in the R Console in RStudio, i.e. you will type code in the Console and press ENTER to run it.\nIn later chapters, you will learn how to save your code in a script file or in Quarto documents, so that you can keep track of which code you have run and make your work reproducible.\n\n4.4.1 R as a calculator\nWrite this code 1 + 2 in the Console, then press ENTER/RETURN to run the code. Fantastic! You should see that the answer of the addition has been printed in the Console, like this:\n[1] 3\n(Never mind the [1] for now).\nNow, try some more operations (write each of the following in the Console and press ENTER). Feel free to add your own operations to the mix!\n\n67 - 13\n2 * 4\n268 / 43\n\nYou can also chain multiple operations.\n\n6 + 4 - 1 + 2\n4 * 2 + 3 * 2\n\n\n\n\n\n\n\nNoteQuiz 2\n\n\n\nAre the following pairs of operations equivalent?\n\n3 * 2 / 4 = 3 * (2 / 4) TRUEFALSE\n10 * 2 + 5 * 0.2 = (10 * 2 + 5) * 0.2 TRUEFALSE\n\n\n\n\n\n\n\n\n\nImportantSpotlight: Arithmetics\n\n\n\n\n\nIf you need a maths refresher, I recommend checking the following pages:\n\nhttps://www.mathsisfun.com/definitions/order-of-operations.html\nhttps://www.mathsisfun.com/algebra/introduction.html\n\n\n\n\n\n\n4.4.2 Variables\n\nForget-me-not.\n\nMost times, we want to store a certain value so that we can use it again later.\nWe can achieve this by creating variables.\n\n\n\n\n\n\nTipVariable\n\n\n\nA variable holds one or more values and it’s stored in the computer memory for later use.\n\n\nYou can create a variable by using the assignment operator &lt;-.\nLet’s assign the value 156 to the variable my_num.\n\nmy_num &lt;- 156\n\nNow, check the list of variables in the Environment tab of the top-right panel of RStudio. You should see the my_num variable and its value there.\nNow, you can just call the variable back when you need it! Write the following in the Console and press ENTER.\n\nmy_num\n\n[1] 156\n\n\nA variable like my_num is also called a numeric vector: i.e. a vector that contains a number (hence numeric).\n\n\n\n\n\n\nTipVector\n\n\n\nA vector is an R object that contains one or more values of the same type.\n\n\nA vector is a type of variable and a numeric vector is a type of vector. However, it’s fine in most cases to use the word variable to mean vector (just note that a variable can also be something else than a vector; you will learn about other R objects in later chapters).\nLet’s now try some operations using variables.\n\nincome &lt;- 1200\nexpenses &lt;- 500\nincome - expenses\n\n[1] 700\n\n\nSee? You can use operations with variables too! And you can also go all the way with variables.\n\nsavings &lt;- income - expenses\n\nAnd check the value…\n\nsavings\n\n[1] 700\n\n\nVectors can hold more than one item or value. Just use the combine c() function to create a vector containing multiple values. The following are all numeric vectors.\n\none_i &lt;- 6\n# Vector with 2 values\ntwo_i &lt;- c(6, 8)\n# Vector with 3 values\nthree_i &lt;- c(6, 8, 42)\n\nCheck the list of variables in the Environment tab. You will see now that before the values of two_i and three_i you get the vector type num for numeric. (If the vector has only one value, you don’t see the type in the Enviroment list but it is still of a specific type).\n\n\n\n\n\n\nTipNumeric vector\n\n\n\nA numeric vector is a vector that holds one or more numeric values.\n\n\nNote that the following are the same:\n\none_i &lt;- 6\none_i\n\n[1] 6\n\none_ii &lt;- c(6)\none_ii\n\n[1] 6\n\n\nAnother important aspect of variables is that they are… variable! Meaning that once you assign a value to one variable, you can overwrite the value by assigning a new one to the same variable.\n\nmy_num &lt;- 88\nmy_num &lt;- 63\nmy_num\n\n[1] 63\n\n\n\n\n\n\n\n\nNoteQuiz 3\n\n\n\nTrue or false?\n\nA vector is a type of variable. TRUEFALSE\nNot all variables are vectors. TRUEFALSE\nA numeric vector can only hold numeric values. TRUEFALSE\n\n\n\n\n\n4.4.3 Functions\n\nR cannot function without… functions.\n\n\n\n\n\n\n\nTipFunction\n\n\n\nA function usually runs an operation on one or more specified arguments.\n\n\nA function in R has the form function() where:\n\nfunction is the name of the function, like sum.\n() are round parentheses, inside of which you write arguments, separated by commas.\n\nLet’s see an example:\n\nsum(3, 5)\n\n[1] 8\n\n\nThe sum() function sums the number listed as arguments. Above, the arguments are 3 and 5.\nAnd of course arguments can be vectors!\n\nmy_nums &lt;- c(3, 5, 7)\n\nsum(my_nums)\n\n[1] 15\n\nmean(my_nums)\n\n[1] 5\n\n\n\n\n\n\n\n\nNoteQuiz 4\n\n\n\nTrue or false?\n\nFunctions can take other functions as arguments. TRUEFALSE\nAll function arguments must be specified. TRUEFALSE\nAll functions need at least one argument. TRUEFALSE\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\n4c\nThe Sys.Date() function and other functions like it don’t take any arguments.\n\n\n\n\n\n\n\n\n\nImportantR Note: R vs Python\n\n\n\n\n\nIf you are familiar with Python, you will soon realise that R and Python, although they share many concepts and types of objects, they can differ substantially. This is because R is a functional programming language (based on functions) while Python is an Object Oriented programming language (based on methods applied on objects).\nGenerally speaking, functions look like print(x) while methods look like x.print()\n\n\n\n\n\n4.4.4 String and logical vectors\n\nNot just numbers.\n\nWe have seen that variables can hold numeric vectors. But vectors are not restricted to being numeric. They can also store strings. A string is basically a set of characters (a word, a sentence, a full text). In R, strings have to be quoted using double quotes \" \".\nChange the following strings to your name and surname. Remember to use the double quotes.\n\nname &lt;- \"Stefano\"\nsurname &lt;- \"Coretta\"\n\nname\n\n[1] \"Stefano\"\n\nsurname\n\n[1] \"Coretta\"\n\n\nStrings can be used as arguments in functions, like numbers can.\n\ncat(\"My name is\", name, surname)\n\nMy name is Stefano Coretta\n\n\nRemember that you can reuse the same variable name to override the variable value.\n\nname &lt;- \"Raj\"\n\ncat(\"My name is\", name, surname)\n\nMy name is Raj Coretta\n\n\nYou can combine multiple strings into a character vector, using the combine function c() (the function works with any type of vectors, not only characters!).\n\n\n\n\n\n\nTipCharacter vector\n\n\n\nA character vector is a vector that holds one or more strings.\n\n\n\nfruit &lt;- c(\"apple\", \"oranges\", \"bananas\")\nfruit\n\n[1] \"apple\"   \"oranges\" \"bananas\"\n\n\nCheck the Environment tab. Character vectors have chr before the list of values.\nAnother type of vector is one that contains either TRUE or FALSE. Vectors of this type are called logical vectors and they are listed as logi in the Environment tab.\n\n\n\n\n\n\nTipLogical vector\n\n\n\nA logical vector is a vector that holds one or more TRUE or FALSE values.\n\n\n\ngroceries &lt;- c(\"apple\", \"flour\", \"margarine\", \"sugar\")\nin_pantry &lt;- c(TRUE, TRUE, FALSE, TRUE)\n\ndata.frame(groceries, in_pantry)\n\n\n  \n\n\n\nTRUE and FALSE values must be written in all capitals and without double quotes (they are not strings!).\n(We will talk about data frames, another type of object in R, in the following chapters.)\n\n\n\n\n\n\nNoteQuiz 5\n\n\n\n\nWhich of the following is not a character vector.\n\n c(1, 2, \"43\") \"s\" c(apple) (assuming apple &lt;- 45) c(letters)\n\nWhich of the following is not a logical vector.\n\n c(T, T, F) TRUE \"FALSE\" c(FALSE)\n\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nYou can use the class() function to check the type (“class”) of a vector.\n\nclass(FALSE)\n\n[1] \"logical\"\n\nclass(c(1, 45))\n\n[1] \"numeric\"\n\nclass(c(\"a\", \"b\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\nWarningExplanation\n\n\n\n\n\n5a\n\nc(1, 2, \"43\") is a character vector because the last number \"43\" is a string (it’s between double quotes!). A vector cannot have a mix of types of elements: they have to be all numbers or all strings or else, but not some numbers and some strings. Numbers are special in that if you include a number in a character vector without quoting it, it is automatically converted into a string. Try the following:\n\n\nchar &lt;- c(\"a\", \"b\", \"c\")\nchar &lt;- c(char, 1)\nchar\nclass(char)\n\n\nc(letters) is a character vector because letters contains the letters of the alphabet as strings (this vector comes with base R).\nc(apple) is not a character vector because the variable apple holds a number, 45!\n\n5b\n\n\"FALSE\" is not a logical vector because FALSE has been quoted (anything that is quoted is a string!).\n\n\n\n\n\n\n\n\n\n\nImportantR Note: For-loops and if-else statements\n\n\n\n\n\nThis course does not cover programming in R in the strict sense, but if you are curious here’s a short primer on for-loops and if-else statements in R.\nFor-loops\n\nfruits &lt;- c(\"apples\", \"mangos\", \"durians\")\n\nfor (fruit in fruits) {\n  cat(\"I like\", fruit, \"\\n\")\n}\n\nI like apples \nI like mangos \nI like durians \n\n\nIf-else\n\nfor (fruit in fruits) {\n  if (grepl(\"n\", fruit)) {\n    cat(fruit, \"has an 'n'\", \"\\n\")\n  } else {\n    cat(fruit, \"does not have an 'n'\", \"\\n\")\n  }\n}\n\napples does not have an 'n' \nmangos has an 'n' \ndurians has an 'n' \n\n\nFor more, check the For loops section of the R4DS book and the R if else statement post from DataMentor.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#summary",
    "href": "ch-r-rstudio.html#summary",
    "title": "4  R basics",
    "section": "4.5 Summary",
    "text": "4.5 Summary\nYou made it! You completed this chapter.\nHere’s a summary of what you learned.\n\n\n\n\n\n\n\nR is a programming language while RStudio is an IDE.\nRStudio projects are folders with an .Rproj file (you can see the name of the project you are currently in in the top-right corner of RStudio).\nYou can perform mathematical operations with +, -, *, /.\nYou can store values in variables.\nA typical object to be stored in a variable is a vector: there are different type of vectors, like numeric, character and logical.\nFunctions are used to perform an operation on its arguments: sum() sums it’s arguments, mean() calculates the mean and cat() prints the arguments.\n\n\n\n\n\n\n\n\n\n\nImportantR Note: Programming in R\n\n\n\n\n\nIf you are interested in learning about programming in R, I recommend you go through Chapters 26-28 of the R4DS book and the Advanced R book.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-packages.html",
    "href": "ch-packages.html",
    "title": "5  R packages",
    "section": "",
    "text": "5.1 Install packages\nWhen you install R, a library of packages is also installed. Packages provide R with extra functionalities, usually by making extra functions available for use. You can think of packages as “plug-ins” that you install once and then you can “activate” them when you need them. The library installed with R contains a set of packages that are collectively known as the base R packages, but you can install more any time!\nNote that the R library is a folder on your computer. Packages are not installed inside RStudio. Remember that RStudio is just an interface.\nYou can check all of the currently installed packages in the bottom-right panel of RStudio, in the Packages tab. There you can also install new packages.\nYou can install extra packages in the R library in two ways:\nGo ahead and try to install a package using the second method. Install the cowsay and the fortunes packages (see picture above for how to write the packages). After installing you will see that the package fortunes is listed in the Packages tab.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-packages.html#install-packages",
    "href": "ch-packages.html#install-packages",
    "title": "5  R packages",
    "section": "",
    "text": "You can use the install.packages() function. This function takes the name of the package you want to install as a string, for example install.packages(\"cowsay\").\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you install a package with the function install.packages(), do so in the Console! You will start using R scripts soon, so please do not include this function in your scripts (this is because you install packages only once, see below).\n\n\n\nOr you can go the Packages tab in the bottom-right panel of RStudio and click on Install. A small window will pop up. See the screenshot below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipInstall packages\n\n\n\nTo install packages, go to the Packages tab of the bottom-right panel of RStudio and click on Install.\nIn the “Install packages” window, list the package names and then click Install.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to install a package ONLY ONCE! Once installed, it’s there for ever, saved in the R library. You will be able to use all of your installed packages in any RStudio/Quarto project you create.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-packages.html#attaching-packages",
    "href": "ch-packages.html#attaching-packages",
    "title": "5  R packages",
    "section": "5.2 Attaching packages",
    "text": "5.2 Attaching packages\nNow, to use a package you need to attach the package to the current R session with the library() function. Attaching a package makes the functions that come with the package available to us.\n\n\n\n\n\n\nImportant\n\n\n\nYou need to attach the packages you want to use once per R session.\nNote that every time you open RStudio, a new R session is started.\n\n\nLet’s attach the cowsay and fortunes packages. Run the following code in the Console.\n\nlibrary(cowsay)\nlibrary(fortunes)\n\nNote that library(cowsay) takes the name of the package without quotes, although if you put the name in quotes it also works. You need one library() function per package (there are other ways, but we will stick with this one).\n\n\n\n\n\n\nTipAttaching packages\n\n\n\nPackages are attached with the library(pkg.name) function, where pkg.name is the name of the package.\n\n\nNow you can use the functions provided by the attached packages. Try out the say() function from the cowsay package.\n\nsay(\"hot diggity\", \"frog\")\n\n(I know, the usefulness of the package might be questionable, but it is fun!)\n\n\n\n\n\n\nImportant\n\n\n\nRemember, you need to install a package only once but you need to attach it with library() every time you start R.\nThink of install.packages() as mounting a light bulb (installing the package) and library() as the light switch (attaching the package).",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-packages.html#package-documentation",
    "href": "ch-packages.html#package-documentation",
    "title": "5  R packages",
    "section": "5.3 Package documentation",
    "text": "5.3 Package documentation\nTo learn what a function does, you can check its documentation by typing in the Console the function name preceded by a ? question mark. Type ?say in the Console and hit ENTER to see the function documentation. You should see something like this:\n\n\n\n\n\nThe Description section is usually a brief explanation of what the function does.\nIn the Usage section, the usage of the function is shown by showing which arguments the function has and which default values (if any) each argument has. When the argument does not have a default value, NULL is listed as the value.\nThe Arguments section gives a thorough explanation of each function argument. (Ignore … for now).\nHow many arguments does say() have? How many arguments have a default value?\nDefault argument values allow you to use the function without specifying those arguments. Just write say() in your script on a new line and run it. Does the output make sense based on the Usage section of the documentation?\nThe rest of the function documentation usually has further details, which are followed by Examples. It is always a good idea to look at the example and test them in the Console when learning new functions.\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nWhich of the following statements is wrong?\n\n You attach libraries with library().\n install.packages() does not load packages.\n The R library is a folder.\n\n\n\n\n\n\n\n\n\nImportantExplanation\n\n\n\n\n\nThis was a question about terminology. In R, you attach packages from the library using (confusingly) the library() function.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-inference.html",
    "href": "ch-inference.html",
    "title": "6  Inference",
    "section": "",
    "text": "6.1 Uncertainty and variability\nImbuing numbers with meaning is a good characterisation of the inference process. Here is how it works. We have a question about something. Let’s imagine that this something is the population of British Sign Language signers. We want to know whether the cultural background of the BSL signers is linked to different pragmatic uses of the sign for BROTHER. But we can’t survey the entire population of BSL signers. So instead of surveying all BSL users, we take a sample from the BSL population. The sample is our data (the product of our study or observation). Now, how do we go from data/observation to answering our question about the use of BROTHER? We can use the inference process!\nThe figure below is a schematic representation of the inference process.\nThe inference process has two main stages: producing data and inference. For the first step, producing data, we start off with a population. Note that population can be a set of anything, not just a specific group of people. For example, the words in a dictionary can be a “population”; or the antipassive constructions of Austronesian languages, and so on. From that population, we select a sample and that sample produces our data. We analyse the data to get results. Finally, we use inference to understand something about the population based on the results from the sampled data. Inference can take many forms and the type of inference we are interested in here is statistical inference: i.e. using statistics to do inference.\nHowever, despite inference being based on data, it does not guarantee that the answers to our questions are right or even that they are true. In fact, any observation we make comes with a certain degree of uncertainty and variability.\nPliny the Elder was a Roman philosopher who died in the Vesuvius eruption in 79 CE. He certainly did not expect to die then. Leaving dark irony aside, as researchers we have to deal with uncertainty and variability.\nSo uncertainty is a feature of each mesurement, while variability occurs between different measurements. Together, uncertainty and variability render the inference process more complex and can interfere with its outcomes.\nThe following picture is a reconstruction of what Galileo Galilei saw when he pointed one of his first telescopes towards Saturn, based on his 1610 sketch: a blurry circle flanked by two smaller blurry circles.\nOnly six years later, telescopes were much better and Galileo could correctly identify that the flaking circles were not spheres orbiting around Saturn, but rings. The figures below show how the sketches evolved over time between first observation and publication.\nThe moral of the story is that at any point in history we are like Galileo in at least some of our research: we might be close to understanding something but not quite yet.\nTo give a more concrete example of how each sample is but an imperfect represetation of the population it is taken from, let’s look at reaction times (RTs) data from the MALD dataset (Tucker et al. 2019). The study the data comes from used an auditory lexical decision task to elicit RTs and accuracy data. In each trial, participants listened to a word and pressed a button to say if the word is a real English word or not. The RT is the time lag between the offset of the auditory stimulus (the target word) and the button press. Note that to keep things more manageable, the data we will read is just a subset of the full data. Figure 6.1 shows a density plot of the data: the x-axis is the range of RTs (in logged milliseconds), while the y-axis shows the “density” of the data. Higher density means that the data contains a lot of observations in that region. Conversely, low density means that the data does not contain a lot of observations in that range. You will learn more about density plots in Chapter 18.\nCode\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nmald |&gt; \n  filter(RT_log &gt; 6) |&gt; \n  ggplot(aes(RT_log)) +\n  geom_density(fill = \"purple\", alpha = 0.5) +\n  geom_rug(alpha = 0.1) +\n  labs(x = \"Reaction Times (logged ms)\")\n\n\n\n\n\n\n\n\nFigure 6.1: Distribution of logged RT values from the MALD data set. The density is a relative measure of how many observations for particular values there are in the data.\nThe mean logged RT is 6.88 and the standard deviation (a measure of dispersion around the mean, you will learn about them in Chapter 10) is 0.28. We might take these values as the mean and standard deviation of the population of logged RTs. But this would not be correct: these are the sample mean and standard deviation. To show why we cannot take these to be the mean and standard deviation of the population, we can simulate RT data based on those values (you will understand the details of this later on, when you learn about probability distributions in Chapter 18, so for now just stay for the ride).\nCode\nset.seed(9899)\nrt_l &lt;- list()\nfor (i in 1:10) {\n  rt_l[i] &lt;- list(rlnorm(n = 20, mean(mald$RT_log), sd(mald$RT_log)))\n}\nWe sample 20 randomly generated values from a distribution with mean 6.88 and standard deviation 0.28. We then can take the mean and SD of these generated values and compare them to the original distribution’s mean and SD. But we can go further and sample 20 values 10 times. This procedure gives us 10 means and standard deviations, one for each sample of 20 values. The means and standard deviations of these 10 random samples are shown in Table 27.3.\nTable 6.1\n\n\n\n\n\n\nsample\nmean\nsd\n\n\n\n\n1\n6.84\n0.24\n\n\n2\n7.02\n0.30\n\n\n3\n6.92\n0.19\n\n\n4\n6.88\n0.31\n\n\n5\n6.84\n0.27\n\n\n6\n6.80\n0.22\n\n\n7\n6.84\n0.25\n\n\n8\n6.95\n0.34\n\n\n9\n6.99\n0.31\n\n\n10\n6.97\n0.32\nYou will notice that, while all the means and SD are very close to the mean and SD we sampled from, they are not exactly the same: every sample’s mean and SD are slightly different from each other and from the mean and SD we sample from. In other words, it’s very very unlikely that the sample mean and standard deviation are exactly the population mean and standard deviation. Inference is affected by uncertainty and variability. So what do we do with such uncertainty and variability? We can use statistics to quantify them!\nBut what is statistics exactly?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#uncertainty-and-variability",
    "href": "ch-inference.html#uncertainty-and-variability",
    "title": "6  Inference",
    "section": "",
    "text": "TipUncertainty and variability\n\n\n\n\nUncertainty is a characteristic of each observation of a phenomenon, due to measurement error or because we cannot directly measure what we want to measure.\nVariability is found among different observations of the same phenomenon, due to natural fluctuations and measurement error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipStatistics\n\n\n\nStatistics is a tool that helps us quantifying uncertainty and controlling for variability.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#what-is-statistics-and-isnt",
    "href": "ch-inference.html#what-is-statistics-and-isnt",
    "title": "6  Inference",
    "section": "6.2 What is statistics (and isn’t)?",
    "text": "6.2 What is statistics (and isn’t)?\nStatistics is a tool. But what does it do? There are at least four ways of looking at statistics as a tool.\n\nStatistics is the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data. (From UCI Department of Statistics)\nStatistics is the technology of extracting information, illumination and understanding from data, often in the face of uncertainty. (From the British Academy)\nStatistics is a mathematical and conceptual discipline that focuses on the relation between data and hypotheses. (From the Standford Encyclopedia of Philosophy)\nStatistics is the art of applying the science of scientific methods. (From ORI Results, Nature)\n\nTo quote a historically important statistician:\n\n\n\n\n\n\nStatistic is both a science and an art.\nIt is a science in that its methods are basically systematic and have general application and an art in that their successful application depends, to a considerable degree, on the skill and special experience of the statistician, and on his knowledge of the field of application.\n—L. H. C. Tippett\n\n\n\n\n\n\n\n\n\nImportantSpotlight: Etymology\n\n\n\n\n\nThe word statistics is related to state and it is no coincidence. The discipline of statistics was born as a sub-field of politics and economics. Check out the full etymology of statistics here: https://en.wiktionary.org/wiki/statistics#Etymology_1.\n\n\n\nStatistics is a many things, but it is also not a lot of things.\n\nStatistics is not maths, but it is informed by maths.\nStatistics is not about hard truths not how to seek the truth.\nStatistics is not a purely objective endeavour. In fact there are a lot of subjective aspects to statistics (see below).\nStatistics is not a substitute of common sense and expert knowledge.\nStatistics is not just about \\(p\\)-values and significance testing.\n\nAs Gollum would put it, all that glisters is not gold.\n\n\n\n\n\n\n\n\n\n\n\nNoteQuiz 2\n\n\n\nTrue or false?\n\nStatistics is necessary if one wants to know the truth. TRUEFALSE\nStatistics is only relevant to objective science. TRUEFALSE\nStatistics is based on mathematics but it is also informed by philosophy. TRUEFALSE\nWe can completely remove uncertainty with statistics. TRUEFALSE",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#many-analysts-one-data-set-subjectivity-exposed",
    "href": "ch-inference.html#many-analysts-one-data-set-subjectivity-exposed",
    "title": "6  Inference",
    "section": "6.3 Many Analysts, One Data Set: subjectivity exposed",
    "text": "6.3 Many Analysts, One Data Set: subjectivity exposed\nIn Silberzahn et al. (2018), a group of researchers asked 29 independent analysis teams to answer the following question based on provided data: Is there a link between player skin tone and number of red cards in soccer? Crucially, 69% of the teams reported an effect of player skin tone, and 31% did not. In total, the 29 teams came up with 21 unique types of statistical analysis. These results clearly show how subjective statistics is and how even a straightforward question can lead to a multitude of answers. To put it in Silberzah et al’s words: “The observed results from analyzing a complex data set can be highly contingent on justifiable, but subjective, analytic decisions. This is why you should always be somewhat sceptical of the results of any single study: you never know what results might have been found if another research team did the study. This is a reason for why replicating research is very important. You will learn about replication and related concepts in Chapter 17.\nCoretta et al. (2023) tried something similar, but in the context of the speech sciences: they asked 30 independent analysis teams (84 signed up, 46 submitted an analysis, 30 submitted usable analyses) to answer the question: Do speakers acoustically modify utterances to signal atypical word combinations? Outstandingly, the 30 teams submitted 109 individual analyses—a bit more than 3 analyses per team!—and 52 unique measurement specifications in 47 unique model specifications. Coretta et al. (2023) say: “Nine teams out of the thirty (30%) reported to have found at least one statistically reliable effect (based on the inferential criteria they specified). Of the 170 critical model coefficients, 37 were claimed to show a statistically reliable effect (21.8%).” Figure 6.2 illustrates the analytic flexibility typical of acoustic analyses. (A) shows the pipeline of decision a researcher would have to do: which linguistic unit, which temporal window, which acoustic parameters and how to measure those. You can appreciate that there are potentially many combinations. (B) illustrates the fundamental frequency (f0) contour of the sentences “I can’t bear ANOTHER meeting on Zoom” and “I can’t bear another meeting on ZOOM”. In both sentences, the green shaded area marks the word “another”. Finally, in (C) you see the different parameters that can be extracted from the f0 contour of the word “another”. In sum, there are many choices a researcher is faced with and, while most of these choices might be justifiable, they are still subjective, as shows by the large variability of actual analyses carried out by the analyses teams in Coretta et al. (2023).\n\n\n\n\n\n\nFigure 6.2: Illustration of the analytic flexibility associated with acoustic analyses (from Coretta et al. 2023)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#the-new-statistics",
    "href": "ch-inference.html#the-new-statistics",
    "title": "6  Inference",
    "section": "6.4 The “New Statistics”",
    "text": "6.4 The “New Statistics”\nThe Silberzahn et al. (2018) and Coretta et al. (2023) studies are just the tip of the iceberg. We are currently facing a “research crisis”. As mentioned above, we will dig deeper into this subject in Chapter 17. In brief, the research crisis is a mix of problems related to how research is conducted and published. In response to the research crisis, Cumming (2013) introduced a new approach to statistics, which he calls the “New Statistics”. The New Statistics mainly addresses three problems: (1) published research is a biased selection of all (existing and possible) research; (2) data analysis and reporting are often selective and biased, (3) in many research fields, studies are rarely replicated, so false conclusions persist. To help solve those problems, the New Statistics proposes these solutions (among others): (1) promoting research integrity, by which researchers explicitly discuss the subjectivity and shortcomings of quantitative research, (2) shifting away from statistical significance of differences between groups to quantitative estimation of those differences, (3) building a cumulative quantitative discipline, in which phenomena are studied again and again in the same contexts and with the same conditions to ensure they are robust enough.\nKruschke and Liddell (2018) revisit the New Statistics and make a further proposal: to adopt the historically older but only recently popularised approach of Bayesian statistics. They call this the Bayesian New Statistics. The classical approach to statistics is the frequentist method, based on work by Fisher, Neyman and Pearson. Put simply, frequentist statistics is based on rejecting the “null hypothesis” (i.e. the hypothesis that there is no difference between groups) using p-values. Bayesian statistics provides researchers with more appropriate and more robust ways to answer research questions, by reallocating belief or credibility across possibilities. You will learn more about the frequentist and the Bayesian approaches in Chapter 20.\nThis textbook adopts the Bayesian New Statistics approach. Note that we will not really touch upon Bayesian statistics in the strict sense until Chapter 20, just before statistical modelling will be introduced. So you should not worry too much about it for now: just try to appreciate that not only statistics is not intended to objectively separate truths from falsities, but also there are several ways to practice statistics. After all, statistics is a human activity, and like all other human activities it is embedded in the world constructed by humans and their idiosyncrasies.\n\n\n\n\n\n\nNoteQuiz 3\n\n\n\nThree researchers meet at a coffee shop. Each of them tells the other two about their recent findings. Below, you can find what each said. Based on how they talk about the results, which one among them aligns with the New Statistics approach and recognises the shortcomings of research?\n\n Researcher A. My team investigated the effect of emotional dysregulation on speaking rate and they found a significant effect. We have ultimately shown that people with emotional dysregulation speak faster. Researcher B. I wanted to know if it is true that languages with morphologically rich grammars are more difficult to learn than isolating languages. We tested several measures of learning difficulty in two groups of infants, one learning a morphologically rich language and one learning an isolating language. We found that two measures were significantly higher for the morphologically rich language group than the isolating language group. Hence we have found solid evidence that morphologically rich grammars are more difficult to learn. Researcher C. We compared reaction times (RTs) of chimpanzees looking at videos of humans vs chimpanzees signing. If low-level motor perception is mostly affected by the conspecificity (human vs conspecific), we should see differences in RTs of 20-70 ms. If motor resonance is mostly affected (activation of the observer’s own motor system when seeing an action they could perform themselves), we should find differences in RTs of 80-200 ms. We found that in the conspecific condition, RTs were 74-98 ms shorter at 95% probability. This range mostly overlaps with the motor resonance hypothesis (80-200) but it also lies outside of it, somewhat close to the higher end of the low-level perception range (20-70). In sum, we could not establish which hypothesis could better explain the data.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#summary",
    "href": "ch-inference.html#summary",
    "title": "6  Inference",
    "section": "6.5 Summary",
    "text": "6.5 Summary\n\n\n\n\n\n\n\nInference is the process of learning something about a population through a sample.\nUncertainty in each observation and variability across observations affect the inference process.\nStatistics is a tool to quantify uncertainty and variability.\nThe (Bayesian) New Statistics is an approach to statistics that highlights the subjective nature of statistics and stressed the importance of estimation over statistical significance.\n\n\n\n\n\n\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke, Byron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023. “Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses.” Advances in Methods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nCumming, Geoff. 2013. “The New Statistics: Why and How.” Psychological Science 25 (1): 729. https://doi.org/10.1177/0956797613504966.\n\n\nGelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and Objective in Statistics.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 180 (4): 9671033. https://doi.org/10.1111/rssa.12276.\n\n\nKruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin & Review 25 (1): 178206. https://doi.org/10.3758/s13423-016-1221-4.\n\n\nSilberzahn, Raphael, Eric L. Uhlmann, Daniel P. Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahník, Feng Bai, Colin Bannard, and Evelina Bonnier. 2018. “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.” Advances in Methods and Practices in Psychological Science 1 (3): 337356. https://doi.org/10.1177/2515245917747646.\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. “How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis.” Linguistics 59 (5): 13111342. https://doi.org/10.1515/ling-2019-0051.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html",
    "href": "ch-scripts.html",
    "title": "7  R scripts",
    "section": "",
    "text": "7.1 Create an R script\nIn Chapter 4 and Chapter 5, you’ve been writing R code in the Console and run it there. But this is not a very efficient way of using R code. Every time, you need to write the code and execute it in the right order and it quickly becomes very difficult to keep track of everything when things start getting more involved. A solution is to use R scripts.\nFrom now on, you should write all code in an R script, until you will learn about Quarto documents in Chapter 13.\nFirst, create a folder called code in your Quarto project folder. You can do so in two different ways:\nThe code/ folder will be the folder where you will save all of your R scripts and other documents.\nNow, to create a new R script, look at the top-left corner of RStudio: the first button to the left looks like a white sheet with a green plus sign. This is the New file button. Click on that and you will see a few options to create a new file.\nClick on R Script. A new empty R script will be created and will open in the File Editor window of RStudio.\nNote that creating an R script does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\nSave the file inside the code/ folder with exactly the following name: week-02.R.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#create-an-r-script",
    "href": "ch-scripts.html#create-an-r-script",
    "title": "7  R scripts",
    "section": "",
    "text": "You can click on the New Folder button in the Files panel (bottom-right) in RStudio, set the name and click Ok. The folder will be created within the current folder shown in the Files list.\nSince Quarto Projects are just folders on your computer, you can create a new folder as you would with any other folder from your computer File Explorer/Finder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\nSo you can always access them from the Finder or File Explorer! However, do not open a file by double clicking on it from the Finder/File Explorer.\nRather, open the Quarto project by double clicking on the .Rproj file and then open files from RStudio to ensure you are working within the RStudio project and the working directory is set correctly.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#write-code",
    "href": "ch-scripts.html#write-code",
    "title": "7  R scripts",
    "section": "7.2 Write code",
    "text": "7.2 Write code\nNow, let’s start filling up that script! Generally, you start the script with calls to library() to load all the packages you need for the script. Please, get in the habit of doing this from now, so that you can keep your scripts tidy and pretty! You will learn soon about the tidyverse packages in the following chapter so for now just attach the cowsay and fortune packages.\n\n\n\n\n\n\nImportant\n\n\n\nStart your R scripts with calls to library() and attach all of the packages that are needed to run that script.\n\n\nGo ahead, write the following code in the top of the .R script. (The code chunk has a convenient copy button in the top-right corner which appears when you place the cursor inside the chunk. If you click the button the code will be copied and you can then paste it in the script).\n\nlibrary(cowsay)\nlibrary(fortunes)\n\nsay(\"fortune\", \"monkey\")\nsay(\"What a lovely day for a wedding\", \"spider\")\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease, don’t include install.packages() in your R scripts!\nRemember, you only have to install a package once, and you can just type it in the Console.\nBut DO include library() calls at the top of your scripts.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#running-scripts",
    "href": "ch-scripts.html#running-scripts",
    "title": "7  R scripts",
    "section": "7.3 Running scripts",
    "text": "7.3 Running scripts\nFinally, the time has come to run the script.\nThere are several ways of doing this. The most straightforward is to click on the Run button. You can find this in the top-right corner of the script window. Pressing Run will run the line of code your text cursor is currently on. So you should place the cursor back on line one and press Run. The code will be executed and you will see it in the Console. If the code returns any output, this will be shown in the Console too. After the line of code is executed, the text cursor moves to the next line. You can click on Run again and so on to run each line one by one. You can also just select all the code (like you would when selecting text in a text editor) and click Run: in this case, all of the code is run, line by line, in the order they appear in the script.\n\nAn alternative way is to place the text cursor on the line of code you want to run and then press CMD+ENTER/CTRL+ENTER. As with clicking Run, this will run the line of code and move the text cursor to the next line of code. It also works with a selection, like the Run button. Now that you know how to use R scripts and run code in them, I will assume that you will keep writing new code in your script and run it from there.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#comments",
    "href": "ch-scripts.html#comments",
    "title": "7  R scripts",
    "section": "7.4 Comments",
    "text": "7.4 Comments\nSometimes we might want to add a few lines of text in our script, for example to take notes. You can add so-called comments in R scripts, simply by starting a line with #. You can also add trailing comments, by adding a # at the end of a line of R code. For example:\n\n# This is a comment. Let's add 6 + 3.\n6 + 3\n\n[1] 9\n\n3 + 6 # This is a trailing comment. 6 + 3 = 3 + 6\n\n[1] 9\n\n\n\n\n\n\n\n\nTipCode comments\n\n\n\nText that starts with a hash symbol # in an R script is a comment. Comments are not executed.\n\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nIs the following a valid line of R code? TRUEFALSE\nsum(x + 2) # x = 4\n\n\n\n\n\n\n\n\nImportantExplanation\n\n\n\n\n\nIt is a valid line of R code with a trailing comment. If you tried to run it in the Console and got an error it is because the variable x does not exist (unless you had created one earlier). If you add the line x &lt;- 4 before sum(x + 2) # x = 4, the latter will work just fine.\nSo you see there is a difference between valid code and working code.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#ensuring-the-script-runs",
    "href": "ch-scripts.html#ensuring-the-script-runs",
    "title": "7  R scripts",
    "section": "7.5 Ensuring the script runs",
    "text": "7.5 Ensuring the script runs\nThat’s all there is to know about using R scripts. You write code and some comments and you can run code in the script and see the output in the Console. However, there is an important aspect that was not explicitly mentioned above: a script is supposed to work from top (first line) to bottom (last line), so the order of the code in the script matters. A good habit to get into is to restart the R session every now and then and re-run the entire script. To restart the R session you can either go to the Session menu &gt; Restart R or you can press SHIFT+CMD/CTRL+0 (the last key is “zero”). Try this now. Restart the R session and run your script again.\nBut why it is important to restart the session to verify that the script runs? A typical case of scripts that don’t run is when you call a variable in a function before having declared the variable (with &lt;-) or when you call a function without having attached the package the function is from. However, an R session remembers everything you run: if you try to run code with a non-declared variable (like sum(a, 1), but a is not declared) you will get an error; if you now write the code that declares the variable (a &lt;- 3) but you put it after the line of code that uses the variable, the code will run because now the variable is declares and available in the session. If you keep the code this way and restart the session, the code will no longer work. This is because each line is executed in order and by the time R gets to the sum(a, 1) , the line a &lt;- 3 hasn’t been executed yet so a is not available. This example might seem trivial (and it is) but with more complex scripts it is actually quite easy to do things like this (calling a variable on line 10 of the script while it is declared on line 1263).\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nCreate a new script and call it week–02-ex7.1.R (save it in code/). Copy the following code and try to run it. The script will not run because there are several errors: some are code errors (i.e. the code is wrong), others are because the code is not written in the right order. Fix the errors until the script runs correctly. Remember to restart the session!\nlibery(fortunes)\n\na -&gt; 3\nsum(a, b)\nb &lt;- 10\n\n#\nLet's print a fortune\nfotrune(c)\nc &lt;- a + b",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html",
    "href": "ch-statistical-vars.html",
    "title": "8  Statistical variables",
    "section": "",
    "text": "8.1 Estimandum, estimands and statistical variables\nStatistical variables are a fundamental aspect of quantitative data analysis. There isn’t an agreed upon definition of statistical variable, but generally speaking, anything that you have measured or counted is a statistical variable. For example, let’s say you want to measure language proficiency in L2 learners: “language proficiency” is your estimandum, i.e. the concept or entity you wish to measure; you decide to measure language proficiency using the score of a proficiency test, this is the estimand, i.e. the specific measurement of the estimandum “language proficiency”. When the estimand can take on different values, the estimand is a statistical variable: every participant will have a different proficiency score.\nLanguage research involves a large variety of statistical variables. Here just a few examples:\nTry and think of more!\nSo a statistical variable is a measured characteristic. More specifically, a statistical variable is also a mathematical construct: the outcome of the specific mathematical process that generates the values that can be observed and measured. In the case of language proficiency, the statistical variable “proficiency test score” is generated by a process that includes a lot of factors (which can themselves be construed as statistical variables), like actual proficiency, stress levels when taking the test, baseline memory capacity, years of learning and so on. The generative process, i.e. the process that generates the values of a statistical variable, is ultimately what the researcher is interested in.\nWhen you observe or measure something, i.e. when you collect a sample, you are taking note of the values of the statistical variable generated by the generative process. We call them statistical variables because each time you sample the variable, you get different values. In other words, the generative process allows for variation in the output values. The opposite of a variable is a called a statistical constant. Generative processes can contain both variables and constants. Statistical variables and constants are two types of estimands. In practice, you don’t have to worry about whether something is a variable or a constant and in most research contexts you will be working with statistical variables.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html#estimandum-estimands-and-statistical-variables",
    "href": "ch-statistical-vars.html#estimandum-estimands-and-statistical-variables",
    "title": "8  Statistical variables",
    "section": "",
    "text": "TipEstimandum, estimands and variable\n\n\n\nAn estimandum is any characteristic, phenomenon, entity, concept that is the target of the measurement/counting process.\nAn estimand is the specific quantity of an estimandum that can be measured.\nA (statistical) variable is any estimand or characteristics, number, or quantity that has been measured or counted and can vary.\n\n\n\n\nToken number of telic verbs and atelic verbs in a corpus of written Sanskrit.\nVoice Onset Time of stops in Mapudungun.\nFriendliness ratings of synthetic speech.\nAccuracy of responses in a lexical decision task.\nDigit memory span.\nPhrasal headedness (head-initial vs head-final).\n\n\n\n\n\n\n\n\n\nTipGenerative process\n\n\n\nThe generative process of an estimand is the mathematical process that generates the values of the estimand that are observed or measured.\n\n\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nTrue or false?\n\nThe estimandum refers to the specific measurable quantity of a concept or entity. TRUEFALSE\nThe generative process comprises only statistical variables. TRUEFALSE\nA statistical variable is defined as any measurable or countable entity that can vary in value. TRUEFALSE",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html#types-of-variables",
    "href": "ch-statistical-vars.html#types-of-variables",
    "title": "8  Statistical variables",
    "section": "8.2 Types of variables",
    "text": "8.2 Types of variables\nYou will find that some statistics textbooks overcomplicate things when it come to types of statistical variables. From an applied statistics perspective, you only need to be able to identify numeric vs categorical variables and continuous vs discrete variables.\n\n8.2.1 Numeric vs categorical variables\n\n\n\n\n\nThe distinction is quite self-explanatory:\n\nNumeric variables are variables that are numbers.\nCategorical variables are variables that correspond to categories, groups or levels on a scale.\n\n\n\n\n\n\n\nNoteExamples\n\n\n\nNumeric variables\n\nNumber of multi-verb predicates in a book.\nDuration of stressed vowels.\nRating score between 0-100.\n\nCategorical variables\n\nGender (non-binary, female, male, …).\nFirst vs second language users.\nEjective vs non-ejective consonant.\n\n\n\nLearning how to recognise variables is a fundamental skill in quantitative data analysis, since the type of variables determines the type of analyses you can carry out.\n\n\n8.2.2 Continuous vs discrete variables\n\n\n\n\n\nOrthogonal to the numeric/categorical distinction, there is the continuous vs discrete distinction. This one can be at times less straightforward.\n\nA continuous variable is a variable that can take on any value between any two numbers. For example, speech segment duration can be 0.2 s, 0.25 s, 0.2534 s and so on. Segment duration is continuous.\nA discrete variable is a variable that can only take on a set of values, and no value in between. For example, number of gestures is discrete because you can measure 1, 2, 3, 10 gestures but not 3 gestures and three quarters.\n\nNumeric variables can be either continuous or discrete, while categorical variables can only be discrete. There are also sub-types of numeric continuous, numeric discrete and categorical (discrete) variables. The following call-out introduces these sub-types, with examples.\n\n\n\n\n\n\nTipTypes of variables\n\n\n\nNumeric continuous variable: between any two values there is an infinite number of values.\n\nThe variable can take on any positive and negative number, including 0. For example, temperature in degrees Celsius.\nThe variable can take on any positive number only. For example, segment duration, fundamental frequency (f0), reaction times.\nProportions and percentages: The variable can take on any number between 0 and 1. For example, proportion of accurate responses, probability of scalar inference, proportion of voicing during stop closure, acceptability rating on a 0-100 scale.\n\nNumeric discrete variable: between any two consecutive values there are no other values.\n\nCounts: The variable can take only on any positive integer number. For example, number of telic and atelic verbs in a corpus, number of words known by a child, number of turns in a conversation.\n\nCategorical (discrete) variable. There are three main subtypes.\n\nBinary or dichotomous: The variable can take only one of two values. For example, accuracy (incorrect, correct), voicing (voiceless, voiced), headedness (initial vs final).\nThe variable can take any of three of more values (sometimes called a multinomial variable). For example, gender (non-binary, female, male), place of articulation (labial, coronal, dorsal, glottal, …).\nOrdinal: The variable can take any of three of more values and the values have a natural order. For example, Likert scales of attitude (positive, indifferent, negative), proficiency (functional, good, very good, native-like), lenition (stop, fricative, approximant, deletion).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html#sec-operationalise",
    "href": "ch-statistical-vars.html#sec-operationalise",
    "title": "8  Statistical variables",
    "section": "8.3 Operationalisation",
    "text": "8.3 Operationalisation\nIt should be clear now that the estimand is not quite the same thing as the estimandum. The estimand is the researcher’s way to capture the estimandum so that it can be analysed. The relationship between the estimandum and the estimand variable is called operationalisation: an estimandum is operationalised into an estimand. The action of operationalisation consists in choosing how to measure something: as a numeric or as a categorical variable. In some cases, the choice is obvious, but in most cases something could be operationalised either way and different considerations have to be taken into account when choosing, like the particular framework adopted and the study design.\nLet’s think about “age” for a moment: age can be operationalised as years or months (numeric discrete) or as age bins, like young vs old (categorical). Different studies might require one or the other operationalisation of the estimandum “age”. Another example is “acceptability” in morphosyntactic studies: acceptability can be operationalised as a binary categorical variable (grammatical vs agrammatical, and we normally talk of “grammaticality”), as a categorical scale (acceptable, somewhat acceptable, somewhat not acceptable, unacceptable), or a numeric continuous scale (0 to 100). It is important, when planning a study, to carefully think about estimanda (the plural of estimandum) and estimands and how their relationship could be less clear than one might think.\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nThink of all the ways to operationalise the following variables:\n\nVoice Onset Time.\nFriendliness of speech.\nLexical frequency.\n\n\n\n\n\n\n\n\n\nNoteQuiz 2\n\n\n\nWhich of the following sets contains only discrete variables.\n\n Number of occurences in corpus, sentence duration (ms), articulation rate (syllables per second) Reaction times (ms), accuracy (correct/incorrect), 7-point likert scale Accuracy (correct/incorrect), 7-point likert scale, number of occurrences in corpus Fundamental frequency (hz), response accuracy (percentage), reaction times (ms)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html",
    "href": "ch-read-data.html",
    "title": "9  Read data in R",
    "section": "",
    "text": "9.1 Tabular data\nData comes in a lot of different formats, shape and sizes. However, the most common way to store data used in quantitative analysis is so-called tabular data. R is especially designed to work with such data. Tabular (aka rectangular) data is simply data in the form of a table, with columns and rows.\nTabular data can be saved in different file formats. Different file formats have different file extensions. The comma separated values format (file extension .csv) is the best format to save data in because it is basically a plain text file, it’s quick to parse, and can be opened and edited with any software (plus, it’s not a proprietary format like .docx or .xlsx—these formats are specific to particular commercial software).\nThis is what a .csv file looks like when you open it in a text editor (showing only the first few lines). The file contains tabular data (data that is structured as columns and rows, like a spreadsheet).\nThis is what the file would look like when layed out as a table.\nTo separate the values of each column, a .csv file uses a comma , (hence the name “comma separated values”) to separate the values in every row. The first line of the file indicates the names of the columns of the table:\nThere are 11 columns. The rest of the rows is the data, i.e. the values of each column separated by commas.\nThis might look a bit confusing, but you will see later that, after importing this type of file, you can view it as a nice spreadsheet (as you would in Excel), like in the figure above.\nAnother common type of tabular data file is spreadsheets, like spreadsheets created by Microsoft Excel or Apple Numbers. These are all proprietary formats that require you to have the software that were created with if you want to modify them. Portability and openness are important aspects of conducting research, so that using open and non-proprietary file types makes your research more accessible and doesn’t privilege those who have access to specific software (remember, R is free!). Despite of this, a lot of data is shared as Excel files.\nThere are also variations of the comma separated values type, like tab separated values files (.tsv, which uses tab characters instead of commas) and fixed-width files (usually .txt, where columns are separated by as many white spaces as needed so that the columns align).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#tabular-data",
    "href": "ch-read-data.html#tabular-data",
    "title": "9  Read data in R",
    "section": "",
    "text": "WarningImportant\n\n\n\nWhen working through the book, always make sure you are in a Quarto Project by checking the top-right corner of RStudio. If you see the name of the project you are fine, if you see Project (none) then you are not in the Quarto Project. Close RStudio and open the Quarto project.\n\n\n\n\n\n\n\n\n\nTipTabular data\n\n\n\nTabular data is data that has a form of a table: i.e. values structured in columns and rows.\n\n\n\n\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\n\n\n\n\n\n\n\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\n\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\n\n\n\n\n9.1.1 Non-tabular data\nOf course, R can import also data that is not tabular, like map data and complex hierarchical data, including XML, HTML and json data. We will not cover these types of data, but you can check out the resources in the Extra box.\n\n\n\n\n\n\nImportantR Note: Non-tabular data\n\n\n\n\n\n\nSee Chapters 21-24 of R for Data Science.\nLook up the sf package for mapping.\n\n\n\n\n\n\n9.1.2 .rds files\nR has a special way of saving data: .rds files. .rds files allow you to save an R object to a file on your computer, so that you can read that file back in when you need it. A common use for .rds files is to save tabular data that you have processed so that it can be readily used in many different scripts or even by other people, but .rds files can contain any type of R objects, also lists (so not only tabular data). In the following sections you will learn how to import (aka read) three types of data: .csv, Excel and .rds files.\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhich of the following is not tabular data.\n\n a. A file with 3 columns and 100 rows. b. An HTML file. c. An Excel spreadsheet.\n\nNon-tabular data can be saved to .rds files. TRUEFALSE",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#get-the-data",
    "href": "ch-read-data.html#get-the-data",
    "title": "9  Read data in R",
    "section": "9.2 Get the data",
    "text": "9.2 Get the data\nThe data used in this textbook come from a variety of published and unpublished linguistic studies. You can download the data files from the QML Data website according to the following instructions.\n\n\n\n\n\n\nWarningHow to get the data\n\n\n\n\nDownload the zip archive with all the data by clicking on the following link (if this doesn’t work, right-click and choose “Save linked file” or similar): data.zip. The data is in a zip archive.\nUnzip the zip file to extract the contents. (If you don’t know how to do this, search for it online for your operating system! Zip archives are a very common way of distributing data and it is important to know how to use them).\nCreate a folder called data/ (the slash is there just to remind you that it’s a folder, but you don’t have to include it in the name) in the Quarto project you are using for the course. You know how to do this from Chapter 7.\nMove the contents of the data.zip archive into the data/ folder.\n\nOpen a Finder or File Explorer window.\nNavigate to the folder where you have extracted the zip file (it will very likely be the Downloads/ folder).\nCopy the contents of the zip file.\nIn Finder or File Explorer, navigate to the Quarto project folder, then the data/ folder, and paste the contents in there. (You can also drag and drop if you prefer.)\n\n\n\n\nThe rest of this chapter will assume that you have created a folder called data/ in the Quarto project folder and that the files you downloaded are in that folder. The data folder should like something like this:\ndata/\n└── cameron2020/\n    └── gestures.csv\n└── coretta2018/\n    └── formants.csv\n    └── token-measures.csv\n└── ...\nI recommend that you start being very organised with your files in other projects from now on, whether it’s for a course or your dissertation or anything else. I also suggest to avoid overly nested structures (folders in folders in folders in folders…), unless strictly necessary.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#organising-your-files",
    "href": "ch-read-data.html#organising-your-files",
    "title": "9  Read data in R",
    "section": "9.3 Organising your files",
    "text": "9.3 Organising your files\nThe Open Science Framework has the following recommendations that can be applied to any type of research project.\n\nUse one folder per project. The project folder will also be your RStudio/Quarto project folder. Ideally, the project folder should have all the files related to the project (one exception is PDFs of papers that form the literature background of the project: for those I recommend using bibliography managing software, like the free Zotero or JabRef).\nSeparate code from data. A general recommendation is to have a folder code/ or scripts/ with all the code files of the project and a folder data/ that has all the data. This makes keeping files in order easier, since everything has its natural place.\nSeparate raw data from derived data. Raw data is data that you have gathered that, if lost, is lost for ever. Derived data is any data that is derived from raw data and that can be derived again (for example by running a script) if it’s deleted or corrupted.\nMake raw data read-only. You should assume that anything can happen to raw data, so you should treat it as “read-only”.\n\nTo summarise, these recommendations suggest to have a folder for your research project/course/else, and inside the folder two more folders: one for data and one for code. The data/ folder could further contain raw/ for raw data (data that should not be lost or changed, for example collected data or annotations) and derived/ for data that derives from the raw data, for example through automated data processing.\nIt might be useful to also have a separate folder called figs/ or img/ to save figures and plots. Of course which folders you will have it’s ultimately up to you and needs will vary depending on the nature and practical aspects of each study.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#read-.csv-files",
    "href": "ch-read-data.html#read-.csv-files",
    "title": "9  Read data in R",
    "section": "9.4 Read .csv files",
    "text": "9.4 Read .csv files\nIn this section, you will learn how to read .csv files. Reading .csv files is very easy. You can use the read_csv() function from a collection of R packages known as the tidyverse. Specifically, the read_csv() function is from the readr package, one of the tidyverse packages. If you are learning R for the first time, then you won’t already have the tidyverse packages installed (you can check in the Packages tab in the bottom-right panel). Installing the tidyverse packages is easy: you just need to install the tidyverse package and that will take care of installing the most important packages in the collection (called the “core” tidyverse packages). Note that installation of the core tidyverse packages can take some time (but remember that you do this only once). If you need to install the tidyverse packages, do it now.\n\n\n\n\n\n\nImportantDid you open the Quarto project?\n\n\n\nBefore moving on, make sure that you have opened the RStudio Quarto project correctly (see warning at the beginning of the chapter).\n\n\nNow that you have ensured the tidyverse packages are available, let’s read in data from Song et al. (2020). The study consists of a lexical decision task in which participants were first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word. The prime word belonged to one of three possible groups, each of which refers to the morphological relation of the prime and the target word. We will get back to this data in later chapters, so for now it is sufficient if you just read the paper’s abstract to get a general idea of the research context.\nThe read_csv() function from the readr package only requires you to specify the file path as a string (remember, strings are quoted between \" \", for example \"year_data.txt\"). The data to be read are in the data/ folder, in song2020/shallow.csv. On my computer, the file path of song2020/shallow.csv is /Users/ste/qdal/data/song2020/shallow.csv, but on your computer the file path will be different, of course. However, you will learn a trick below, i.e. relative paths, that allows you to specify file paths in a shortened form.\n\nNote that while the read_csv() function does read the data in R, you must assign the output of the read_csv() function (i.e. the data we are reading) to a variable, using the assignment arrow &lt;-, just like we were assigning values to R variables in previous chapters. And since the read_csv() is a function from the tidyverse, you first need to attach the tidyverse packages with library(tidyverse) (remember, you need to attach packages only once per session). This will attach the core tidyverse packages, including readr. Of course, you can also attach the individual packages directly: library(readr). If you use library(tidyverse) there is no need to attach individual tidyverse packages.\nOpen your week-02.R script. Add the following lines in the script (don’t change the file path! explanation below) and run the code (you might want to put the library() line at the top of the script, with the other packages). The read_csv() line will print information about the data and read the data into shallow.\n\nlibrary(tidyverse)\n\nshallow &lt;- read_csv(\"./data/song2020/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you look at the Environment tab, you will see shallow listed under Data. You can preview the data by clicking on the name of the data in the Environment tab. A View tab will be opened in the top-left panel of RStudio and you will see a nicely formatted table, as you would in a programme like Excel. We will dive into this data later, so just have a peak for now.\n\n\n\n\n\n\nTipData frames and tibbles\n\n\n\nIn R, a data table is called a data frame.\nTibbles are special data frame created with the read functions from the tidyverse. If you are curious about the difference, check this page.\nIn this textbook, “data frame” and “tibble” will be used interchangeably (since we are using the read functions from the tidyverse, all resulting data frames will be tibbles).\n\n\nBut wait, what is that \"./data/song2020/shallow.csv\"? That’s a relative path. Let’s understand the concept of relative paths now.\n\n9.4.1 Relative paths\nFile paths can be specified in two formats. One format is called absolute file path. An absolute file path include all folders from the top-most folder, which is normally your computer’s hard drive. For example, /Users/ste/qdal/data/song2020/shallow.csv from above is an absolute path. You know it’s an absolute path because it starts with the forward slash /. This means that there isn’t anything above Users/: it’s the top-most folder. A downside of absolute paths is that they are not portable: if I move the qdal/ folder to ste/Documents then I need to change every occurrence in my scripts to /Users/ste/Documents/qdal/data/song2020/shallow.csv. Moreover, when you share your research code (and you should!), using absolute paths means that each person that wants to run the code has to update the absolute path to reflect their own.\nA solution is to use relative paths. Relative paths work by including the path only from within a specific folder. Whichever folders contain that specific folder do not matter. The specific folder is called the working directory. When you are using Quarto projects, the working directory is the project folder, i.e. the folder with the .Rproj and _quarto.yml files.\n\n\n\n\n\n\nTipWorking directory\n\n\n\nThe working directory is the folder which relative paths are relative to.\nWhen using Quarto projects, the working directory is the project folder.\n\n\nRelative paths are specified by starting the path with ./. For example, if your project is called awesome_proj and it’s in Downloads/stuff/, then if you write read_csv(\"./data/results.csv\") R knows you mean to read the file in Downloads/stuff/awesome_proj/data/results.csv! This works because when working with Quarto projects, all relative paths are relative to the working directory which is automatically set to the project folder.\n\n\n\n\n\n\nTipRelative path\n\n\n\nA relative path is a file path that is relative to a folder (the working directory). The folder the path starts at is represented by ./.\n\n\nThe code read_csv(\"./data/song2020/shallow.csv\") above will work because you are using a Quarto project and inside the project folder there is a folder called data/ and in it there’s the song2020/shallow.csv file. When you run the code, R will “expand” the relative path to the absolute path and correctly find the file to read. I strongly recommend you to use Quarto projects and relative paths to make your work portable. As hinted at above, the benefit of Quarto projects and relative paths is that, if you move your project or rename it, or if you share the project with somebody, all the paths will just work because they are relative.\n\n\n\n\n\n\nWarningExercise 1: Get the working directory\n\n\n\nYou can get the current working directory with the getwd() command.\nRun it now in the Console! Is the returned path the project folder path?\nIf not, it might be that you are not working from a Quarto project. Check the top-right corner of RStudio: is the project name in there or do you see Project (none)?\nIf it’s the latter, you are not in a Quarto project, but you are running R from somewhere else (meaning, the working directory is somewhere else). If so, close RStudio and open the project.\n\n\n\n\n\n\n\n\nNoteQuiz 2\n\n\n\n\nGiven the following absolute path /Users/raj/projects/thesis/data/raw/data.csv and the working directory /Users/raj/projects/, which of the following paths is the correct one to read the data.csv file?\n\n a. /thesis/data/raw/data.csv b. ./projects/thesis/data/raw/data.csv c. ./data/raw/data.csv d. ./thesis/data/raw/data.csv",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#read-excel-sheets",
    "href": "ch-read-data.html#read-excel-sheets",
    "title": "9  Read data in R",
    "section": "9.5 Read Excel sheets",
    "text": "9.5 Read Excel sheets\nTo read an Excel file we need first to attach the readxl package. It should already be installed, because it comes with the tidyverse. If not, install it. Then add the following line to the script.\n\nlibrary(readxl)\n\nNow we can use the read_excel() function. Let’s read the file.\n\nrelatives &lt;- read_excel(\"./data/los2023/relatives.xlsx\")\n\nNow you can view the tibble relatives in the RStudio Viewer. Note that if the Excel file has more than one sheet, you can specify the sheet number when reading the file (the default is sheet = 1).\n\nrelatives_2 &lt;- read_excel(\"./data/los2023/relatives.xlsx\", sheet = 2)\n\nThe second sheet in los2023/relatives.xlx contains the description of the columns in the first sheet.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#import-.rds-files",
    "href": "ch-read-data.html#import-.rds-files",
    "title": "9  Read data in R",
    "section": "9.6 Import .rds files",
    "text": "9.6 Import .rds files\nAnother useful type of data files is a file type specifically designed for R: .rds files. Each .rds file can only contain a single R object, like a tibble. You can read .rds files with the readRDS() function.\n\nglot_status &lt;- readRDS(\"./data/coretta2022/glot_status.rds\")\n\nAs always, you need to assign the output of the function to a variable, here glot_status.\n\n\n\n\n\n\nTip.rds files\n\n\n\n.rds files are a type of R file which can store any R object and save it on disk.\nR objects can be saved to an .rds file with the saveRDS() function and they can be read with the readRDS() function.\n\n\nView the glot_status tibble now. It is also very easy to save a tibble to an .rds file with the saveRDS() function. For example:\n\nsaveRDS(shallow, \"./data/song2020/shallow.rds\")\n\nThe first argument is the name of the tibble object and the second argument is the file path to save the object to.\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nRead the following files in R, making sure you use the right read_*() function. You can write your code in the week-02.R script.\n\ndata/koppensteiner2016/takete_maluma.txt (a tab separated file).\ndata/pankratz2021/si.csv.\nGo to https://datashare.ed.ac.uk/handle/10283/4006, download the file conflict_data_.xlsx, and save it in data/. Read both sheets (“conflict_data2” and “demographics”). Any issues? (I suggest looking at the spreadsheet in Excel).\n\n\n\n\n\n\n\nSong, Yoonsang, Youngah Do, Arthur L. Thompson, Eileen R. Waegemaekers, and Jongbong Lee. 2020. “Second Language Users Exhibit Shallow Morphological Processing.” Studies in Second Language Acquisition 42 (5): 11211136. https://doi.org/10.1017/s0272263120000170.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html",
    "href": "ch-sum-measures.html",
    "title": "10  Summary measures",
    "section": "",
    "text": "10.1 Overview\nAs you learned in Chapter 3, quantitative data analysis can be conceived as three activities: summarising, visualising and modelling data. In this chapter, you will learn about summarising data. When we say “summarising data” we usually mean summarising data variables, by themselves or in group. We can summarise statistical variables using summary measures. There are two types of summary measures.\nAlways report a measure of central tendency together with its measure of dispersion! A central tendency measure captures only one aspect of the “distribution” of the values and variables with the same central tendency value could have very different dispersion, and hence be very different in nature. For example, look at the density plot in Figure 10.1 (you will learn more about them in Chapter 18). These plots are good at showing the distribution of values of numeric variables. The higher the density the curve, the more the values under that part of the curve are represented in the sample. Variable a and b have the same mean (central tendency): the mean is 0. But a has a standard deviation (measure of dispersion, more on this below) of 1 while b’s standard deviation is 3. You can appreciate how different a and b are, despite having exactly the same mean. This should show how important it is to not only report (and think about) central tendencies, like the mean, but also the dispersion of the data around the central tendency.\nFigure 10.1\nThe following call-outs list common measures of central tendency and dispersions and how they are calculated. You will probably be familiar with most of them and you don’t have to memorise the formulae. The sections after this one will dive into when to use each measure (and how to get them in R), which is much more important.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#overview",
    "href": "ch-sum-measures.html#overview",
    "title": "10  Summary measures",
    "section": "",
    "text": "Measures of central tendency indicate the typical or central value of a variable.\nMeasures of dispersion indicate the spread or dispersion of the variable values around the central tendency value.\n\n\n\n\n\n\n\n\n\n\nNoteMeasures of central tendency\n\n\n\nMean\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + ... + x_n}{n}\\]\nMedian\n\\[\\text{if } n \\text{ is odd, } x_\\frac{n+1}{2}\\]\n\\[\\text{if } n \\text{ is even,  } \\frac{x_\\frac{n}{2} + x_{\\frac{n}{2}+1}}{2}\\]\nMode\nThe mode is simply the most common value.\n\n\n\n\n\n\n\n\nNoteMeasures of dispersion\n\n\n\nMinimum and maximum values\nRange\n\\[ max(x) - min(x)\\]\nThe range is the difference between the largest and smallest value.\nStandard deviation\n\\[\\text{SD} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + ... + (x_n - \\bar{x})^2}{n-1}}\\]",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#measures-of-central-tendency-1",
    "href": "ch-sum-measures.html#measures-of-central-tendency-1",
    "title": "10  Summary measures",
    "section": "10.2 Measures of central tendency",
    "text": "10.2 Measures of central tendency\nA measure of central tendency approximately tells you where the data is most concentrated. There are three common measures of central tendency: mean, median and mode.\n\n10.2.1 Mean\nUse the mean with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\nmean(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] -0.354\n\n\n\nThe variable can take on any positive number only.\n\n\nmean(c(0.32, 2.58, 1.5, 0.12, 1.09))\n\n[1] 1.122\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDon’t take the mean of proportions and percentages!\nBetter to calculate the proportion/percentage across the entire data, rather than take the mean of individual proportions/percentages: see this blog post. If you really really have to, use the median.\n\n\n\n\n10.2.2 Median\nUse the median with numeric (continuous and discrete) variables.\n\n# odd N\nmedian(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] 0.09\n\n# even N\neven &lt;- c(4, 6, 3, 9, 7, 15)\nmedian(even)\n\n[1] 6.5\n\n# the median is the mean of the two \"central\" number\nsort(even)\n\n[1]  3  4  6  7  9 15\n\nmean(c(6, 7))\n\n[1] 6.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are two important characteristics of the mean and the median:\n\nThe mean is very sensitive to outliers.\nThe median is not.\n\nThe following list of numbers does not have obvious outliers. The mean and median are not to different.\n\n# no outliers\nmedian(c(4, 6, 3, 9, 7, 15))\n\n[1] 6.5\n\nmean(c(4, 6, 3, 9, 7, 15))\n\n[1] 7.333333\n\n\nIn the following case, there is quite a clear outlier, 40. Look how the mean is higher than the median. This is because the outlier 40 pulls the mean towards it.\n\n# one outlier\nmedian(c(4, 6, 3, 9, 7, 40))\n\n[1] 6.5\n\nmean(c(4, 6, 3, 9, 7, 40))\n\n[1] 11.5\n\n\n\n\n\n\n10.2.3 Mode\nUse the mode with categorical (discrete) variables. Unfortunately the mode() function in R is not the statistical mode, but rather it returns the R object type.\nYou can use the table() function to “table” out the number of occurrences of elements in a vector.\n\ntable(c(\"red\", \"red\", \"blue\", \"yellow\", \"blue\", \"green\", \"red\", \"yellow\"))\n\n\n  blue  green    red yellow \n     2      1      3      2 \n\n\nThe mode is the most frequent value: here it is red, with 3 occurrences.\n\n\n\n\n\n\nImportant\n\n\n\nLikert scales are ordinal (categorical) variables, so the mean and median are not appropriate! This is true even when Likert scales are represented with numbers, like “1, 2, 3, 4, 5” for a 5-point scale.\nYou should use the mode (you can use the median with Likert scales if you really really need to…).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#measures-of-dispersion-1",
    "href": "ch-sum-measures.html#measures-of-dispersion-1",
    "title": "10  Summary measures",
    "section": "10.3 Measures of dispersion",
    "text": "10.3 Measures of dispersion\nA measure of dispersion measures how much spread the data is around the measure of central tendency.\n\n10.3.1 Minimum and maximum\nYou can report minimum and maximum values for any numeric variable.\n\nx_1 &lt;- c(-1.12, 0.95, 0.41, -2.1, 0.09)\n\nmin(x_1)\n\n[1] -2.1\n\nmax(x_1)\n\n[1] 0.95\n\nrange(x_1)\n\n[1] -2.10  0.95\n\n\nNote that the range() function does not return the statistical range (see next section), but simply prints both the minimum and the maximum.\n\n\n10.3.2 Range\nUse the range with any numeric variable.\n\nx_1 &lt;- c(-1.12, 0.95, 0.41, -2.1, 0.09)\nmax(x_1) - min(x_1)\n\n[1] 3.05\n\nx_2 &lt;- c(0.32, 2.58, 1.5, 0.12, 1.09)\nmax(x_2) - min(x_2)\n\n[1] 2.46\n\nx_3 &lt;- c(4, 6, 3, 9, 7, 15)\nmax(x_3) - min(x_3)\n\n[1] 12\n\n\n\n\n10.3.3 Standard deviation\nUse the standard deviation with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\nsd(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] 1.23658\n\n\n\nThe variable can take on any positive number only.\n\n\nsd(c(0.32, 2.58, 1.5, 0.12, 1.09))\n\n[1] 0.9895555\n\n\n\n\n\n\n\n\nImportant\n\n\n\nStandard deviations are relative and depend on the measurement unit/scale!\nDon’t use the standard deviation with proportions and percentages!",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#summary-table-of-summary-measures",
    "href": "ch-sum-measures.html#summary-table-of-summary-measures",
    "title": "10  Summary measures",
    "section": "10.4 Summary table of summary measures",
    "text": "10.4 Summary table of summary measures\nTo conclude, here is a table that summarises when each measure should be used, depending on the nature of the variable. You can use this table as a cheat-sheet. Green cells indicate that the measure is appropriate for the variable, red cells indicates that they are not and should not be used, and orange cells indicate you should exercise caution when using those measures with those variables. Gray cells indicate that it’s mathematically impossible to apply that measure to that type of variable.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html",
    "href": "ch-summarise.html",
    "title": "11  Summarise data",
    "section": "",
    "text": "11.1 Summarise with summarise()\nNow that you have learned about summary measures, we can talk about how to summarise data in R, rather than just vectors as we did in the previous chapter. When you work with data, you always want to get summary measures for most of the variables in the data. Data reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable, which was covered in the previous section. Now, you will learn how to obtain summary measures using the summarise() function from the dplyr tidyverse package. Let’s practice with the data from Song et al. (2020) you read in Chapter 9. We want to get a measure of central tendency and dispersion for the reaction times, in the RT column. In order to decide which measures to pick, think about the nature of the RT variable. Reaction times is a numeric and continuous statistical variable, and it can only have positive values. So the mean and standard deviations are appropriate measures. Let’s start with the mean of the reaction time column RT. Go to your week-02.R script: if you followed Chapter 9 (and you should have), the script should already have the code to attach the tidyverse and read the song2020/shallow.csv file into a variable called shallow.\nNow let’s calculate the mean of RT with summarise(). The summarise() function takes at least two arguments: (1) the tibble to summarise, (2) one or more summary functions applied to columns in the tibble. In this case we just want the mean RTs. To get this, you write RT_mean = mean(RT) which tells the function to calculate the mean of the RT column and save the result in a new column called RT_mean. Yes, summarise() returns a tibble (a data frame)! It might seem overkill now, but you will see below that it is useful when you are grouping the data, so that for example you can get the mean of different groups in the data. Here is the code with its output:\nsummarise(shallow, RT_mean = mean(RT))\nGreat! The mean reaction times of the entire sample is 867.3592 ms. Sometimes you might want to round the numbers. You can round numbers with the round() function. For example:\nnum &lt;- 867.3592\nround(num)\n\n[1] 867\n\nround(num, 1)\n\n[1] 867.4\n\nround(num, 2)\n\n[1] 867.36\nThe second argument of the round() function sets the number of decimals to round to (by default, it is 0, so the number is rounded to the nearest integer, that is, to the nearest whole number with no decimal values). Let’s recalculate the mean by rounding it this time.\nsummarise(shallow, RT_mean = round(mean(RT)))\nWhat if we want also the standard deviation? Easy: we use the sd() function. Round the mean and SD with the round() function when you write the code in your week-02.R script.\n# round the mean and SD\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer). Let’s go all the way and also get the minimum and maximum RT values with the min() and max() functions (again, round all the summary measures).\nFab! When writing a data report, you could write something like this.\nRemember that standard deviations are a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data. However, you won’t be able to compare standard deviations across different measures: for example, you can’t compare the standard deviation of reaction times and of vowel formants because the first is in milliseconds and the second in Hertz; these are two different numeric scales. When required, you can use the median() function to calculate the median, instead of the mean(). Go ahead and calculate the median reaction times in the data. Is it similar to the mean?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#summarise-with-summarise",
    "href": "ch-summarise.html#summarise-with-summarise",
    "title": "11  Summarise data",
    "section": "",
    "text": "WarningExercise 1\n\n\n\nComplete this code to also get the minimum and maximum RT and round all measures to the nearest integer.\n\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = ..., RT_max = ...\n)\n\n\n\n\n\n\n\n\n\nImportantSolution\n\n\n\n\n\nThe functions for minimum and maximum are just a few lines above! Have you tried it yourself before seeing the solution?\n\n\nShow me\n\n\n\nsummarise(\n  shallow,\n  RT_mean = round(mean(RT)), RT_sd = round(sd(RT)),\n  RT_min = round(min(RT)), RT_max = round(max(RT))\n)\n\n\n\n\n\n\n\n\nReaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\n\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nCalculate the median of RTs in the shallow data.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#na-not-available",
    "href": "ch-summarise.html#na-not-available",
    "title": "11  Summarise data",
    "section": "11.2 NA: Not Available",
    "text": "11.2 NA: Not Available\nMost base R functions, like mean(), sd(), median() and so on, behave unexpectedly if the vector they are used on contains NA values. NA is a special object in R, that indicates that a value is Not Available, meaning that that observation does not have a value (or that the value was not observed in that case). For example, in the following numeric vector, there are 5 objects:\n\na &lt;- c(3, 5, 3, NA, 4)\n\nFour are numbers and one is NA. If you calculate the mean of a with mean() something strange happens.\n\nmean(a)\n\n[1] NA\n\n\nThe functions returns NA. This is because by default when just one value in the vector is NA then operations on the vector will return NA.\n\nmean(a)\n\n[1] NA\n\nsum(a)\n\n[1] NA\n\nsd(a)\n\n[1] NA\n\n\nIf you want to discard the NA values when operating on a vector that contains them, you have to set the na.rm (for “NA remove”) argument to TRUE.\n\nmean(a, na.rm = TRUE)\n\n[1] 3.75\n\nsum(a, na.rm = TRUE)\n\n[1] 15\n\nsd(a, na.rm = TRUE)\n\n[1] 0.9574271\n\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhat does the na.rm argument of mean() do?\n\n It changes NAs to FALSE. It converts NAs to 0s. It removes NAs before taking the mean.\n\nWhich is the mean of c(4, 23, NA, 5) when na.rm has the default value?\n\n NA. 0. 10.66.\n\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nCheck the documentation of ?mean.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#grouping-data-with-group_by",
    "href": "ch-summarise.html#grouping-data-with-group_by",
    "title": "11  Summarise data",
    "section": "11.3 Grouping data with group_by()",
    "text": "11.3 Grouping data with group_by()\nMore often, you will want to calculate summary measures for specific subsets of the data. An elegant way of doing this is with the group_by() function from dplyr. This function takes a tibble, groups the data based on the specified columns, and returns another tibble with the grouping.\n\nshallow_g &lt;- group_by(shallow, Group)\n\nIt looks as if nothing happened, but now the rows in the shallow_g tibble are grouped depending on the value of Group (L1 or L2). If you print out the tibble in the console (just write shallow_g in the Console and press enter), you will notice that the second line of the output says Groups: Group [2], like in the output below. This line tells you how the tibble is grouped: here it is grouped by Group and there are two groups.\n\n\n# A tibble: 6,500 × 11\n# Groups:   Group [2]\n   Group ID    List  Target        ACC    RT logRT Critical_Filler Word_Nonword\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;       \n 1 L1    L1_01 A     banoshment      1   423  6.05 Filler          Nonword     \n 2 L1    L1_01 A     unawareness     1   603  6.40 Critical        Word        \n 3 L1    L1_01 A     unholiness      1   739  6.61 Critical        Word        \n 4 L1    L1_01 A     bictimize       1   510  6.23 Filler          Nonword     \n 5 L1    L1_01 A     unhappiness     1   370  5.91 Critical        Word        \n 6 L1    L1_01 A     entertainer     1   689  6.54 Filler          Word        \n 7 L1    L1_01 A     unsharpness     1   821  6.71 Critical        Word        \n 8 L1    L1_01 A     fersistent      1   677  6.52 Filler          Nonword     \n 9 L1    L1_01 A     specificity     0   798  6.68 Filler          Word        \n10 L1    L1_01 A     termination     1   610  6.41 Filler          Word        \n# ℹ 6,490 more rows\n# ℹ 2 more variables: Relation_type &lt;chr&gt;, Branching &lt;chr&gt;\n\n\nThe grouping information is stored as an “attribute” in the tibble, named groups. You can check this attribute with attr(). You get a tibble with the groupings. Hopefully now you understand that, even if nothing seems to have happened, the tibble has been grouped. Since you saved the output of group_by() into a new variable shallow_g, note that shallow was not affected (try running atrr(shallow, \"groups\") and you will get a NULL). Here’s the output:\n\n\n# A tibble: 2 × 2\n  Group       .rows\n  &lt;chr&gt; &lt;list&lt;int&gt;&gt;\n1 L1        [2,900]\n2 L2        [3,600]\n\n\nThere are 2,900 rows in Group = L1 and 3,600 rows in Group = L2. Now let’s take the shallow_g data and calculate summary measures for L1 and L2 participants separately (as per the Group column).\n\n\n\n\n\n\nWarningExercise 3\n\n\n\nGet the rounded mean, median, SD, minimum and maximum of RTs for L1 and L2 participants in shallow_g.\n\n\n\n\n\n\n\n\nImportantSolution\n\n\n\n\n\nYou can do it! You’ve done this above but with shallow. Now you just need to use shallow_g plus get the mininimum and maximum.\n\n\nShow me\n\n\n\nsummarise(\n  shallow_g,\n  mean = round(mean(RT)),\n  median = round(median(RT)),\n  sd = round(sd(RT))\n)\n\n\n\n\n\n\nThis way of grouping the data first with group_by() first and then using summarise() on the grouped tibble works, but it can become tedious if you want to get summaries for different groups and/or combinations of groups. There is a more succinct way of doing this using the pipe |&gt;. Read on to learn about it.\n\n11.3.1 What the pipe!?\nThink of a pipe |&gt; as a teleporter. The pipe |&gt; teleports whatever is on its left into whatever is on its right. The pipe allows you to “stack” multiple operations into a pipeline, without the need to assign each output to a variable. This means that the code is more succinct and even more readable because the way you write code follows exactly the pipeline. So we can get summary measures for each group in Group like so:\n\nshallow |&gt; \n  group_by(Group) |&gt; \n  summarise(mean = round(mean(RT)))\n\n\n  \n\n\n\nThe code says:\n\nTake the shallow data.\nPipe it into group_by() and group it by Group.\nSummarise the grouped data with summarise().\n\nHopefully this just makes sense, but check the R Note box below if you want more details.\ngroup_by() can group according to more than one column, by listing the columns separated by commas (like group_by(Col1, Col2, Col3)). When you list more than one column, the grouping is fully crossed: you get a group for each combination of the grouping columns. Try to group the data by Group and Word_Nonword and get summary measures.\n\n\n\n\n\n\nWarningExercise 4\n\n\n\nGroup shallow by Group and Word_Nonword and get summary measures of RTs. Use the pipe.\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\ngroup_by(Group, Word_Nonword)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#counting-observations-with-count",
    "href": "ch-summarise.html#counting-observations-with-count",
    "title": "11  Summarise data",
    "section": "11.4 Counting observations with count()",
    "text": "11.4 Counting observations with count()\nIf you want to count observations you can use the summarise() function with n(), another dplyr function that returns the group size. For example, let’s count the number of languages by their endangerment status. The data in coretta2022/glot_status.rds contains the endangerment status for 7,845 languages from Glottolog. There are thousands of languages in the world, but most of them are losing speakers, and some are already no longer spoken. The column status contains the endangerment status of a language in the data, on a scale from not endangered (languages with large populations of speakers) through threatened, shifting and nearly extinct, to extinct (languages that have no living speakers left). Read the coretta2022/glot_status.rds data and check it out.\nTo count the number of languages by status, we group the data by status and we summarise with n().\n\nglot_status |&gt; \n  group_by(status) |&gt; \n  summarise(n = n())\n\n\n  \n\n\n\nThis approach works. However, dplyr offers a more compact way to get counts with the count() function! You can think of this function as a group_by/summarise combo. You list the columns you want to group by as arguments to count() and the output gives you a column n with the counts. It works with a single column or more than one, like group_by().\n\nglot_status |&gt; \n  count(status)\n\n\n  \n\n\n\n\n\n\n\n\n\nWarningExercise 5\n\n\n\nGet the number of languages by status and Macroarea.\n\n\n\n\n\n\n\n\nImportantR Note: Piping\n\n\n\n\n\nWith the release of R 4.1.0, a new feature was introduced to the base language that has significantly improved the readability and expressiveness of R code: the native pipe operator, written as |&gt;. The native pipe allows the result of one expression to be passed automatically as the first argument to another function. This simple idea has a profound impact on how we write R code, particularly when we are performing a sequence of data transformations.\nBefore the native pipe, it was common to see deeply nested function calls that could be difficult to read and reason about. For example, consider the task of computing the square root of the sum of a vector:\nsqrt(sum(c(1, 2, 3, 4)))\nWhile this is relatively simple, as functions become more complex and more transformations are chained together, nested calls quickly become cumbersome. The native pipe solves this by allowing you to write each operation in a left-to-right, stepwise manner, which mirrors the logical flow of data. The key principle of the native pipe is that the left-hand side (LHS) is evaluated first, and its result is automatically passed as the first argument to the right-hand side (RHS). This means that for a simple pipe like:\nx |&gt; f()\nit is equivalent to writing:\nf(x)\nThis principle is important because it defines the natural behavior of the pipe: whatever computation you produce on the LHS will be injected as the first input to the next function. Consider the mtcars dataset, which is built into R. Suppose we want to compute the average miles per gallon (mpg) for each number of cylinders (cyl). Using the native pipe in combination with tidyverse functions, the code is straightforward and highly readable:\nlibrary(dplyr)\n\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(avg_mpg = mean(mpg))\nLet’s break this down:\n\nThe mtcars dataset is the left-hand side. It is evaluated first and becomes the input for the next function.\ngroup_by(cyl) receives the dataset as its first argument, groups the data by the cyl column, and returns a grouped dataframe.\nThe grouped dataframe is then piped into summarise(avg_mpg = mean(mpg)), which calculates the mean mpg for each cylinder group.\n\nNotice that each step receives the output from the previous step as its first argument. This eliminates the need for intermediate variables and nested function calls, creating a natural, readable sequence of transformations.\n\nComparison with the magrittr pipe (%&gt;%)\nBefore R introduced the native pipe, the magrittr package popularized piping with the %&gt;% operator. Functionally, it achieves a very similar goal: passing the result of one expression to another function. For example, the earlier group_by and summarise operation can be written with magrittr as:\nlibrary(magrittr)\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg))\nSome differences between the native pipe and magrittr:\n\nThe native pipe is built into base R, so no external package is required.\nThe native pipe always passes the LHS value to the first argument of the RHS function.\nmagrittr allows more flexibility via the . placeholder, which can inject the LHS value into any argument.\nPerformance-wise, the native pipe has minimal overhead compared to %&gt;%, which is a function call.\n\nOverall, the native pipe provides a simple, consistent, and readable way to chain operations, especially when working with tidyverse workflows. For users already familiar with %&gt;%, the transition is intuitive, with the added benefit that this feature is now a part of base R.\n\n\n\n\n\n\n\n\nSong, Yoonsang, Youngah Do, Arthur L. Thompson, Eileen R. Waegemaekers, and Jongbong Lee. 2020. “Second Language Users Exhibit Shallow Morphological Processing.” Studies in Second Language Acquisition 42 (5): 11211136. https://doi.org/10.1017/s0272263120000170.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-transform.html",
    "href": "ch-transform.html",
    "title": "12  Transform data",
    "section": "",
    "text": "12.1 Filter rows\nData transformation is a fundamental aspect of data analysis. After the data you need to use is imported into R, you will often have to filter rows, create new columns, summarise data or join data frames, among many other transformation operations. In Chapter 11 you have already learned about summarising data, and in this chapter you will learn how to use filter() to filter the data and mutate() to mutate or create new columns.\nFiltering is normally used to filter rows in the data according to specific criteria: in other words, keep certain rows and drop others. Filtering data couldn’t be easier with filter(), from the dplyr package (one of the tidyverse core packages), Let’s work with the coretta2022/glot_status.rds data. Below you can find a preview of the data. Create a new R script called week-03.R and save it in code/. Read the coretta2022/glot_status.rds data.\nglot_status\nIn the following sections we will filter the rows of the data based on the status column. Before we can move on onto filtering however, we first need to learn about logical operators.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transform data</span>"
    ]
  },
  {
    "objectID": "ch-transform.html#filter-rows",
    "href": "ch-transform.html#filter-rows",
    "title": "12  Transform data",
    "section": "",
    "text": "12.1.1 Logical operators\n\n\n\n\n\n\nTipLogical operators\n\n\n\nLogical operators are symbols that compare two objects and return either TRUE or FALSE.\nThe most common logical operators are ==, !=, &gt;, and &lt;.\n\n\nThere are four main logical operators, each testing a specific logical statements:\n\nx == y: x equals y.\nx != y: x is not equal to y.\nx &gt; y: x is greater than y.\nx &lt; y: x is smaller than y.\n\nLogical operators return TRUE or FALSE depending on whether the statement they convey is true or false. Remember, TRUE and FALSE are logical values.\nTry the following code in the Console:\n\n# This will return FALSE\n1 == 2\n\n[1] FALSE\n\n# FALSE\n\"apples\" == \"oranges\"\n\n[1] FALSE\n\n# TRUE\n10 &gt; 5\n\n[1] TRUE\n\n# FALSE\n10 &gt; 15\n\n[1] FALSE\n\n# TRUE\n3 &lt; 4\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhich of the following does not contain a logical operator?\n\n 3 &gt; 1 \"a\" = \"a\" \"b\" != \"b\" 19 &lt; 2\n\nWhich of the following returns c(FALSE, TRUE)?\n\n 3 &gt; c(1, 5) c(\"a\", \"b\") != c(\"a\") \"apple\" != \"apple\"\n\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\n1a.\nCheck for errors in the logical operators.\n1b.\nRun them in the console to see the output.\n\n\n\n\n\n\n\n\n\nWarningExplanation\n\n\n\n\n\n1a.\nThe logical operator == has TWO equal signs. A single equal sign = is an alternative way of writing the assignment operator &lt;-, so that a = 1 and a &lt;- 1 are equivalent.\n1b.\nLogical operators are “vectorised” (you will learn more about this below), i.e they are applied sequentially to all elements in pairs. If the number of elements on one side does not match than of the other side of the operator, the elements on the side that has the smaller number of elements will be recycled.\n\n\n\nYou can use these logical operators with filter() to filter rows that match with TRUE in all the specified statements with logical operators.\n\n\n12.1.2 The filter() function\nFiltering in R with the tidyverse is straightforward. You can use the filter() function. filter() takes one or more statements that return TRUE or FALSE. A common use case is with logical operators. The following code filters the status column so that only the extinct status is included in the new data frame extinct. You’ll notice we are using the pipe |&gt; to transfer the data into the filter() function (you learned about the pipe in Chapter 11). The output of the filter function is assigned &lt;- to extinct. The flow might seem a bit counter-intuitive but you will get used to think like this when writing R code soon enough (although see the R Note box on assignment direction)!\n\nextinct &lt;- glot_status |&gt;\n  filter(status == \"extinct\")\n\nextinct\n\n\n  \n\n\n\nNeat! extinct contains only those languages whose status is extinct. What if we want to include all statuses except extinct? Easy, we use the non-equal operator !=.\n\nnot_extinct &lt;- glot_status |&gt;\n  filter(status != \"extinct\")\n\nnot_extinct contains all languages whose status is not extinct. And if we want only non-extinct languages from South America? We can include multiple statements separated by a comma!\n\nsouth_america &lt;- glot_status |&gt;\n  filter(status != \"extinct\", Macroarea == \"South America\")\n\nCombining statements like this will give you only those rows where all statements return TRUE. You can add as many statements as you need.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t assign the output of filter() to a variable (like in south_america &lt;-), the resulting tibble will be printed in the Console and you won’t be able to do more operations on it! Always remember to assign the output of filtering to a new variable and to avoid overwriting the full tibble, use a different name.\n\n\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nFilter the glot_status data so that you include only not_endangered languages from all macro-areas except Eurasia.\n\n\nThis is all great, but what if we want to include more than one status or macro-area? To do that we need another operator: %in%.\n\n\n12.1.3 The %in% operator\n\n\n\n\n\n\nTip%in%\n\n\n\nThe %in% operator is a special logical operator that returns TRUE if the value to the left of the operator is one of the values in the vector to its right, and FALSE if not.\n\n\nTry these in the Console:\n\n# TRUE\n5 %in% c(1, 2, 5, 7)\n\n[1] TRUE\n\n# FALSE\n\"apples\" %in% c(\"oranges\", \"bananas\")\n\n[1] FALSE\n\n\nBut %in% is even more powerful because the value on the left does not have to be a single value, but it can also be a vector! We say %in% is vectorised because it can work with vectors (most functions and operators in R are vectorised).\n\n# TRUE, TRUE\nc(1, 5) %in% c(4, 1, 7, 5, 8)\n\n[1] TRUE TRUE\n\nstocked &lt;- c(\"durian\", \"bananas\", \"grapes\")\nneeded &lt;- c(\"durian\", \"apples\")\n\n# TRUE, FALSE\nneeded %in% stocked\n\n[1]  TRUE FALSE\n\n\nTry to understand what is going on in the code above before moving on.\nNow we can filter glot_status to include only the macro-areas of the Global South and only languages that are either “threatened”, “shifting”, “moribund” or “nearly_extinct”.\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nFilter glot_status to include only the macro-areas (Macroarea) of the Global South and only languages that are either “threatened”, “shifting”, “moribund” or “nearly_extinct”. I have started the code for you, you just need to write the line for filtering status.\n\nglobal_south &lt;- glot_status |&gt;\n  filter(\n    Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"),\n    ...\n  )\n\nThis should not look too alien! The first statement, Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\") looks at the Macroarea column and, for each row, it returns TRUE if the current row value is in c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"), and FALSE if not.\n\n\n\n\n\n\n\n\nImportantSolution\n\n\n\n\n\n\nglobal_south &lt;- glot_status |&gt;\n  filter(\n    Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"),\n    status %in% c(\"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\")\n  )",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transform data</span>"
    ]
  },
  {
    "objectID": "ch-transform.html#mutate-columns",
    "href": "ch-transform.html#mutate-columns",
    "title": "12  Transform data",
    "section": "12.2 Mutate columns",
    "text": "12.2 Mutate columns\nTo change existing columns or create new columns, we can use the mutate() function from the dplyr package. To learn how to use mutate(), we will re-create the status column (let’s call it Status this time) from the Code_ID column in glot_status. The Code_ID column contains the status of each language in the form aes-STATUS where STATUS is one of not_endangered, threatened, shifting, moribund, nearly_extinct and extinct. You can check the labels in a column with the unique() function. This function is not from the tidyverse, but it is a base R function, so you need to extract the column from the tibble with $ (the dollar-sign operator). unique() will list all the unique labels in the column (note that it works with numbers too).\n\nunique(glot_status$Code_ID)\n\n[1] \"aes-shifting\"       \"aes-extinct\"        \"aes-moribund\"      \n[4] \"aes-nearly_extinct\" \"aes-threatened\"     \"aes-not_endangered\"\n\n\n\n\n\n\n\n\nTipThe dollar sign `$`\n\n\n\nYou can use the dollar sign $ to extract a single column from a data frame as a vector.\n\n\nWe want to create a new column called Status which has only the status part of the label without the aes- part. To remove aes- from the Code_ID column we can use the str_remove() function from the stringr package. Check the documentation of ?str_remove to learn which arguments it uses.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = str_remove(Code_ID, \"aes-\")\n  )\n\nIf you check glot_status now you will find that a new column, Status, has been added. This column is a character column (chr). You see that, as with filter(), you have to assign the output of mutate() to a variable. In the code above we are re-assigning the output to the glot_status variable which we started with. This means that we are overwriting the original glot_status. However, since we have added a new column, we have in practice only added the new column to the old data. If you use the name of an existing column, you will be effectively overwriting that column, so you must be careful with mutate().\nLet’s count the number of languages for each endangerment status using the new Status column. You learned about the count() feature in Chapter 11.\n\nglot_status |&gt;\n  group_by(status) |&gt; \n  count()\n\n\n  \n\n\n\nYou might have noticed that the order of the levels of Status does not match the order from least to most endangered/extinct. Try count() now with the pre-existing status column (with a lower case “s”). You will get the sensible order from least to most endangered/extinct. Why? This is because status (the pre-existing column) is a factor column with a specified order of the different statuses. A factor column is a column that is based on a factor vector (note that tibble columns are vectors), i.e. a vector that contains a list of values, called levels, from a specified set. Factor vectors (or factors for short) allow the user to specify the order of the values. If the order is not specified, the alphabetical order is used by default. Differently from factor vector/columns, character columns (columns that are character vectors) can only use the default alphabetical order. The Status column we created above is a character column. Check the column type by clicking on the small white triangle in the blue circle next to the name of the tibble in the Environment panel (tip-right panel of RStudio). Next to the Status column name you will see chr, for character. But if you look next to status you will see Factor.\n\n\n\n\n\n\nTipFactor vector\n\n\n\nA factor vector (or column) is a vector that contains a list of values (called levels) from a closed set.\nThe levels of a factor are ordered alphabetically by default.\n\n\nA vector/column can be mutated into a factor column with the as.factor() function. In the following code, we change the existing column Status, in other words we overwrite it (this happens automatically, because the Status column already exists, so it is replaced).\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = as.factor(Status)\n  )\n\nlevels(glot_status$Status)\n\n[1] \"extinct\"        \"moribund\"       \"nearly_extinct\" \"not_endangered\"\n[5] \"shifting\"       \"threatened\"    \n\n\nThe levels() functions returns the levels of a factor column in the order they are stored in the factor: as mentioned above, by default the order is alphabetical. What if we want the levels of Status to be ordered in a more logical manner: not_endangered, threatened, shifting, moribund, nearly_extinct and extinct? Easy! We can use the factor() function instead of as.factor() and specify the levels and their order in the levels argument.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = factor(\n      Status,\n      levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\")\n    )\n  )\n\nlevels(glot_status$Status)\n\n[1] \"not_endangered\" \"threatened\"     \"shifting\"       \"moribund\"      \n[5] \"nearly_extinct\" \"extinct\"       \n\n\nYou see that now the order of the levels returned by levels() is the one we specified. Transforming character columns to vector columns is helpful to specify a particular order of the levels which can then be used when summarising, counting or plotting.\n\n\n\n\n\n\nWarningExercise 3\n\n\n\nUse count() again with the new factor Status column.\n\n\nHere is a preview of data plotting in R, which you will learn in Chapter 15, with the status in the logical order from least to most endangered and extinct.\n\n\nCode\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nFigure 12.1: Number of languages by endangerment status (repeated).\n\n\n\n\n\n\n\n\n\n\n\nImportantR Note: Assignment direction\n\n\n\n\n\nR has two assignment operators: the assignment arrow &lt;- and =. Current R coding practices favour &lt;- over =, hence this book uses exclusively the former. Both have the same assignment direction: the object to the right of the operator is assigned to the variable name to the left of the operator. This is virtually how all programming languages work.\nIt is less know, however, that the assignment arrow can be “reversed”, -&gt; so that assignment goes from left to right. The following are equivalent.\na &lt;- 2\n2 -&gt; a\nWe could re-write the mutate code above like this:\nglot_status |&gt;\n  mutate(\n    Status = factor(\n      Status,\n      levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\")\n    )\n  ) -&gt; glot_status\nThe code fully follows the flow: you take glot_status, you pipe it into mutate, you create a new column, you assign the output to glot_status.\nHowever, I don’t particularly encourage this practice because it makes spotting variable assignment in your scripts more difficult, now that the variable is assigned at the end of the pipeline.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transform data</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html",
    "href": "ch-quarto.html",
    "title": "13  Quarto",
    "section": "",
    "text": "13.1 Code… and text!\nBefore moving onto data visualisation, it is time now to step up your coding organisation skills. Keeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\nYou will learn about reproducibility and related concepts in more details in ?sec-open-research. R scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #). But for longer text or complex data analysis reports, R scripts can be a bit cumbersome. A solution to this is using Quarto files (they have the .qmd extension).\nQuarto is a file format that allows you to mix code and formatted text in the same file. This means that you can write dynamic reports using Quarto files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report file and all code output (plots, tables, etc…) is updated accordingly!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#code-and-text",
    "href": "ch-quarto.html#code-and-text",
    "title": "13  Quarto",
    "section": "",
    "text": "TipDynamic reports in Quarto\n\n\n\nQuarto is a file type with extension .qmd in which you can write formatted text and code together.\nQuarto can be used to generate dynamic reports: these are files that are generated automatically from the file source, ensuring data and results in the report are always up to date.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#formatting-text",
    "href": "ch-quarto.html#formatting-text",
    "title": "13  Quarto",
    "section": "13.2 Formatting text",
    "text": "13.2 Formatting text\nR comments in R scripts cannot be formatted (for example, you can’t make text bold or italic). Text in Quarto files can be fully formatted using a simple but powerful mark-up language called Markdown. You don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, at your pace. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #:\n\n# This is a level-1 heading\n\n## This is a level-2 heading\n\n\n\n\n\n\nTipMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#create-a-.qmd-file",
    "href": "ch-quarto.html#create-a-.qmd-file",
    "title": "13  Quarto",
    "section": "13.3 Create a .qmd file",
    "text": "13.3 Create a .qmd file\n\n\n\n\n\n\nWarningImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course Quarto Project you created before. You know you are in a Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light-blue cube icon. If you see Project (none) in the top-right corner, that means you are not in the Quarto Project.\nTo make sure you are in the Quarto project, you can open the project by going to the project folder in File Explorer (Windows) or Finder (macOS) and double click on the .Rproj file.\n\n\nTo create a new .qmd file, just click on the New file button (the white square with the green plus symbol), then Quarto Document.... (If you are asked to install/update packages, do so.)\n\n\n\n\n\nA window will open. Add a title of your choice and your name. Make sure the Use visual markdown editor is NOT ticked, then click Create (you will be free to use the visual editor later, but it is important that you first see what a Quarto document looks like under the hood first).\n\n\n\n\n\nA new .qmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating a Quarto file does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\n\n\n\n\nSave the file inside the code/ folder with the following name: week-03.qmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\n\n13.3.1 Parts of a Quarto file\nA Quarto file usually has three main parts:\n\nThe YAML header (green in the screenshot below).\nCode chunks (red).\nText (blue).\n\n\n\n\n\n\nEach Quarto file has to start with a YAML header, but you can include as many code chunks and as much text as you wish, in any order. You can also show the document outline, marked as a dashed yellow box in the figure above. To show/hide the outline, just click on the Outline button. See the R Note box below for tips.\n\n\n\n\n\n\nTipQuarto: YAML header\n\n\n\nThe header of a .qmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nYAML headers start and end with three dashes ---.\n\n\n\n\n\n\n\n\nTipQuarto: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\n{r} indicates that the code is R code. Settings can be specified inside the chunk with the #| prefix: for example #| label: setup sets the name of the chunk (the label) to setup.\n\n\n\n\n\n\n\n\nImportantR Note: Quarto outline and chunk labels\n\n\n\n\n\nBy default, the Quarto outline only shows the Markdown section headers. I find it very useful to also see the code chunks in the outline and if they have a label, that will be shown in italics. It makes navigating long documents very easy and clear, descriptive chunk labels sure help.\nTo enable code labels in the outline, go to Global Options &gt; R Markdown &gt; Basic panel &gt; Show in document outline: Sections and all chunks.\n\n\n\n\n\n\n\n\n\n\n13.3.2 Working directory\nWhen using Quarto projects, the working directory (the directory all relative paths are relative to) is the project folder. However, when running code from a Quarto file, the code is run as if the working directory were the folder in which the file is saved. This isn’t an issue if the Quarto file is directly in the project folder, but in our case our Quarto files live in the code/ folder within the project folder (and it is good practice to do so!). We can instruct R to always run code from the project folder (i.e. the working directory is the project folder). This is when the _quarto.yml file comes into play.\nOpen the _quarto.yml file in RStudio (you can simply click on the file in the Files tab and that will open the file in the RStudio editor). Add the line execute-dir: project under the title. Note that indentation should be respected, so the line you write should align with title:, not with project:.\nproject:\n  title: \"qml-2024\"\n  execute-dir: project\nNow, all code in Quarto files, no matter where they are saved, will be run with the project folder as the working directory.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#how-to-add-and-run-code",
    "href": "ch-quarto.html#how-to-add-and-run-code",
    "title": "13  Quarto",
    "section": "13.4 How to add and run code",
    "text": "13.4 How to add and run code\nYou will use the Quarto document you created to write text and code in this chapter. Delete everything in the Quarto document below the YAML header. It’s just example text—we’re not attached to it! This is what the Quarto document should look like now (if your YAML header also contains “format:html, that’s completely fine):\n\n\n\n\n\nNow add an empty line and in the following line write a second-level heading ## Attach packages, followed by two empty lines. Like so:\n\n\n\n\n\nNow we can insert a code chunk to add the code to attach the tidyverse. To insert a new code chunk, you can click on the Insert a new code chunk button (the little green square icon with a C and a plus) , or you can press OPT+CMD+I/ALT+CTRL+I.\n\n\n\n\n\nA new R code chunk will be inserted at the text cursor position. Now go ahead and add the following lines of code inside the R code chunk.\n#| label: setup\n\nlibrary(tidyverse)\nTo run the code, you have two options:\n\nYou click the small green triangle in the top-right corner of the chunk. This runs all the code in the code chunk.\nEnsure the text cursor is inside the code chunk and press SHIFT+CMD+ENTER/SHIFT+CTRL+ENTER. This too runs all the code in the code chunk.\n\nIf you want to run line by line in the code chunk, you can place the text cursor on the line you want to run and press CMD+ENTER/CTRL+ENTER. The current line is run and the text cursor is moved to the next line. Just like in the .R scripts that we’ve been using in past weeks. Run the setup chunk now.\n\n\n\n\n\nYou will see messages printed below the code chunk, in your Quarto file (don’t worry about the Conflicts, they just tell you that some functions from the tidyverse packages have replaced the base R functions, which is OK). Now, complete the following tasks before moving on.\n\nCreate a new second-level heading (with ##) called Read data.\nCreate a new R code chunk.\nSet the label of the chunk to read-data.\nAdd code to read the following files (hint: think about where these files are located relative to the working directory, that is, the project folder). Assign the datasets to the variable names polite and glot_status respectively.\n\nwinter2012/polite.csv\ncoretta2022/glot_status.rds\n\nRun the code.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#render-quarto-files-to-html",
    "href": "ch-quarto.html#render-quarto-files-to-html",
    "title": "13  Quarto",
    "section": "13.5 Render Quarto files to HTML",
    "text": "13.5 Render Quarto files to HTML\nYou can render a .qmd file into a nicely formatted HTML file.\nTo render a Quarto file, just click on the Render button and an HTML file will be created and saved in the same location of the Quarto file.\n\n\n\n\n\nIt may be shown in the Viewer pane (like in the picture below) or in a new browser window. There are a few ways you can set this option to whichever version you prefer. Follow the instructions that work for you—they all do the same thing.\n\nTools &gt; Global Options &gt; R Markdown &gt; Show output preview in…\nPreferences &gt; R Markdown &gt; Basics &gt; Show output preview in….\nRight beside the Render button, you will see a little white gear. Click on that gear, and a drop-down menu will open. Click on Preview in Window or Preview in Viewer Pane, whichever you prefer.\n\n\n\n\n\n\nRendering Quarto files is not restricted to HTML, but also PDFs and even Word documents! This is very handy when you are writing an analysis report you need to share with others. You could even write your dissertation in Quarto!\n\n\n\n\n\n\nTipQuarto: Rendering\n\n\n\nQuarto files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\nNow that you have done all of this hard work, why don’t you try and render the Quarto file you’ve been working on to an HTML file? Click on the Render button and if everything works fine you should see a rendered HTML file in a second! Note that if you are enrolled in the QML course, you will be asked to render your Quarto files to PDF for the assessments, so I recommend you try this out now by completing the next section.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#render-quarto-files-to-pdf",
    "href": "ch-quarto.html#render-quarto-files-to-pdf",
    "title": "13  Quarto",
    "section": "13.6 Render Quarto files to PDF",
    "text": "13.6 Render Quarto files to PDF\nRendering a Quarto file to PDF is done through LaTeX, a typesetting system with it’s own programming language. You don’t really need to learn LaTeX to render to PDF, because the conversion is handles by Quarto, but since usually computers don’t come with LaTeX you will have to install it (you can learn more about LaTeX in the Spotlight box below). Do this now, by running the following code in the Terminal in RStudio.\n\n\n\n\n\n\nImportant\n\n\n\nNote that you are supposed to run the code in the RStudio Terminal, not the RStudio R console! The RStudio terminal can be found next to the console in the bottom-left corner of RStudio.\n\n\nquarto install tinytex\nA LaTeX distribution will be installed. Now, add the following lines to the YAML preamble of your Quarto file and save the file.\nformat:\n  html: default\n  pdf: default\nYou will see that not there is a small arrow next to the Render button. If you click on it a drop-down menu will appear, with two options: “HTML” and “PDF”. This is because we have instructed Quarto that the file should be rendered in two formats in the yaml preamble with the yaml code above. Click on the PDF option now. If all is well, you should soon see your Quarto file rendered to PDF!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantSpotlight: LaTeX\n\n\n\n\n\nLaTeX is a mark-up language for writing beautifully typeset documents, like academic articles and books. It is very popular in disciplines that make heavy use of maths or statistics and it has been adopted by a lot of researchers working in linguistics, although adoption rates vary by sub-field.\nLaTeX documents are rendered (compiled) to PDF. In other words, you write a document with extension .tex using LaTeX syntax and the document can be compiled to a PDF. Typesetting is dealt with by LaTeX so you just need to specify what you want, like sections, bullet points and so on. This is in the same spirit as Markdown and Quarto. In computing, this approach is called What You See Is What You Mean (WYSIWYM). This contrasts with text editors like MS Office Word which are What You See Is What You Get (WYSIWYG).\nYou can learn the basics of LaTeX in 30 minutes by following the tutorial Learn LaTeX in 30 minutes on Overleaf. Overleaf is an online LaTeX editor service, where you can write and compile LaTeX documents and even collaborate live with other people.\nHere is a tiny example of what a LaTeX document looks like.\n\\documentclass{article}\n\n\\begin{document}\n\n\\section{My first LaTeX document}\n\nThis is my first LaTeX document. How exciting!\n\n\\end{document}\nThis LaTeX document produces the following PDF. Nothing too exciting, but with LaTeX you can put together professional-looking and highly technical documents.\n\n\n\nA PDF document generated with LaTeX.\n\n\nIf you use Quarto to produce a PDF, you don’t really have to be very proficient in LaTeX but it helps to know about it, in case something goes wrong with the conversion from Quarto to PDF.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#summary",
    "href": "ch-quarto.html#summary",
    "title": "13  Quarto",
    "section": "13.7 Summary",
    "text": "13.7 Summary\n\n\n\n\n\n\n\nQuarto files can be used to create dynamic and reproducible reports.\nMark-up languages are text-formatting systems that specify text formatting and structure using symbols or keywords. Markdown is the mark-up language that is used in Quarto documents.\nThe main parts of a .qmd file are the YAML header, text and code chunks.\nYou can render Quarto files into HTML, PDF, Word documents and more.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html",
    "href": "ch-viz-principles.html",
    "title": "14  Visualisation principles",
    "section": "",
    "text": "14.1 Good data visualisation\nAs you learned in Chapter 2, quantitative data analysis can be conceived as three activities: summarising, visualising and modelling data. This chapter introduces you to basic principles of good data visualisation, while in Chapter 15 you will learn the basics of plotting data in R.\nAlberto Cairo has identified four common features of good data visualisation (Spiegelhalter 2019: 64-66):\nLet’s see a few examples that illustrate each point. Since you will learn about the plotting code in the next chapter, the code is not shown by default here, but you can see it by clicking on the expandable Code button above each plot.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#good-data-visualisation",
    "href": "ch-viz-principles.html#good-data-visualisation",
    "title": "14  Visualisation principles",
    "section": "",
    "text": "TipGood data visualisation\n\n\n\n\nIt contains reliable information.\nThe design has been chosen so that relevant patterns become noticeable.\nIt is presented in an attractive manner, but appearance should not get in the way of honesty, clarity and depth.\nWhen appropriate, it is organized in a way that enables some exploration.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#information-is-not-reliable",
    "href": "ch-viz-principles.html#information-is-not-reliable",
    "title": "14  Visualisation principles",
    "section": "14.2 Information is (not) reliable",
    "text": "14.2 Information is (not) reliable\nLet’s use the coretta2022/glot_status data. to create the plots because you will learn about it later. The following plot is titled Number of endangered languages by macroarea and status, but the plot contains both endangered and non-endangered languages.\n\n\nCode\nglot_status %&gt;%\n  # filter(status != \"extinct\") %&gt;%\n  ggplot(aes(Macroarea, fill = status)) +\n  geom_bar() +\n  scale_fill_brewer(type = \"seq\", palette = \"BuPu\") +\n  labs(\n    title = \"Number of endangered languages by macroarea and status\",\n    caption = \"Stacked bar-chart\"\n  )\n\n\n\n\n\n\n\n\n\nWe can fix that by filtering the data so that it contains only endangered languages.\n\n\nCode\nglot_status %&gt;%\n  filter(status != \"extinct\") %&gt;%\n  ggplot(aes(Macroarea, fill = status)) +\n  geom_bar() +\n  scale_fill_brewer(type = \"seq\", palette = \"BuPu\") +\n  labs(\n    title = \"Number of endangered languages by macroarea and status\",\n    caption = \"Stacked bar-chart\"\n  )",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#patterns-are-not-noticeable",
    "href": "ch-viz-principles.html#patterns-are-not-noticeable",
    "title": "14  Visualisation principles",
    "section": "14.3 Patterns are (not) noticeable",
    "text": "14.3 Patterns are (not) noticeable\nThe coretta2021/albvot data contains data on VOT in Albanian. It has data from 6 speakers (Coretta et al. 2022). The following plot uses a bar chart to show the VOT of different stops, but what you can’t really see is that there is a lot of variability within and among stops and within and among speakers.\n\n\nCode\nalbvot %&gt;%\n  filter(consonant %in% c(\"p\", \"t\", \"k\", \"b\", \"d\", \"ɡ\")) %&gt;%\n  ggplot(aes(vot, consonant)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Albanian Voice Onset Time\"\n  )\n\n\n\n\n\n\n\n\n\nWe can do better. The following plot shows individual measurements of VOT for different stops and speakers. Now an interesting pattern emerges: speaker 5 (s05) has particularly long VOT for /t/ and /k/ relative to the other speakers.\n\n\nCode\nalbvot %&gt;%\n  filter(consonant %in% c(\"p\", \"t\", \"k\", \"b\", \"d\", \"\\u261\")) %&gt;%\n  mutate(consonant = factor(consonant, levels = rev(c(\"p\", \"t\", \"k\", \"b\", \"d\", \"\\u261\")))) %&gt;%\n  ggplot(aes(consonant, vot, colour = speaker)) +\n  geom_line(aes(group = interaction(speaker, consonant)), position = position_dodge(width = 0.5)) +\n  geom_point(size = 1.5, alpha = 0.9, position = position_dodge(width = 0.5), aes(group = speaker)) +\n  geom_hline(aes(yintercept = 0)) +\n  scale_y_continuous(breaks = seq(-200, 200, by = 50)) +\n  coord_flip() +\n  labs(\n    ttile = \"Albanian Voice Onset Time\",\n    y = \"Voice Onset Time (ms)\", x = \"Consonant\",\n    caption = \"Time 0 corresponds to the plosive release.\"\n  )\n\n\n\n\n\n\n\n\n\nBar charts are unfortunately overused in research, even in those cases when they are not appropriate.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#aesthetics-should-not-get-in-the-way",
    "href": "ch-viz-principles.html#aesthetics-should-not-get-in-the-way",
    "title": "14  Visualisation principles",
    "section": "14.4 Aesthetics (should not) get in the way",
    "text": "14.4 Aesthetics (should not) get in the way\n\n\n\n\n\nThe graph above has a lot of issues:\n\nThe bar length and thickness are not proportional. Compare Japanese with 123 million speakers vs English with 765 million speakers.\nThe graph mixes two scales: million speakers and billion speakers. This makes it look as if Chinese does not have that many more speakers.\nThe shade of orange of the bars does not seem to become proportionally darker with more speakers. Look at Arabic and Hindi: they have a very similar number of speakers but one bar is darker than the other.\nThe three dudes speaking are just fillers. Are they really necessary? Also, they are all white men…\n\nCan you find other issues? See more examples on Ugly Charts.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#does-not-enable-exploration",
    "href": "ch-viz-principles.html#does-not-enable-exploration",
    "title": "14  Visualisation principles",
    "section": "14.5 Does (not) enable exploration",
    "text": "14.5 Does (not) enable exploration\nThe plot below shows the number of gestures enacted by infants of English, Bengali and Chinese background as recorded during a controlled session (Cameron-Faulkner et al. 2020). Three different types of gestures are shown: hold out and give gestures (ho_gv), index-finger pointing (point) and reach out gestures (reach). Moreover the plot shows the number of gestures at 10 and 12 months.\n\n\nCode\ngestures %&gt;%\n  filter(months %in% c(10, 12)) %&gt;%\n  drop_na(count) %&gt;%\n  group_by(months, background, gesture) %&gt;%\n  summarise(\n    count_sum = sum(count), .groups = \"drop\"\n  ) %&gt;%\n  ggplot(aes(as.factor(months), count_sum, fill = background)) +\n  geom_bar(stat = \"identity\") +\n  facet_grid(background ~ gesture) +\n  scale_fill_brewer(type = \"qual\") +\n  labs(\n    title = \"Infant gesture counts (tally) at 10 and 12 mo\",\n    x = \"Months old\", y = \"Gesture count\"\n  )\n\n\n\n\n\n\n\n\n\nA bar chart is appropriate with count data, like in this case, but it does not allow for much exploration. Each infant was recorded at 10 and 12 months of age, but in the plot you don’t see whether individual infants changed their number of gestures. We can only notice that overall the number of gestures increases from 10 to 12 months old.\nWe can use a “connected point” plot: each infant is represented by a dot at 10 and 12 months and the dots of the same infant are connected by a line. This allows us to see whether an individual infant uses more gestures at 12 months.\n\n\nCode\ngestures %&gt;%\n  filter(months %in% c(10, 12)) %&gt;%\n  drop_na(count) %&gt;%\n  ggplot(aes(as.factor(months), count, colour = background)) +\n  geom_line(aes(group = id), alpha = 0.5) +\n  geom_point(alpha = 0.7) +\n  facet_grid(background ~ gesture) +\n  scale_color_brewer(type = \"qual\") +\n  labs(\n    title = \"Infant gesture counts at 10 and 12 mo\",\n    x = \"Months old\", y = \"Gesture count\"\n  )\n\n\n\n\n\n\n\n\n\nYou will notice that some infants don’t really use more gestures and others even use slightly less gestures. You would not be able to see any of this if you used a bar chart, like we used above.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#practical-tips",
    "href": "ch-viz-principles.html#practical-tips",
    "title": "14  Visualisation principles",
    "section": "14.6 Practical tips",
    "text": "14.6 Practical tips\nHere is a list of practical visualisation tips for you to think about.\n\n\n\n\n\n\nTip\n\n\n\n\nShow raw data (e.g. individual observations, participants, items…).\nSeparate data in different panels as needed.\nUse simple but informative labels for axes, panels, etc…\nUse colour as a visual aid, not just for aesthetics.\nReuse labels, colours, shapes throughout different plots to indicate the same thing.\n\n\n\n\n\n\n\nCameron-Faulkner, Thea, Nivedita Malik, Circle Steele, Stefano Coretta, Ludovica Serratrice, and Elena Lieven. 2020. “A Cross-Cultural Analysis of Early Prelinguistic Gesture Development and Its Relationship to Language Development.” Child Development 92 (1): 273290. https://doi.org/10.1111/cdev.13406.\n\n\nCoretta, Stefano, Josiane Riverin-Coutlée, Enkeleida Kapia, and Stephen Nichols. 2022. “Northern Tosk Albanian.” Journal of the International Phonetic Association, 123. https://doi.org/10.1017/s0025100322000044.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html",
    "href": "ch-plotting.html",
    "title": "15  Plotting",
    "section": "",
    "text": "15.0.1 Base R plotting function\nIn the previous chapter, Chapter 14, you have learned about basic visualisation principles.\nWith these principles in mind, this chapter will teach you the basics of data visualisation (aka plotting) in R. In R, you can create plots using different systems: base R, ggplot2, plotly, lattice and others. This book focusses on the ggplot2 system, which is part of the tidyverse, but before we dive in, it is useful to have a look at the base R plotting system.\nLet’s create two numeric vectors, q and w and plot them. The function plot() takes two arguments: the first argument x takes a vector with the horizontal coordinates (x-axis), here q, and the argument y takes a vector of the same length as the vector of the first argument, with the vertical coordinates (y-axis).\n# N:M is a shortcut for all integer numbers between N and M\nq &lt;- 1:10\n# w is the cube of q\nw &lt;- q^3\n\n# Plot a scatter plot with x as the x-axis and y as the y-axis\nplot(x = q, y = w)\nThe function takes care of adding tick-marks with numbers on the x and y axis, name the axes with the names of the vectors and add the points based on the coordinates in the vectors. It could not be easier! Now let’s add a few more things to this basic plot. Let’s specify we want a line plot (type = \"l\") instead of points, that the line should be coloured purple (col = \"purple\"), with a width of 3 (lwd = 3) and dashed (lty = \"dashed\"). The function connects the points from the coordinates given in the vectors with a line.\nplot(q, w, type = \"l\", col = \"purple\", lwd = 3, lty = \"dashed\")\nWith plots as simple as this one, the base R plotting system is sufficient, but to create more complex plots (which is virtually always the case), base R gets incredibly complicated. Instead, we can use the tidyverse package ggplot2. ggplot2 works well with the other tidyverse packages and it follows the same principles, so it is convenient to use it for data visualisation instead of base R. The following sections will go through the basics of plotting with ggplot2.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#your-first-ggplot2-plot",
    "href": "ch-plotting.html#your-first-ggplot2-plot",
    "title": "15  Plotting",
    "section": "15.1 Your first ggplot2 plot",
    "text": "15.1 Your first ggplot2 plot\nThe tidyverse package ggplot2 provides users with a consistent set of functions to create captivating graphics, and the package works well in combination with the other tidyverse packages. We will plot data from winter2012/polite.csv (Winter and Grawunder 2012) to learn the basics. We can read the data with read_csv() from readr and plot it with ggplot() from ggplot2. Since both readr and the ggplot2 package are part of the tidyverse, it is sufficient to attach the tidyverse with library(tidyverse).\n\nlibrary(tidyverse)\n\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\npolite\n\n\n  \n\n\n\nThe polite data contains several acoustic measurements from utterances spoken by Korean students in Germany. Each row is a single utterance and each participant has spoken many utterances. These are the columns we will focus on.\n\nf0mn: the mean f0 (fundamental frequency). This is the mean f0 of each utterance (i.e. the f0 is calculated along the entire utterance and the mean is taken).\nH1H2: the difference between H2 and H1 (second and first harmonic; the paper reports that this “was based on the central vowel portion of each vowel” although it is not clear if the H1-H2 value of each vowel in the utterance was averaged to produce a mean H1-H2 difference per utterance). A higher H1-H2 difference indicates that the voice is more breathy (as opposed to modal).\ngender: the gender of the speaker (F = female, M = male).\n\nFigure 15.1 shows the plot we will end up with and you will learn how to create it bit by bit below. This plot is a scatter plot, with mean f0 on the x-axis and the H1-H2 difference on the y-axis. Each point represent a an observation in the data, i.e. a row. The points are coloured based on the gender of the participant. You might notice that when mean f0 is high, the H1-H2 difference is lower. In other words, higher mean f0 corresponds to breathier voice.\n\n\n\n\n\n\n\n\nFigure 15.1: Mean f0 and H1-H2 difference in Korean speakers, by gender (Winter and Grawunder 2012).\n\n\n\n\n\nEach ggplot2 plot has a minimum of two constituents (which correspond to two arguments of the ggplot() function): the data and aesthetics mapping.\n\n\n\n\n\n\nTipggplot2 basic constituents\n\n\n\n\nThe data: you have to specify the data frame with the data (i.e. columns) you want to plot.\nThe mapping: the mapping tells ggplot how to map data columns to parts of the plot like the axes or groupings within the data. For example, which variable is shown on the x axis, and which one is on the y axis? If data comes from two different groups, should each group get its own colour? These different parts of the plot are called aesthetics, or aes for short.\n\n\n\nYou can specify the data and mapping with the data and mapping arguments of the ggplot() function. Note that the mapping argument is always specified with aes(): mapping = aes(…). In the following bare plot, we are just mapping f0mn to the x-axis and H1H2 to the y-axis, from the polite data frame. From this point on I will assume you’ll be creating a new code chunk, copy-paste the code and run it, without explicit instructions.\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n)\n\n\n\n\n\n\n\n\nNot much to see here: just two axes! So where’s the data? Don’t worry, we didn’t do anything wrong. Showing the data itself requires a further step, adding geometries, which we’ll turn to next.\n\n\n\n\n\n\nNoteQuiz 2\n\n\n\nIs the following code correct? Why? TRUEFALSE\nggplot(\n  data = polite,\n  mapping = c(x = total_duration, y = articulation_rate)\n)\n\n\n\n15.1.1 Let’s add geometries\nOur code so far makes nice axes, but we are missing the most important part: showing the data! Data is represented with geometries, or geoms for short. geoms are added to the base ggplot with functions whose names all start with geom_.\n\n\n\n\n\n\nTipGeometries\n\n\n\nGeometries are plot elements that show the data through geometric shapes.\nDifferent geometries are added to a ggplot using one of the geom_*() functions.\n\n\nFor this plot, you want to use geom_point(). This geom simply adds point to the plot based on the data in the polite data frame. To add geoms to a plot, you write a plus sign + at the end of the ggplot() command and include the geom on the next line.1 The geom_point() geometry creates a scatter plot, which is a plot with two continuous axes where data is represented with points. Figure 15.2 is a scatter plot of mean f0 (mnf0) and H1-H2 difference (H1H2).\n\n\n\n\n\n\nTipScatter plot\n\n\n\nA scatter plot is a plot with two numeric axes and points indicating the data. It is used when you want to show the relationship between two numeric variables.\nTo create a scatter plot, use the geom_point() geometry.\n\n\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 15.2: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\nLook at Figure 15.2: is there a relationship between mean f0 and H1-H2? A pattern can be observed: when mean f0 is low, H1-H2 is high (meaning more breathiness) and when f0 is high, H1-H2 is low (meaning less breathiness). Statistically, this is called a negative relationship. The opposite is a positive relationship, when an increase in \\(x\\) corresponds to an increase in \\(y\\). Spoiler: the negative relationship in the plot is a mirage: if you look more closely, you might spot two subgroups in the data: one up to about 175 hz and one from 175 hz up. We will see below that these two groups correspond to the speakers’ genders.\nFor the time being, let’s pretend we don’t know that and we want to write a description of the plot and the pattern. You could describe the plot this way:\n\nFigure 15.2 shows a scatter plot of mean f0 on the x-axis and H1-H2 difference on the y-axis. The plot suggest an overall negative relationship between mean f0 and H1-H2 difference. In other words, increasing mean f0 corresponds to decreasing breathiness.\n\n\n\n\n\n\n\nImportantR: The Layered Grammar of Graphics\n\n\n\n\n\nUsing the + is a quirk of ggplot(). The idea behind it is that you start from a bare plot and you add (+) layers of data on top of it. This is because of the philosophy behind the package, called the Layered Grammar of Graphics. In fact, Grammar of Graphics is where you get the GG in ggplot!\n\n\n\n\n\n15.1.2 Function arguments\nNote that the data and mapping arguments don’t have to be named explicitly (with data = and mapping =) in the ggplot() function, since they are obligatory and they are specified in that order. So you can write:\n\nggplot(\n  polite,\n  aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\nIn fact, you can also leave out x = and y =.\n\nggplot(\n  polite,\n  aes(f0mn, H1H2)\n) +\n  geom_point()\n\nBut we can go further. You can use the pipe |&gt;, which you have encountered in Chapter 11.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()\n\nYou can of course stack multiple functions in the pipeline, like for example filtering the data before plotting it, like so:\n\npolite |&gt;\n  # include only rows where f0mn &lt; 300\n  filter(f0mn &lt; 300) |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()\n\n\n\n\n\n\n\nFigure 15.3: Scatter plot of mean f0 and H1-H2 difference (filtered).\n\n\n\n\n\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nRun ?ggplot in the Console and check the documentation of the function. Pay special attention to the arguments of the function and the order they appear in.\n\n\n\n\n\n\n\n\nNoteQuiz 3\n\n\n\nWhich of the following will produce the same plot as Figure 15.2? Reason through it first without running the code, then run all of these to check whether they look the way you expected.\n\n ggplot(polite, aes(H1H2, f0mn)) + geom_point() ggplot(polite, aes(y = H1H2, x = f0mn)) + geom_point() ggplot(polite, aes(y = f0mn, x = H1H2)) + geom_point()\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nWhen specifying arguments, the order matters when not using the argument names.\nSo aes(a, b) is different from aes(b, a).\nBut aes(y = b, x = a) is the same as aes(a, b).",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#working-with-aesthetics",
    "href": "ch-plotting.html#working-with-aesthetics",
    "title": "15  Plotting",
    "section": "15.2 Working with aesthetics",
    "text": "15.2 Working with aesthetics\nSo far, the only aesthetics you have been using were the x and y aesthetics, which correspond to the x and y axes. ggplot2 has many other aesthetics that can be employed to represent other variables in the plot: in this section you will learn about colour (which is used to colour geometries, like points) and alpha (which is used to set the transparency of geometries).\n\n15.2.1 colour aesthetic\nAs mentioned above, there seems to be two subgroups within the data: one below about 175 Hz and one above it. These subgroups are in fact related to the gender of the participants. We can colour the points by gender, using the colour aesthetic.2 Figure 15.4 shows a scatter plot of mean f0 and the H1-H2 difference, with points coloured depending on the gender of the speaker. Now the two subgroups are quite visible, although we can also appreciate some overlap between the two gender subgroups (some blue points overlap with the red points and there is one red point that has a very low mean f0).\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 15.4: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nNotice how colour = gender must be inside the aes() function, because we are trying to map colour to the values of the column gender (when you map values to aesthetics, the aesthetics have to be inside aes()). Colours are automatically assigned to each level in gender (here, F for female which gets red and M for male which gets blue).\nThe default colour palette is used, but you can customise it. One way to quickly change the palette it to use one of the scale_colour_*() functions. A good option for our plot is scale_colour_brewer(). This function creates palettes based on ColorBrewer 2.0. There are three types of palettes (see the linked website for examples):\n\nSequential (seq): a gradient sequence of hues from lighter to darker.\nDiverging (div): useful when you need a neutral middle colour and sequential colours on either side of the neutral colour.\nQualitative (qual): useful for categorical variables.\n\nLet’s use the default qualitative palette, since gender is a categorical variable in the data. Figure 15.5 is the same as Figure 15.4, but we are now using a qualitative ColorBrewer palette.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_brewer(type = \"qual\")\n\n\n\n\n\n\n\nFigure 15.5: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nChange the palette argument of the scale_colour_brewer() function to different palettes. Check the function documentation for a list of available palettes.\n\n\nAnother set of palettes is provided by scale_colour_viridis_d() (the d stands for “discrete” palette, to be used for categorical variables like gender). Figure 15.6 uses the “B” palette from the viridis palettes.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_viridis_d(option = \"B\")\n\n\n\n\n\n\n\nFigure 15.6: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\nImportantR Note: The default colour palette\n\n\n\n\n\nIf you want to know more about the default colour palette, check this blog post out.\n\n\n\n\n\n15.2.2 alpha aesthetic\nAnother useful ggplot2 aesthetic is alpha. This aesthetic sets the transparency of the geometry: 0 means completely transparent and 1 means completely opaque. When you are setting the value of an aesthetic yourself that should apply to all instances of some geometry, rather than mapping an aesthetic to values in a specific column (like we did above with colour), you should add the aesthetic outside of aes() and usually in the geom function you want to set the aesthetic for. Set alpha for the point geometry to 0.5.\n\n\n\n\n\n\nTipHint\n\n\n\n\n\ngeom_point(alpha = ...)\n\n\n\nSetting a lower alpha is useful when there are a lot of points or other geometries that overlap with each other and it just looks like a blob of colour (so that, for example, you can’t really see the individual points). It is not the case here, and in fact reducing the alpha makes the plot quite illegible!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#labels",
    "href": "ch-plotting.html#labels",
    "title": "15  Plotting",
    "section": "15.3 Labels",
    "text": "15.3 Labels\nThe labels of the plot, like the axes labels and the legend, are automatically included by ggplot2 based on the names of the variables/columns. If you want to change the labels to something you set yourself, you can use the labs() function, like in Figure 15.7 below.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 15.7: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nLet’s rewrite out description of the plot from above to reflect the updates.\n\nFigure 15.7 shows a scatter plot of mean f0 on the x-axis and H1-H2 difference on the y-axis, with points coloured by gender. The plot suggest an overall negative relationship between mean f0 and H1-H2 difference. However, the negative relationship appears to be an artefact of the presence of the two gender subgroups: male participants have lower mean f0 and higher H1-H2 difference (less breathiness), while female participants have higher f0 and lower H1-H2 difference (more breathiness).\n\n\n\n\n\n\n\nWarningExercise 3\n\n\n\nAdd a title and a subtitle (use these two arguments within the labs() function).\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nFor example, labs(title = \"...\", ...).",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#summary",
    "href": "ch-plotting.html#summary",
    "title": "15  Plotting",
    "section": "15.4 Summary",
    "text": "15.4 Summary\n\n\n\n\n\n\n\nggplot2 is a plotting package from the tidyverse.\nTo create a basic plot, you use the ggplot() function and specify data and mapping.\n\nThe aes() function allows you to specify aesthetics (like axes, colours, …) in the mapping argument.\nGeometries map data values onto shapes in the plot. All geometry functions are of the type geom_*().\n\nScatter plots are created with geom_point() and can be used with two numeric variables set as the x and y aesthetics.\nThe colour and alpha aesthetics set the geometry’s colour and transparency.\nIf you need to set an aesthetic to be applied to the entire geometry, you can specify the aesthetic in the geometry, without the aes() function.\n\n\n\n\n\n\n\n\nWinter, Bodo, and Sven Grawunder. 2012. “The Phonetic Profile of Korean Formal and Informal Speech Registers.” Journal of Phonetics 40 (6): 808–15. https://doi.org/10.1016/j.wocn.2012.08.006.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#footnotes",
    "href": "ch-plotting.html#footnotes",
    "title": "15  Plotting",
    "section": "",
    "text": "Note that going on the next line is just for reasons of code clarity and you could write the entire code for a plot on a single line.↩︎\nTo make ggplot inclusive, it’s possible to write the colour aesthetic either as the British-style colour or the American-style color! Both will get the job done.↩︎",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-more-plotting.html",
    "href": "ch-more-plotting.html",
    "title": "16  More plotting",
    "section": "",
    "text": "16.1 Bar charts\nIn Figure 12.1 from Chapter 12, you saw how to visualise counts with a bar chart. In this chapter you will learn how to create bar charts with ggplot2. We will first create a plot with counts of the number of languages in global_south (filtered data from coretta2022/glot_status.rds) by their endangerment status and then a plot where we also split the counts by macro-area. To create a bar chart, you use the geom_bar() geometry.\nRead the coretta2022/glot_status.rds data and filter it so that you include only languages from Africa, Australia, Papunesia and South America, with any status except not endangered and extinct.\nIn a simple bar chart, you only need to specify one axis, the x-axis, in the aesthetics aes(). This is because the counts that are placed on the y-axis are calculated by the geom_bar() function under the hood. This quirk is something that confuses many new learners, so make sure you internalise this. Go ahead and complete the following code to create a bar chart.\nglobal_south |&gt;\n  ggplot(aes(x = status)) +\n  ...\nThe counting for the y-axis is done automatically. R looks in the status column and counts how many times each value in the column occurs in the data frame. The counts are then plotted as bars. If you did things correctly, you should get the following plot. The x-axis is now status and the y-axis corresponds to the number of languages by status (count).\nFigure 16.1: Number of languages by endangerment status.\nYou could write a description of the plot that goes like this:\nWhat if we want to show the number of languages by endangerment status within each of the macro-areas that make up the Global South? Easy! You can make a stacked bar chart.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>More plotting</span>"
    ]
  },
  {
    "objectID": "ch-more-plotting.html#bar-charts",
    "href": "ch-more-plotting.html#bar-charts",
    "title": "16  More plotting",
    "section": "",
    "text": "TipBar charts\n\n\n\nBar charts are useful when you are counting things. For example:\n\nNumber of verbs vs nouns vs adjectives in a corpus.\nNumber of languages by geographic area.\nNumber of correct vs incorrect responses.\n\nThe bar chart geometry is geom_bar().\n\n\n\n\n\n\n\n\n\nThe number of languages in the Global South by endangered status is shown as a bar chart in Figure 16.1. Among the languages that are endangered, the majority are threatened or shifting.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>More plotting</span>"
    ]
  },
  {
    "objectID": "ch-more-plotting.html#stacked-bar-charts",
    "href": "ch-more-plotting.html#stacked-bar-charts",
    "title": "16  More plotting",
    "section": "16.2 Stacked bar charts",
    "text": "16.2 Stacked bar charts\nA special type of bar charts are the so-called stacked bar charts.\n\n\n\n\n\n\nTipStacked bar chart\n\n\n\nA stacked bar chart is a bar chart in which each bar contains a “stack” of shorter bars, each indicating the counts of some sub-groups.\nThis type of plot is useful to show how counts of something vary depending on some other grouping (in other words, when you want to count the occurrences of a categorical variable based on another categorical variable). For example:\n\nNumber of languages by endangerment status, grouped by geographic area.\nNumber of infants by head-turning preference, grouped by first language.\nNumber of past vs non-past verbs, grouped by verb class.\n\n\n\nTo create a stacked bar chart, you just need to add a new aesthetic mapping to aes(): fill. The fill aesthetic lets you fill bars or areas with different colours depending on the values of a specified column. Let’s make a plot on language endangerment by macro-area. Complete the following code by specifying that fill should be based on status.\n\nglobal_south |&gt;\n  ggplot(aes(x = Macroarea, ...)) +\n  geom_bar()\n\nYou should get the following.\n\n\n\n\n\n\n\n\nFigure 16.2: Number of languages by macro-area and endangerment status.\n\n\n\n\n\nA write-up example:\n\nFigure 16.2 shows the number of languages by geographic macro-area, subdivided by endangerment status. Africa, Eurasia and Papunesia have substantially more languages than the other areas.\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nWhat is wrong in the following code?\ngestures |&gt;\n  ggplot(aes(x = status), fill = Macroarea) +\n  geom_bar()",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>More plotting</span>"
    ]
  },
  {
    "objectID": "ch-more-plotting.html#filled-stacked-bar-charts",
    "href": "ch-more-plotting.html#filled-stacked-bar-charts",
    "title": "16  More plotting",
    "section": "16.3 Filled stacked bar charts",
    "text": "16.3 Filled stacked bar charts\nIn the plot above it is difficult to assess whether different macro-areas have different proportions of endangerment. This is because the overall number of languages per area differs between areas. A solution to this is to plot proportions instead of raw counts. You could calculate the proportions yourself, but there is a quicker way: using the position argument in geom_bar(). You can plot proportions instead of counts by setting position = \"fill\" inside geom_bar(), like so:\n\nglobal_south |&gt;\nggplot(aes(x = Macroarea, fill = status)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nFigure 16.3: Proportion of languages by macro-area and endangerment status.\n\n\n\n\n\nThe plot now shows proportions of languages by endangerment status for each area separately. Note that the y-axis label is still “count” but should be “proportion”. Use labs() to change the axes labels and the legend name.\n\nglobal_south |&gt;\nggplot(aes(x = Macroarea, fill = status)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    ...\n  )\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nIf to change the name of the colour legend, you use the colour argument in labs(), guess which argument you should use for fill?\n\n\n\nYou should get this.\n\n\n\n\n\n\n\n\nFigure 16.4: Proportion of languages by macro-area and endangerment status.\n\n\n\n\n\nWith this plot it is easier to see that different areas have different proportions of endangerment. In writing:\n\nFigure 16.4 shows proportions of languages by endangerment status for each macro-area. Australia, South and North America have a substantially higher proportion of extinct languages than the other areas. These areas also have a higher proportion of near extinct languages. On the other hand, Africa has the greatest proportion of non-endangered languages followed by Papunesia and Eurasia, while North and South America are among the areas with the lower proportion, together with Australia which has the lowest.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>More plotting</span>"
    ]
  },
  {
    "objectID": "ch-more-plotting.html#faceting-and-panels",
    "href": "ch-more-plotting.html#faceting-and-panels",
    "title": "16  More plotting",
    "section": "16.4 Faceting and panels",
    "text": "16.4 Faceting and panels\nSometimes we might want to separate the data into separate panels within the same plot. We can achieve that easily using faceting. Let’s revisit the plots from Chapter 15. We will use the winter2012/polite.csv data again. This is the plot you previously made. Try and reproduce it by writing the code yourself (you also have to read in the data!).\n\n\n\n\n\n\n\n\nFigure 16.5: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\nThat looks great, but we want to know if being a music student has an effect on the relationship of f0mn and H1H2. In the plot above, the aesthetics mappings are the following: f0mn on the x-axis, H1H2 on the y-axis, gender as colour. How can we separate data further depending on whether the participant is a music student or not (musicstudent)? We can create panels using facet_grid(). This function takes lists of variables to specify panels in rows and/or columns.\nFaceting a plot allows to split the plot into multiple panels, arranged in rows and columns, based on one or more variables. To facet a plot, use the facet_grid() function. The syntax is a bit strange. You can specify rows of panels with the rows argument and columns of panels with cols argument, but you have to include column names inside vars(), like this:\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(cols = vars(musicstudent)) +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 16.6: Scatter plot of mean f0 and H1-H2 difference in non-music students (left) vs music students (right).\n\n\n\n\n\nYou could write a description of this plot like this:\n\nFigure 16.6 shows mean f0 and H1-H2 difference as a scatter plot. The two panels indicate whether the participant was a student of music. Within each panel, the participant’s gender is represented by colour (red for female and blue for male). Male participants tend to have higher H1-H2 differences and lower mean f0 than females. From the plot it can also be seen that there is greater variability in H1-H2 difference in female music students compared to female non-music participants. Within each group of gender by music student there does not seem to be any specific relation between mean f0 and H1-H2 difference.\n\nThe polite data also has a column attitude with values inf for informal and pol for polite. Subjects were asked to speak either as if they were talking to a friend (inf attitude) or to someone with a higher status (pol attitude). Recreate the last plot, this time faceting also by attitude. Use the rows column to create two separate rows for each value of attitude.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(cols = vars(musicstudent), rows = ...)\n\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nWrite a description for the last plot.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>More plotting</span>"
    ]
  },
  {
    "objectID": "ch-more-plotting.html#summary",
    "href": "ch-more-plotting.html#summary",
    "title": "16  More plotting",
    "section": "16.5 Summary",
    "text": "16.5 Summary\n\n\n\n\n\n\n\nCreate bar charts with geom_bar() to show counts.\nUse stacked bar charts to show groupings within counts and filled stacked bar charts to show proportions.\nYou can create panels with facet_grid().",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>More plotting</span>"
    ]
  },
  {
    "objectID": "ch-research-cycle.html",
    "href": "ch-research-cycle.html",
    "title": "17  Research cycle",
    "section": "",
    "text": "17.1 Researcher’s degrees of freedom\nIn Chapter 1, you learned about the research process, which includes the research context, data acquisition, data analysis and communication. A different perspective on the research process that highlights the temporal succession of the process steps is the research cycle, represented in an idealised form in Figure 17.1.\nThe cycle starts with the development of research questions and hypotheses. This step involves a thorough literature review and the identification of the topic, research problem, goal, questions and, possibly, hypotheses (as described in Chapter 2). Once the research questions and hypotheses have been determined, the researcher proceeds with the design of the study which sets out to answer the research questions and assess the research hypotheses. The study design process includes determining a large number of interconnected aspects, like materials, procedures, data management and data analysis plans, target population, sampling method and so on. At times the study design process reveals shortcomings or unforeseen aspects of the research questions/hypotheses which can be updated accordingly.\nOnce the study design has been finalised, one proceeds with acquiring data based on the protocols detailed in their plan. After the completion of data acquisition, researchers analyse data and interpret the results in light of the research questions and hypotheses. Finally, the outcomes of the study are published in some form and the next study cycle begins once again.\nThis sounds all very reasonable, but in reality, the researchers’ practice is quite different. This chapter introduces the concept of “researcher’s degrees of freedom” and describes the so-called Questionable Research Practices (QRPs). We will review literature that shows the grim reality of how common QRPs are. In ?sec-open-research, you will learn about principles and tools that are designed to help minimise the presence and impact of QRPs in one’s own research.\nData analysis involves many decisions, such as how to operationalise and measure a given phenomenon or behaviour, which data to submit to statistical modelling and which to exclude in the final analysis, or which inferential approach to employ. This “freedom” can be problematic because humans show cognitive biases that can lead to erroneous inferences (Tversky and Kahneman 1974). For example, humans are prone to see coherent patterns even in the absence of them (Brugger 2001), convince themselves of the validity of prior expectations by cherry-picking evidence (aka confirmation bias, “I knew it,” Nickerson 1998), and perceive events as being plausible in hindsight (“I knew it all along,” Fischhoff 1975). In conjunction with an academic incentive system that rewards certain discovery processes more than others (Koole and Lakens 2012; Sterling 1959), we often find ourselves exploring many possible analytic pathways but reporting only a selected few depending on the quality of the narrative that we can achieve with them.\nThis issue is particularly amplified in fields in which the raw data lend themselves to many possible ways of being measured (Roettger 2019). Combined with a wide variety of conceptual and methodological traditions as well as varying levels of quantitative training across sub-fields, the inherent flexibility of data analysis might lead to a vast plurality of analytic approaches that can itself lead to different scientific conclusions (Roettger, Winter, and Baayen 2019). Analytic flexibility has been widely discussed from a conceptual point of view (Nosek and Lakens 2014; Simmons, Nelson, and Simonsohn 2011; Wagenmakers et al. 2012) and in regard to its application in individual scientific fields (e.g., Charles et al. 2019; Roettger, Winter, and Baayen 2019; Wicherts et al. 2016). This notwithstanding, there are still many unknowns regarding the extent of analytic plurality in practice.\nConsequently, a substantial body of published articles likely present overconfident interpretations of data and statistical results based on idiosyncratic analytic strategies (e.g., Gelman and Loken (2014); Simmons, Nelson, and Simonsohn (2011)). These interpretations, and the conclusions that derive from them, are thus associated with an unknown degree of uncertainty (dependent on the strength of evidence provided) and with an unknown degree of generalizability (dependent on the chosen analysis). Moreover, the same data could lead to very different conclusions depending on the analytic path taken by the researcher. However, instead of being critically evaluated, scientific results often remain unchallenged in the publication record.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Research cycle</span>"
    ]
  },
  {
    "objectID": "ch-research-cycle.html#researchers-degrees-of-freedom",
    "href": "ch-research-cycle.html#researchers-degrees-of-freedom",
    "title": "17  Research cycle",
    "section": "",
    "text": "This section is reproduced from Coretta et al. (2023) (CC-BY-NC) with minor edits.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Research cycle</span>"
    ]
  },
  {
    "objectID": "ch-research-cycle.html#questionable-research-practices",
    "href": "ch-research-cycle.html#questionable-research-practices",
    "title": "17  Research cycle",
    "section": "17.2 Questionable Research Practices",
    "text": "17.2 Questionable Research Practices\n\n\n\n\n\n\nThis section contains text from Coretta (2020) (CC-BY-NC) .\n\n\n\n\n\n\n\n\n\nFigure 17.2: The research cycle and questionable research practices\n\n\n\nQuestionable research practices are practices, whether intentional or not, that undermine the robustness of research (Simmons, Nelson, and Simonsohn 2011; Morin 2015; Flake and Fried 2020). Questionable research practices are practices that negatively affect the research enterprise, but that are employed (most of the time unintentionally) by a surprisingly high number of researchers (John, Loewenstein, and Prelec 2012). For each step in the research cycle, questionable practices are available to researchers. These are part of the researcher’s degrees of freedom, introduced in the previous section. In this section, we will briefly review some of the most common questionable research practices identified in the literature.\nMakel, Plucker, and Hegarty (2012) looked at the publication history of 100 psychological journals since 1900. They found that only 1.07% of the papers (that is, 1 in 100 papers) were replications of previous studies. This means that the vast majority of studies are run only once and the field moves on. As Tukey (1969, 84) said, “Confirmation comes from repetition. Any attempt to avoid this statement leads to failure and more probably to destruction”. This lack of replication attempts is problematic, given than we can’t be certain the results obtain from the one study would replicate if the study is run again. While the study in Makel, Plucker, and Hegarty (2012) focused on psychology, Kobrock and Roettger (2023) find that linguistics shows a more dire situation: only 0.08% of experimental articles contains an independent direct replication (1 in 1250).\nAnother issue that affects modern research regards study design, including aspects related to sample size. Several studies have found that most research employs study designs that grant a 50% probability of being able to find effects of medium size (Cohen 1962; Sedlmeier and Gigerenzer 1992; Bezeau and Graves 2001). Gaeta and Brydges (2020) find a similar scenario in speech, language and hearing research: the majority of studies they screened did not have an adequate sample size to be able to detect medium-sized effects.\nIn a study about the prevalence of questionable research practices, John, Loewenstein, and Prelec (2012) found that about 50% of the researchers interviewed admitted to selective reporting, i.e. reporting only some of the statistical analyses or studies conducted. Combined with a theoretical admission rate, the authors argue for a 100% rate of selective reporting (in other words, we can expect all published studies to be affected by selective reporting). They also found that about 35% of the researchers admitted to having changed the research question/hypothesis after seeing the results (or “claiming to have predicted an unexpected finding”), also known as HARKing (Hypothesising After the Results are Known, Kerr 1998). Combined with the theoretical admission rate, they estimate an actual rate of 90%.\nWe will talk more about sharing research data when you will learn about Open Research practices in ?sec-open-research, but Wicherts et al. (2006) contacted the authors of 141 articles in psychology asking to share the research data with them and a worrying 73% of the authors never shared their data. Bochynska et al. (2023) surveyed 600 linguistic articles and less than 10% of them shared their data as part of the publication.\nPublication bias is used to refer to the bias towards publishing “positive” results (i.e. results that indicate the presence of an effect). Fanelli (2010); Fanelli (2012) found that about 80% of published results are positive results across disciplines, while the prevalence of positive results was higher in fields like psychology and economics (about 90%). Of course, the very high prevalence of positive results indicates that a lot of “negative” results (i.e. results that don’t suggest the presence of an effect) are not published, because in a neutral scenario (where researchers propose and test hypotheses, in an iterative process), there should be many more negative results. Ioannidis (2005), for example, shows through computational modelling that a prevalence rate of positive results of 50% or above would be very difficult to obtain and concludes that “most published research findings are false”. Relatedly, Nissen et al. (2016) also use computational modelling to show how false claims can frequently become canonized as fact, in the absence of sufficient negative results. Further to these points, Scheel (2022) stresses that “most psychological research findings are not even wrong”, in that most claims made in the literature are “so critically underspecified that attempts to empirically evaluate them are doomed to failure” (Scheel 2022, 1).\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\na. True or false?\n\nIn the research cycle, hypotheses are always fixed after the study design is finalised and cannot be changed. TRUEFALSE\nResearcher’s degrees of freedom refers to the many decisions involved in data analysis, which can influence outcomes and interpretations. TRUEFALSE\nPublication bias describes the tendency for journals to publish studies with negative results more often than those with positive results. TRUEFALSE\n\nb. Which of the following is considered a Questionable Research Practice.\n\n Running a replication study to confirm findings. Selectively reporting only some of the analyses conducted. Increasing sample size to ensure adequate statistical power. Publishing negative results alongside positive ones.\n\n\n\n\n\n\n\nBezeau, Scott, and Roger Graves. 2001. “Statistical Power and Effect Sizes of Clinical Neuropsychology Research.” Journal of Clinical and Experimental Neuropsychology 23 (3): 399–406. https://doi.org/10.1076/jcen.23.3.399.1181.\n\n\nBochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas, Irys-Amélie Champagne, Kaidi Chen, Melanie Röthlisberger, Erin M. Buchanan, and Timo B. Roettger. 2023. “Reproducible Research Practices and Transparency Across Linguistics.” Glossa Psycholinguistics 2 (1). https://doi.org/10.5070/g6011239.\n\n\nBrugger, Peter. 2001. “From Haunted Brain to Haunted Science: A Cognitive Neuroscience View of Paranormal and Pseudoscientific Thought.” Hauntings and Poltergeists: Multidisciplinary Perspectives, 195213.\n\n\nCharles, Sarah J., James E. Bartlett, Kyle J. Messick, Thomas J. Coleman, and Alex Uzdavines. 2019. “Researcher Degrees of Freedom in the Psychology of Religion.” The International Journal for the Psychology of Religion 29 (4): 230245.\n\n\nCohen, Jacob. 1962. “The Statistical Power of Abnormal-Social Psychological Research: A Review.” The Journal of Abnormal and Social Psychology 65 (3): 145–53. https://doi.org/10.1037/h0045186.\n\n\nCoretta, Stefano. 2020. “Open Science in Phonetics and Phonology.” https://doi.org/10.31219/osf.io/4dz5t.\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke, Byron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023. “Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses.” Advances in Methods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nFanelli, Daniele. 2010. “Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data.” Edited by Enrico Scalas. PLoS ONE 5 (4): e10271. https://doi.org/10.1371/journal.pone.0010271.\n\n\n———. 2012. “Negative Results Are Disappearing from Most Disciplines and Countries.” Scientometrics 90 (3): 891–904. https://doi.org/10.1007/s11192-011-0494-7.\n\n\nFischhoff, Baruch. 1975. “Hindsight Is Not Equal to Foresight: The Effect of Outcome Knowledge on Judgment Under Uncertainty.” Journal of Experimental Psychology: Human Perception and Performance 1 (3): 288.\n\n\nFlake, Jessica Kay, and Eiko I. Fried. 2020. “Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them.” Advances in Methods and Practices in Psychological Science 3 (4): 456465. https://doi.org/10.1177/2515245920952393.\n\n\nGaeta, Laura, and Christopher R. Brydges. 2020. “An Examination of Effect Sizes and Statistical Power in Speech, Language, and Hearing Research.” Journal of Speech, Language, and Hearing Research 63 (5): 15721580. https://doi.org/10.1044/2020_jslhr-19-00299.\n\n\nGelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in Science: Data-Dependent Analysis. A “Garden of Forking Paths”explains Why Many Statistically Significant Comparisons Don’t Hold Up.” American Scientist 102 (6): 460466.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1080/09332480.2019.1579573.\n\n\nJohn, Leslie K., George Loewenstein, and Drazen Prelec. 2012. “Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.” Psychological Science 23 (5): 524532. https://doi.org/10.1177/0956797611430953.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” Personality and Social Psychology Review 2 (3): 196217. https://doi.org/10.1207/s15327957pspr0203_4.\n\n\nKobrock, Kristina, and Timo B. Roettger. 2023. “Assessing the Replication Landscape in Experimental Linguistics.” Glossa Psycholinguistics 2 (1). https://doi.org/10.5070/g6011135.\n\n\nKoole, Sander L, and Daniël Lakens. 2012. “Rewarding Replications: A Sure and Simple Way to Improve Psychological Science.” Perspectives on Psychological Science 7 (6): 608614.\n\n\nMakel, Matthew C., Jonathan A. Plucker, and Boyd Hegarty. 2012. “Replications in Psychology Research: How Often Do They Really Occur?” Perspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMorin, Olivier. 2015. “A Plea for “Shmeasurement” in the Social Sciences.” Biological Theory 10 (3): 237245. https://doi.org/10.1007/s13752-015-0217-z.\n\n\nNickerson, Raymond S. 1998. “Confirmation Bias: A Ubiquitous Phenomenon in Many Guises.” Review of General Psychology 2 (2): 175220. https://doi.org/10.1037/1089-2680.2.2.175.\n\n\nNissen, Silas Boye, Tali Magidson, Kevin Gross, and Carl T. Bergstrom. 2016. “Publication Bias and the Canonization of False Facts.” Elife 5: e21451. https://doi.org/10.7554/eLife.21451.\n\n\nNosek, Brian A, and Daniël Lakens. 2014. “A Method to Increase the Credibility of Published Results.” Social Psychology 45 (3): 137141.\n\n\nRoettger, Timo B. 2019. “Researcher Degrees of Freedom in Phonetic Sciences.” Laboratory Phonology: Journal of the Association for Laboratory Phonology 10 (1): 127.\n\n\nRoettger, Timo B., Bodo Winter, and Harald Baayen. 2019. “Emergent Data Analysis in Phonetic Sciences: Towards Pluralism and Reproducibility.” Journal of Phonetics 73: 17. https://doi.org/10.1016/j.wocn.2018.12.001.\n\n\nScheel, Anne M. 2022. “Why Most Psychological Research Findings Are Not Even Wrong.” Infant and Child Development 31 (1): e2295. https://doi.org/10.1002/icd.2295.\n\n\nSedlmeier, Peter, and Gerd Gigerenzer. 1992. “Do Studies of Statistical Power Have an Effect on the Power of Studies?” In, 389–406. Washington: American Psychological Association. https://doi.org/10.1037/10109-032.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” Psychological Science 22 (11): 13591366.\n\n\nSterling, Theodore D. 1959. “Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significanceor Vice Versa.” Journal of the American Statistical Association 54 (285): 3034.\n\n\nTukey, John W. 1969. “Analyzing Data: Sanctification or Detective Work?” American Psychologist 24 (2): 83–91. https://doi.org/10.1037/h0027108.\n\n\nTversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases: Biases in Judgments Reveal Some Heuristics of Thinking Under Uncertainty.” Science 185 (4157): 11241131. https://doi.org/10.1126/science.185.4157.1124.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der Maas, and Rogier A. Kievit. 2012. “An Agenda for Purely Confirmatory Research.” Perspectives on Psychological Science 7 (6): 632638. https://doi.org/10.1177/1745691612463078.\n\n\nWicherts, Jelte M., Denny Borsboom, Judith Kats, and Dylan Molenaar. 2006. “The Poor Availability of Psychological Research Data for Reanalysis.” American Psychologist 61 (7): 726.\n\n\nWicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen. 2016. “Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking.” Frontiers in Psychology 7. https://doi.org/10.3389/fpsyg.2016.01832.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Research cycle</span>"
    ]
  },
  {
    "objectID": "ch-probability.html",
    "href": "ch-probability.html",
    "title": "18  Probability distributions",
    "section": "",
    "text": "18.1 Probabilities\nProbability as a discipline is the study of chance and uncertainty. It provides a systematic way to describe and reason about events whose outcomes cannot be predicted with certainty. In everyday life, probabilities are used to talk about situations ranging from rolling dice and drawing cards to forecasting the weather or assessing risks. A probability is expressed as a number between 0 and 1, where 0 means the event is impossible, 1 means it is certain, and values in between reflect varying degrees of probability. So for example if I say the probability of rain tomorrow is 0, it means that raining tomorrow is impossible. Conversely, if I say that the probability of rain tomorrow is 1, I mean that raining tomorrow is certain: it will happen. Probabilities are also expressed as percentages: so 0 is 0% and 1 is 100% percent. An 80% probability of rain tomorrow is a high probability, but not quite certainty. Thinking in terms of probability allows us to quantify uncertainty and to make informed statements about how likely different outcomes are, even when we cannot predict exactly what will happen.\nThe rules of probability ensure that these numbers behave consistently. The non-negativity rule states that no probability can be less than 0. The normalization rule requires that the probability of all possible outcomes of a situation must add up to exactly 1, which guarantees that something in the set of possible outcomes will happen. The addition rule tells us that if two events cannot both occur at the same time—such as rolling a 3 or rolling a 5 on a single die throw—the probability of either happening is the sum of their individual probabilities. The multiplication rule applies when two events are independent, meaning the outcome of one does not affect the other—for instance, tossing a coin and rolling a die. In that case, for example, the probability of getting heads and a 3 together is the product of their individual probabilities (i.e. the probability of getting heads, 1/2 or 0.5, and the probability of getting 3, 1/6 or 0.166): if we multiply 0.5 by 0.166, we get approximately 0.083. So there is an 8.3% probability of getting heads and a 3 when flipping a coin and rolling a die. These rules provide the logical foundation for reasoning about probabilities and serve as the basis for describing probability distributions, which organize and model probabilities across a whole set of possible outcomes. However, you will see that in practice you will rarely have to use them, yourself.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#probabilities",
    "href": "ch-probability.html#probabilities",
    "title": "18  Probability distributions",
    "section": "",
    "text": "NoteQuiz 1\n\n\n\nTrue or false?\n\nA certain event is an event that has a probability equal to or greater than 1. TRUEFALSE\nAn event that has probability of 0 is an impossible event. TRUEFALSE\nProbabilities are expressed with a number between 0 and 1 (inclusive). TRUEFALSE\nWhen one event cannot occur if another does occur, these are called equally likely events. TRUEFALSE",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#probability-distributions",
    "href": "ch-probability.html#probability-distributions",
    "title": "18  Probability distributions",
    "section": "18.2 Probability distributions",
    "text": "18.2 Probability distributions\n\n\n\n\n\nA probability distribution is a way of describing how probabilities are assigned to all possible outcomes of a random process. Conceptually, you can think of a probability distribution as a distribution of probabilities: i.e. a list of possible outcomes (values) and their probability. Instead of focusing on the probability of a single event (like getting a 4 on a die), a distribution gives the full picture: it tells us the probability of every possible value a random variable can take. For example, when rolling a fair six-sided die, the probability distribution assigns a probability of \\(1/6\\) to each face, reflecting that all outcomes (i.e. all numbers from 1 to 6) are equally likely. In other cases, probabilities may not be spread evenly, as with a biased coin or the distribution of heights in a population (there are more people of mean height that very short and very tall people). By summarizing the probability of all outcomes at once, probability distributions allow us to see patterns in a clear and structured way.\nThere are two broad types of probability distributions: discrete and continuous probability distributions. Discrete probability distributions apply to discrete variables, such as the result of a dice roll, the number of words known by an infant, or the accuracy of a response (correct vs incorrect). Here, probabilities are assigned to distinct values, and the total across all possible outcomes must equal 1. So, on a six-sided die, each outcome has a \\(1/6\\) (one in six) probability and since there are six outcomes, \\(1/6 * 6 = 1\\). Continuous probability distributions, on the other hand, are used when outcomes can take on any value within a range, such as height, reaction times, or phone duration. In these cases, probabilities are described by smooth curves rather than discrete points, and instead of assigning probability to individual values, we consider intervals, like for example, the probability that a person’s height lies between 160 cm and 170 cm (more on this in Section 19.2 below). Figure 18.1 shows an example of a categorical probability distribution (the probability of respondents answering “no” or “yes” in a survey) and a continuous probability distribution (the proportion of voicing during closure of a stop).\n\nCode\np &lt;- 0.8\nbernoulli_df &lt;- tibble(\n  x = c(\"No\", \"Yes\"),\n  probability = c(1 - p, p)\n)\n\nggplot(bernoulli_df, aes(x = factor(x), y = 0 , yend = probability)) +\n  geom_segment(colour = \"steelblue\", linewidth = 2) +\n  geom_point(aes(y = probability), colour = \"steelblue\", size = 5) +\n  labs(x = element_blank(), y = \"Probability\") +\n  ylim(0, 1)\n\nalpha &lt;- 1.5\nbeta &lt;- 4\nbeta_df &lt;- tibble(\n  x = seq(0, 1, length.out = 100),\n  density = dbeta(x, alpha, beta)\n)\n\nggplot(beta_df, aes(x = x, y = density)) +\n  geom_line(color = \"darkorange\", linewidth = 1.2) +\n  labs(x = \"Proportion of voicing\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Categorical probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n(b) Continuous probability distribution.\n\n\n\n\n\n\n\nFigure 18.1: Example of a categorical probability distributiona and a continous probability distribution.\n\n\n\n\n\n\n\n\n\nTipProbability distributions\n\n\n\nA probability distribution describes how probabilities are distributed across outcomes of a random variable.\nThere are two main types: discrete probability distributions and continuous probability distributions.\n\n\nSeveral well-known distributions serve as fundamental building blocks in probability and statistics. Among discrete distributions, the binomial distribution describes the number of successes in a fixed number of independent trials (like accuracy data from a behavioural task), while the Poisson distribution is used for counting events that occur randomly over time or space (such as number of relatives sentences in a corpus). In the continuous case, the Gaussian distribution (which we will explore below) has many useful mathematical properties and it features prominently in any statistical textbook. Other continuous distributions are, for example, the beta distribution, illustrated in Figure 18.1 (b), used for continuous variables bounded between 0 and 1, and the uniform distribution, which distributes probabilities equally across the entire range of real numbers (so it is a “flat” distribution, since it looks like a horizontal line).\nProbability distributions are more than just mathematical descriptions—they provide tools for making sense of variation and uncertainty in the world. They allow us to calculate probabilities for complex events, to compare different random processes, and to build models that reflect real-world randomness. Once the distribution of a random variable is known, we can derive useful summaries such as averages, variability, and the probability of extreme outcomes. In this way, probability distributions bridge the abstract rules of probability with the practical task of describing how probability operates across a whole set of possibilities.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#probability-mass-and-density-functions",
    "href": "ch-probability.html#probability-mass-and-density-functions",
    "title": "18  Probability distributions",
    "section": "18.3 Probability mass and density functions",
    "text": "18.3 Probability mass and density functions\nHow is the probability of different outcomes of ranges of outcomes calculated? For discrete distributions, the probability mass function (PMF) is used to compute probabilities. Let’s take the Bernoulli probability distribution from figure Figure 18.1 (a): the PMF of a Bernoulli distribution is \\(p\\) for the probability of the outcome being 1, and \\(q = 1 - p\\) for the probability of the outcome being 0. In the figure, 1 = “Yes” and 0 = “No”. There is a probability \\(p = 0.8\\) of getting a “Yes”, hence there is a probability \\(q = 1 - p = 1- 0.8 = 0.2\\) of getting a “No”. Continuous probability distributions use probability density functions (PDFs, nothing to do with the file format): these don’t tell you the probability of a specific value, but rather the probability density or in other words how dense the probability is around a point. I will spare you the mathematical details of PDFs, but they are the reason for using density plots. You’ve encountered a density plot already, in Figure 18.1 (b) above. The curve you see in that figure is calculated with the PDF of the beta distribution. Because we used a PDF, this is a theoretical probability distribution. In practice, you will almost never have to use PMFs and PDFs yourself, but it helps to know about them.\n\n\n\n\n\n\nTipPMF and PDF\n\n\n\nThe Probability Mass Function (PMF) and the Probability Density Function (PDF) are mathematical functions used to compute theoretical probability distributions.\n\n\nWhat if we want the density of a sample from a random variable, like logged RTs from the MALD data set (Tucker et al. 2019)? Obtaining the density curve of a sample is done with Kernel Density Estimation (KDE): this is a function that creates a smooth, continuous estimate of a probability density from a finite set of data points (the sample). As with PMFs and PDFs, you will very rarely have to use KDE directly, since R applies it for you. So let’s see how to make a density plot in ggplot2.\n\n\n\n\n\n\nTipKernel Density Estimation (KDE)\n\n\n\nKernel Density Estimation is a function that creates a smooth estimate of a probability density from a finite set of data points. It returns an empirical probability distribution.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#density-plots",
    "href": "ch-probability.html#density-plots",
    "title": "18  Probability distributions",
    "section": "18.4 Density plots",
    "text": "18.4 Density plots\nYou can create a density plot (i.e. a plot that shows the density curve as obtained from KDE) of sample values from a continuous variable with the density geometry: geom_density().\n\n# First read the data\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nmald |&gt; \n  ggplot(aes(RT)) +\n  geom_density() +\n  labs(x = \"Reaction Times (ms)\")\n\n\n\n\n\n\n\nFigure 18.2: Density plot of reaction times.\n\n\n\n\n\nFigure 18.2 shows the density of reaction times from the MALD data set, in milliseconds. The higher the curve, the higher the density around the values below that part of the curve. In this sample of RTs, the density is high around about 900 ms. It drops quite sharply below 900 ms to about 500, while on the other side of 900 it has a more gentle slope. When a density plot looks like that, we say the distribution is right-skewed or that is has positive skew. This is because the distribution is skewed towards larger values. We can visualise this better by adding a “rug” to the plot with geom_rug(). This geometry adds a tick below the curve, on the x-axis, for each value in the sample. Look at Figure 18.3. Note how dense the ticks are where the density is high, and how sparse the ticks are where the density is lower. This makes sense, that’s what the density represents. Setting alpha = 0.1 makes the ticks transparent so that the denseness is even more obvious (darker areas mean greater density because the ticks overlap, thus becoming darker). Now also notice how there are many more ticks to the right of the highest part of the density than to the left. This is the right-skewness we were talking about. This aspect will be relevant when you will learn about regression models in later chapters, but in fact we will not directly address this again until ?sec-lognormal.\n\nmald |&gt; \n  ggplot(aes(RT)) +\n  geom_density() +\n  geom_rug(alpha = 0.1) +\n  labs(x = \"Reaction Times (ms)\")\n\n\n\n\n\n\n\nFigure 18.3: Density plot of reaction times with rug.\n\n\n\n\n\nDensity plots can also be made for different groupings, like in the following plot where we show the density of RTs depending on the lexical status of the word (real or non-real). You can use the fill aesthetics to fill the area under the density curve with colour by IsWord. Figure 18.4 shows the densities of RTs depending on lexical status. It is subtle, but we can see that the density peak for nonce words (IsWord = FALSE) is to the right of the peak for real words. This means that we can expect on average higher RTs for nonce words than for real words. Moreover, the curve for nonce words is overall lower than that for real words: this means that RTs with real words are more tightly concentrated around the peak, while RTs with nonce words are more spread.\n\nmald |&gt; \n  ggplot(aes(RT, fill = IsWord)) +\n  geom_density(alpha = 0.7) +\n  geom_rug(alpha = 0.1) +\n  scale_fill_brewer(type = \"qual\") +\n  scale_color_brewer(type = \"qual\") +\n  labs(x = \"Reaction Times (ms)\")\n\n\n\n\n\n\n\nFigure 18.4: Density plot of reaction times by lexical status.\n\n\n\n\n\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nRead winter2016/senses_valence.csv and produce the following plot:\n\nA density plot of affective valence Val, with densities split and areas coloured by Modality.\nRemove the black density curve (only the filled area should be shown).\nAdd a rug, also coloured by Modality.\nChange the labels if you like.\nAny interesting patterns?\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\n\nTo remove an element of a geometry, like the line of density, you set colour to NA.\nBe careful with how you specify colour, since the density and rug geometry need different colour specifications.\n\n\n\n\n\n\n\n\n\n\nImportantSolution\n\n\n\n\n\nI got you! No solution for this exercise, just do a little internet research if necessary.\n\n\n\n\n\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-gaussian.html",
    "href": "ch-gaussian.html",
    "title": "19  Working with distributions",
    "section": "",
    "text": "19.1 The Gaussian distribution\nIn the previous section, we have seen that you can visualise probability distributions by plotting the probability mass or density function for theoretical probabilities and by using kernel density estimation for sample (aka empirical) distributions. Visualising probability distributions is more practical than listing all the possible values and their probability (especially with continuous variables—since they are continuous there is an infinite number of values!). Another convenient way to express probability distributions is to specify a set of parameters, which can reconstruct the entire distribution. With theoretical distributions, the parameters allow you to reconstruct exact distributions, while empirical distributions can usually be only approximated. That’s the whole point of taking a sample: you want to reconstruct the “underlying” probability distribution that generated the sample, in other words the (theoretical) probability distribution of the population.\nDifferent probability distribution families have a different number of parameters and different parameters. A probability family is an abstraction of specific probability distributions that can be represented with the same set of parameters. An example of a probability distribution family is the Gaussian [ˈgaʊsɪən] probability distribution, also called the “normal” distribution and nick-named the “bell-curve”, because it looks like the shape of a bell. Figure 19.1 should make this more obvious.\nCode\nggplot() +\n  aes(x = seq(-4, 4, 0.01), y = dnorm(seq(-4, 4, 0.01))) +\n  geom_path(colour = \"sienna\", linewidth = 2) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nFigure 19.1\nThe Gaussian distribution is a continuous probability distribution and it has two parameters:\nYou have already encountered means and standard deviations in Chapter 10. It is no coincidence that the go-to summary measures for continuous variables are the mean and the standard deviation. When you don’t know exactly what the underlying distribution of a variable is and all you want is a measure of central tendency and of dispersion, one assumes a Gaussian distribution and calculates mean and standard deviations. Note that in most cases we know a bit more than that and it fact the Gaussian distribution is very rare in nature. This is why we will call it Gaussian and not “normal”, since it is only “normal” from a statistical-theoretical perspective (it has simple mathematical properties that makes it easy to use in applied statistics).\nFigure 19.2 shows Gaussian distributions with fixed standard deviation (2) but different means (-5, 0, 10) in Figure 19.2 (a) and Gaussian distributions with fixed mean (5) but different SDs (1, 2, 4) in Figure 19.2 (b). The mean shifts the distribution horizontally (lower values to the left, higher values to the right), while the SD affects the width of the distribution: lower SDs correspond to a narrower or tighter distribution, while higher SDs correspond to a wider distribution. Since the total area under the curve has to sum to 1, if the distribution is narrower, the peak will also be relatively higher, while with a wider distribution the peak will be lower. You have seen this in Figure 18.4.\nIn statistical notation, we write the Gaussian distribution family like this:\n\\[\nGaussian(\\mu, \\sigma)\n\\]\nSpecific types of Gaussian distributions will have specific values for the parameters \\(\\mu\\) and \\(\\sigma\\): for example \\(Gaussian(0, 1)\\), \\(Gaussian(50, 32)\\), \\(Gaussian(2.5, 6.25)\\), and so on. All of these specific probability distributions belong to the Gaussian family. So hopefully you understand now why we say that a distribution family stands for specific families: here \\(Gaussian(\\mu, \\sigma)\\) stands as the parent of all the specific Gaussian distributions (i.e. all of the Gaussian distributions with a specific mean and SD).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Working with distributions</span>"
    ]
  },
  {
    "objectID": "ch-gaussian.html#the-gaussian-distribution",
    "href": "ch-gaussian.html#the-gaussian-distribution",
    "title": "19  Working with distributions",
    "section": "",
    "text": "The mean, represented with the Greek letter \\(\\mu\\) [mjuː]. This parameter is the probability’s central tendency. Values around the mean have higher probability than values further away from the mean.\nThe standard deviation, represented with the Greek letter \\(\\sigma\\) [ˈsɪgmə]. This parameter is the probability’s dispersion around the mean. The higher \\(\\sigma\\) the greater the spread (i.e. the dispersion) of values around the mean.\n\n\n\n\n\n\n\n\nTipGaussian distribution\n\n\n\nThe Gaussian distribution (also called “normal” and nick-named the “bell curve”) is a continuous probability distribution family defined by a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\).\n\\[\nGaussian(\\mu, \\sigma)\n\\]\n\n\n\n\nCode\nx &lt;- seq(-10, 20, length.out = 1000)\nmeans &lt;- c(0, -5, 10)\nsd_fixed &lt;- 2\n\ndf_means &lt;- crossing(x = x, mean = means) |&gt; \n  mutate(\n    y = dnorm(x, mean = mean, sd = sd_fixed),\n    mean = factor(mean)\n  ) |&gt; \n  arrange(mean, x)\n\nmu &lt;- ggplot(df_means, aes(x = x, y = y, color = mean)) +\n  geom_line(linewidth = 1) +\n  labs(x = element_blank(), y = \"Density\", caption = \"SD = 2.\")\n\nsds &lt;- c(1, 2, 4)\nmean_fixed &lt;- 5\n\ndf_sds &lt;- crossing(x = x, SD = sds) |&gt; \n  mutate(\n    y = dnorm(x, mean = mean_fixed, sd = SD),\n    SD = factor(SD)\n  ) |&gt; \n  arrange(SD, x)\n\nsig &lt;- ggplot(df_sds, aes(x = x, y = y, color = SD)) +\n  geom_line(linewidth = 1) +\n  labs(x = element_blank(), y = \"Density\", caption = \"Mean = 5.\")\n\nplot(mu)\nplot(sig)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Different means, same SD.\n\n\n\n\n\n\n\n\n\n\n\n(b) Same mean, different SDs.\n\n\n\n\n\n\n\nFigure 19.2: Illustrating Gaussian distributions with different means and standard deviations.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Working with distributions</span>"
    ]
  },
  {
    "objectID": "ch-gaussian.html#sec-intervals",
    "href": "ch-gaussian.html#sec-intervals",
    "title": "19  Working with distributions",
    "section": "19.2 Cumulative distribution function (CDF)",
    "text": "19.2 Cumulative distribution function (CDF)\nA useful way to investigate theoretical probability distributions is to ask what is the probability that the random variable the probability represents is less than or equal to a certain value. For example, for a distribution \\(Gaussian(0, 1)\\) of the random variable \\(X\\), what is the probability that \\(X\\) is -1 or less? Figure 19.3 gives us a visual explanation: the size of the shaded area under the density curve is the probability that \\(X \\leq -1\\). This works because the area under the density curve must add to 1, so that the entire area under the curve covers 100% of the probability distribution.\n\n\nCode\nq &lt;- -1\np &lt;- pnorm(q)\nlbl &lt;- sprintf(\"P(X \\u2264 %.0f) = %.4f\", q, p)\n\ndf &lt;- tibble(x = seq(-4, 4, length.out = 2000)) |&gt;\n  mutate(dens = dnorm(x))\n\ndf_shade &lt;- df |&gt; filter(x &lt;= q)\n\nggplot(df, aes(x, dens)) +\n  geom_line(linewidth = 1) +\n  geom_area(data = df_shade, aes(y = dens), alpha = 0.4) +\n  geom_vline(xintercept = q, linetype = \"dashed\") +\n  annotate(\"text\", x = q - 3, y = dnorm(0) * 0.4, label = lbl, hjust = 0) +\n  labs(\n    y = \"Density\", x = \"X\"\n  )\n\n\n\n\n\n\n\n\nFigure 19.3: Illustration of the lower-tail probability with Gaussian(0, 1).\n\n\n\n\n\nThe mathematical function that calculates the probability that a variable is less than or equal to a value is the cumulative distribution function (CDF). In R, you can get the CDF value of a Gaussian distribution (i.e. the probability that \\(X\\) is less than or equal to any value \\(x\\)) with the pnorm() function (p for probability and norm for normal or Gaussian). The function takes three arguments: the \\(x\\) value represented by the q argument, the mean and the SD of the Gaussian distribution (the default are mean = 0 and SD = 1).\n\npnorm(q = -1, mean = 0, sd = 1)\n\n[1] 0.1586553\n\n# or\npnorm(-1, 0, 1)\n\n[1] 0.1586553\n\n# or\npnorm(-1)\n\n[1] 0.1586553\n\n\n\n\n\n\n\n\nTipCumulative Distribution Function\n\n\n\nThe Cumulative Distribution Function (CDF) is a function that gives, for any value \\(x\\), the probability that the random variable \\(X\\) takes a value less than or equal to \\(x\\).\n\n\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nCalculate the probability that \\(X\\) is:\n\nless or equal than -2 with \\(Gaussian(0, 1)\\)\nless or equal than +1.75 with \\(Gaussian(0, 1)\\).\nless or equal than 700 with \\(Gaussian(900, 200)\\).\n\n\n\nBy default, pnorm() uses the lower-tail CDF: this returns the “less than or equal to” probability. It’s on the “lower” tail of the distribution, or the tail to the left of the density peak. But we can also compute the upper-tail probability. Figure 19.4 shows an upper-tail probability with \\(X \\geq -1\\) with \\(Gaussian(0, 1)\\). To obtain the upper-tail probability with pnorm(), rather than the lower-tail probability, set lower.tail to FALSE.\n\npnorm(-1, lower.tail = FALSE)\n\n[1] 0.8413447\n\n\n\n\nCode\nq &lt;- -1\np &lt;- pnorm(q, lower.tail = FALSE)\nlbl &lt;- sprintf(\"P(X \\u2265 %.0f) = %.4f\", q, p)\n\ndf &lt;- tibble(x = seq(-4, 4, length.out = 2000)) |&gt;\n  mutate(dens = dnorm(x))\n\ndf_shade &lt;- df |&gt; filter(x &gt;= q)\n\nggplot(df, aes(x, dens)) +\n  geom_line(linewidth = 1) +\n  geom_area(data = df_shade, aes(y = dens), alpha = 0.4) +\n  geom_vline(xintercept = q, linetype = \"dashed\") +\n  annotate(\"text\", x = q + 3, y = dnorm(0) * 0.4, label = lbl, hjust = 0) +\n  labs(\n    y = \"Density\", x = \"X\"\n  )\n\n\n\n\n\n\n\n\nFigure 19.4: Illustration of an upper-tail probability with Gaussian(0, 1).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Working with distributions</span>"
    ]
  },
  {
    "objectID": "ch-gaussian.html#intervals",
    "href": "ch-gaussian.html#intervals",
    "title": "19  Working with distributions",
    "section": "19.3 Intervals",
    "text": "19.3 Intervals\nProbability intervals provide a further way of locating and interpreting values within a probability distribution. They partition the distribution into regions associated with specified probability levels. A quantile is a value below which a given proportion of the distribution lies. You can think of this as the opposite of finding the probability given \\(x\\): given the probability \\(q\\), which is \\(x\\)? For a continuous distribution like the Gaussian, the q-th quantile (denoted \\(Q(q)\\)) is defined as the value \\(x\\) such that:\n\\[\nP(X \\leq x) = q, 0 \\leq q \\leq 1\n\\]\nwhich is, \\(q\\) is the probability that the outcome \\(X\\) is less than or equal to \\(x\\). So for example, given a Gaussian distribution with mean 0 and SD 1, which is the 0.15th quantile? To calculate a quantile, the quantile function is used. This is the inverse of the CDF. Figure 19.5 shows that the 0.15th quantile of a \\(Gaussian(0, 1)\\) distribution is approximately -1.04.\n\n\nCode\np &lt;- 0.15\nq &lt;- qnorm(p)\nlbl &lt;- sprintf(\"Q(%.2f) = %.4f\", p, q)\n\ndf &lt;- tibble(x = seq(-4, 4, length.out = 2000)) |&gt;\n  mutate(dens = dnorm(x))\n\ndf_shade &lt;- df |&gt; filter(x &lt;= q)\n\nggplot(df, aes(x, dens)) +\n  geom_line(linewidth = 1) +\n  geom_area(data = df_shade, aes(y = dens), alpha = 0.4) +\n  geom_vline(xintercept = q, linetype = \"dashed\") +\n  annotate(\"text\", x = q - 2.5, y = dnorm(0) * 0.4, label = lbl, hjust = 0) +\n  labs(\n    y = \"Density\", x = \"X\"\n  )\n\n\n\n\n\n\n\n\nFigure 19.5: The 0.15th quantile of Gaussian(0, 1).\n\n\n\n\n\n\n\n\n\n\n\nTipQuantile\n\n\n\nA quantile is a value below which a given proportion of a probability distribution lies.\n\n\n\n\n\n\n\n\nTipQuantile function\n\n\n\nThe quantile function is the inverse of the Cumulative Distribution Function (CDF) and returns a quantile.\n\n\n\n\n\n\n\n\nImportantSpotlight: PDF, CDF and quantile function of Gaussian distributions\n\n\n\n\n\nYou don’t really have to know the actual mathematical formulae of the PDF, CDF and quantile function of a distribution, because the computation is done for you by the respective R functions, but if you are mathematically inclined, here are the PDF, CDF and quantile function of a Gaussian distribution.\nPDF\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)\n\\]\nCDF\n\\[\nF(x) = \\frac{1}{2}\\left[1 + \\operatorname{erf}\\!\\left(\\frac{x-\\mu}{\\sigma \\sqrt{2}}\\right)\\right]\n\\]\nQuantile function (inverse CDF)\n\\[\nQ(p) = \\mu + \\sigma \\sqrt{2}\\,\\operatorname{erf}^{-1}(2p-1), \\quad 0&lt;p&lt;1\n\\]\n\n\n\nIn R, qnorm() returns quantiles using the quantile function. qnorm() takes three arguments: the probability, and the mean and SD of the Gaussian distribution (again, by default 0 and 1 respectively). As with pnorm(), you can obtain the upper-tail quantile with lower.tail = FALSE. With a Gaussian distribution with mean 0 and SD 1, the upper-tail quantile value is the same the lower-tail but with opposite sign. The following code shows how to use qnorm() (the values are rounded to the second digit with round(2)).\n\n# Lower-tail quantile with Gaussian(0, 1)\nqnorm(0.15) |&gt; round(2)\n\n[1] -1.04\n\n# Upper-tail quantile with Gaussian(0, 1)\nqnorm(0.15, lower.tail = FALSE) |&gt; round(2)\n\n[1] 1.04\n\n# Lower-tail quantile with Gaussian(10, 2)\nqnorm(0.15, 10, 2) |&gt; round(2)\n\n[1] 7.93\n\n# Upper-tail quantile with Gaussian(10, 2)\nqnorm(0.15, 10, 2, lower.tail = FALSE) |&gt; round(2)\n\n[1] 12.07\n\n\n\n19.3.1 Quartiles\nQuartiles are quantiles that split the distribution into four quarters, each holding 25% of the probability mass. With a Gaussian probability, the first quartile marks the point below which 25% of the area lies, the second quartile, also called the median (which you encountered in Chapter 10), splits it at 50%, and the third quartile leaves 25% above it, so that it covers 75% of the area. Because of the symmetry of the Gaussian, the first and third quartiles are equidistant from the mean. Figure 19.6 shows quartiles on a Gaussian distribution with mean 0 and SD 1. Note how the second quartile (Q2) splits the distribution in half: 50% of the distribution is to the left of Q2 and the other 50% is to the right of it. The interval between the first (Q1) and third quartile (Q3) is called the inter-quartile range (IQR), which indicates the middle 50% of the probability distribution.\n\n\nCode\nquartiles &lt;- qnorm(c(0, 0.25, 0.5, 0.75, 1))\nquart_labels &lt;- c(\"Q1\", \"Q2 (median)\", \"Q3\")\n\ndf &lt;- tibble(x = seq(-4, 4, length.out = 2000)) |&gt;\n  mutate(dens = dnorm(x))\n\ndf &lt;- df |&gt; mutate(\n  quartile = case_when(\n    x &lt;= quartiles[2] ~ \"Q1\",\n    x &lt;= quartiles[3] ~ \"Q2\",\n    x &lt;= quartiles[4] ~ \"Q3\",\n    TRUE ~ \"Q4\"\n  )\n)\n\nggplot(df, aes(x, dens, fill = quartile)) +\n  geom_area(alpha = 0.5) +\n  geom_line(linewidth = 1) +\n  scale_fill_brewer(type = \"seq\", direction = -1) +\n  geom_vline(xintercept = quartiles[2:4], linetype = \"dashed\", alpha = 0.5) +\n  annotate(\n    \"label\",\n    x = quartiles[2:4],\n    y = 0.2,\n    label = c(\"Q1\", \"Q2\", \"Q3\")\n  ) +\n  annotate(\n    \"text\",\n    x = c(-1.3, -0.35, 0.35, 1.3),\n    y = 0.05,\n    label = c(\"25%\", \"25%\", \"25%\", \"25%\")\n  ) +\n  annotate(\n    \"errorbar\",\n    xmin = quartiles[2], xmax = quartiles[4],\n    y = 0.1,\n    colour = \"purple\", linewidth = 1, width = 0.025\n  ) +\n  annotate(\n    \"text\",\n    x = 0, y = 0.12,\n    label = \"IQR\"\n  ) +\n  labs(\n    y = \"Density\", x = element_blank()\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 19.6: Quartiles of a Gaussian distribution.\n\n\n\n\n\nTo get the quartiles of a Gaussian distribution you can use the qnorm() function: a quartile is just a type of quantile. Q1 corresponds to the 0.25th quantile, Q2 to the 0.5th and Q3 to the 0.75th quantile.\n\nqnorm(c(0.25, 0.5, 0.75)) |&gt; round(2)\n\n[1] -0.67  0.00  0.67\n\n\n\n\n\n\n\n\nTipQuartiles\n\n\n\nQuartiles are quantiles that split the distribution into four quarters, each holding 25% of the probability mass.\n\n\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nCalculate the quartiles of the following Gaussian distributions.\n\n\\(Gaussian(10, 2)\\).\n\\(Gaussian(900, 200)\\).\n\\(Gaussian(-30, 10)\\).\n\n\n\n\n\n19.3.2 Percentiles\nAnother type of quantile are percentiles. These split the probability in 100 percentiles, each holding 1% of the probability mass. Percentiles are used to define central probability intervals, i.e. probability intervals that leave equal probability in both tails of the distribution. It is usually implied that you mean a central interval, so you don’t really have to say “central” every time. A (central) 95% interval is defined as the interval between 2.5th and the 97.5th percentile of the distribution. Figure 19.7 illustrates the 95% interval of \\(Gaussian(0, 1)\\). The shaded are is the 95% interval, while the two white areas at the tails hold each 2.5% of the distribution, thus making the rest 5% of the distribution not included in the 95% interval. Remember, probability intervals leave equal probability on both tails.\n\n\nCode\np &lt;- 0.15\n\ndf &lt;- tibble(x = seq(-4, 4, length.out = 2000)) |&gt;\n  mutate(dens = dnorm(x))\n\ndf_shade &lt;- df |&gt; filter(x &gt;= qnorm(0.025) & x &lt;= qnorm(0.975))\n\nggplot(df, aes(x, dens)) +\n  geom_area(data = df_shade, aes(y = dens), alpha = 0.4) +\n  geom_line(linewidth = 1) +\n  annotate(\n    \"label\", x = 0, y = 0.15, label = \"95% interval\"\n  ) +\n  annotate(\n    \"text\",\n    x = c(qnorm(0.025) - 0.1, qnorm(0.975) + 0.15), y = 0.1,\n    label = c(\"2.5th\", \"97.5th\")\n  ) +\n  labs(\n    y = \"Density\", x = \"X\"\n  )\n\n\n\n\n\n\n\n\nFigure 19.7: The 95% interval of Gaussian(0, 1).\n\n\n\n\n\n\n\n\n\n\n\nTipPercentiles\n\n\n\nPercentiles are quantiles that split the distribution into 100 quantiles, each holding 1% of the probability mass.\n\n\n\n\n\n\n\n\nTipCentral probability intervals\n\n\n\nCentral probability intervals are intervals that leave equal probability in both tails of the distribution.\nA 95% interval is defined as the interval between the 2.5th and the 97.5th percentile.\n\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhich of the following percentiles define an 80% interval?\n\n (0.1, 0.9) (0.2, 0.8) (0.05, 0.95)\n\nWhich of the following percentiles define a non-central 85% interval?\n\n (0.075, 0.925) (0.1, 0.95) (0.05, 0.95) (0.15, 0.85)\n\nWhich interval do the 40th and 60th percentile define?\n\n A 60% central interval. A 20% non-central interval. A 40% non-central interval. A 20% central interval.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Working with distributions</span>"
    ]
  },
  {
    "objectID": "ch-bayes.html",
    "href": "ch-bayes.html",
    "title": "20  Bayesian inference",
    "section": "",
    "text": "In Chapter 18 and Chapter 19 you learned about probabilities, probability distributions and probability intervals. Statistical inference (Chapter 6) is built on probability, but, while probabilities and distributions are precise mathematical concepts, their more philosophical interpretation varies depending on which stance one adopts. Among the two most common approaches to interpreting probability there are the frequentist and the Bayesian approach. Most of current research is carried out with frequentist methods. This is a historical accident, based on both an initial misunderstanding of Bayesian statistics (which is, by the way, older than frequentist statistics) and the fact that frequentist maths was much easier to work with (and personal computers did not exist). Despite the wide-spread use of frequentist statistics, this textbook (and related course) teaches you statistics in the Bayesian approach. There are several reasons for preferring Bayesian over frequentist statistics, both from a pedagogical and practical perspective, but until you learn more about frequentist statistics in Chapter 29, you will have to trust us for now.\nProbabilities in a frequentist framework are about average occurrences of events in a hypothetical series of repetitions of those events. Imagine you observe a volcano for a long period of time. The number of times the volcano erupts within that time tells us the frequency of occurrence of the event of volcanic eruption. In other words, it tells us its (frequentist) probability. In the Bayesian framework, probabilities are about the level of (un)certainty that an event will occur at any specific time given certain conditions. This is probably the way we normally think about probabilities: like in the weather forecast, if somebody tells you tomorrow it will rain with a probability of 85%, you intuitively know that it is very likely that it will rain tomorrow although it is not certain. In the context of research, a frequentist probability tells you the probability of obtaining the same result again and again given an imaginary series of replications of the study that generated that probability. On the other hand, a Bayesian probability tells you the probability of your hypothesis given the results of your study and your prior beliefs.\nBayesian inference approaches are now gaining momentum in many fields, including linguistics. The main advantage of Bayesian inference is that it allows researchers to answer research questions in a more straightforward way, using a more intuitive take on uncertainty and probability than what frequentist methods can offer. Bayesian inference is based on the concept of updating prior beliefs in light of new data. Given a set of prior probabilities and observations, Bayesian inference allows us to revise those prior probabilities and produce posterior probabilities. This is possible through the Bayesian interpretation of probabilities in the context of Bayes’ Theorem, which takes the name from Rev. Thomas Bayes (1701–1771).\nIn simple conceptual terms, the Bayesian interpretation of Bayes’ Theorem states that the probability of a hypothesis \\(h\\) given the observed data \\(d\\) is proportional to the product of the prior probability of \\(h\\) and the probability of \\(d\\) given \\(h\\).\n\\[\nP(h|d) \\sim P(h) \\cdot P(d|h)\n\\]\nThe prior probability \\(P(h)\\) represents the researcher’s beliefs towards \\(h\\). These beliefs can be based on expert knowledge, previous studies or mathematical principles.\nLet’s see a practical example of Bayesian updating based on the “globe-tossing” scenario described in McElreath (2020), Ch 2 (originally from Gelman, Nolan, and Nolan (2011)). Imagine holding a small globe that represents Earth. You want to know what fraction of its surface is covered by water. To estimate this, you adopt a simple method: toss the globe into the air, and when you catch it, note whether the spot under your right index finger is water (W) or land (L). Then toss it again and repeat. This process produces a sequence of observations. For example, the first nine outcomes might be: WLWWWLWLW. In this sequence, six outcomes are water and three are land. We call this sequence the data, i.e. \\(d\\). What we are trying to estimate here is the true proportion of water.\nSo what about our prior beliefs about the true proportion of water, i.e. \\(P(h)\\)? Let’s say that our prior belief (assuming complete ignorance about the true proportion of water) is that all proportions are equally probable. This is called a uniform prior, or a flat prior. You can see why in Figure 20.1. If you look at the top-right panel (the one with “n = 1”), the dashed line represents our prior belief: all proportions of water (on the x-axis) are equally probable, so that the prior probability distribution is flat. Note that a probability of 0 means Earth is all land, and a probability of 1 means Earth is all water. Now, let’s update the flat prior distribution with the first observation in the globe-tossing exercise: the first outcome was W, water. This observations corresponds to a distribution in which 1 has the greatest probability and values below it have decreasing probability. This is represented in the top-right panel of Figure 20.1 as the solid slanted line. This is \\(P(d | h)\\). If we combine the flat prior and the probability of the data we get the dashed line in the second top panel (with “n = 2”). That is the posterior probability distribution resulting from the Bayesian update at step “n = 1”. This becomes the prior probability distribution at step “n = 2”.\n\n\nCode\n# Code adapted from: https://bookdown.org/content/4857/small-worlds-and-large-worlds.html#bayesian-updating.\n\nsequence_length &lt;- 50\n\nd &lt;- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |&gt; \n  mutate(n_trials  = 1:9,\n         n_success = cumsum(toss == \"w\"))\n\nd |&gt; \n  expand_grid(p_water = seq(from = 0, to = 1, length.out = sequence_length)) |&gt; \n  group_by(p_water) |&gt; \n  # to learn more about lagging, go to:\n  # https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lag\n  # https://dplyr.tidyverse.org/reference/lead-lag.html\n  mutate(lagged_n_trials  = lag(n_trials),\n         lagged_n_success = lag(n_success)) |&gt; \n  ungroup() |&gt; \n  mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) |&gt; \n  # the next three lines allow us to normalize the prior and the likelihood, \n  # putting them both in a probability metric \n  group_by(n_trials) |&gt; \n  mutate(prior      = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) |&gt;   \n  \n  # plot!\n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior), \n            linetype = 2) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ strip, scales = \"free_y\")\n\n\n\n\n\n\n\n\nFigure 20.1: Bayesian updating of a prior based on observations. From McElreath (2020) and Kurtz (2023).\n\n\n\n\n\nNow let’s update our latest prior with the probability taken from the second observation: this was L, land. The solid line in the second panel is the probability of getting a W and L: the probability indicates that a proportion of 0.5 is the most probable. This makes sense: if in two tosses you got one W and one L, then the most probable hypothesis is that there is 50% of water and 50% of land. We combine again our prior (dashed line) with the data (solid line) to obtain the dashed line in the third top panel. This is our new prior. The rest of the figure shows how the prior gets updated at each new observation of W or L. You see that the highest density of the dashed lines quickly moves to the right. They are now suggesting that the globe has a higher proportion of water than land. Chapter 2 of McElreath (2020) goes into much more details and I recommend you read that at some point if you feel this section felt a bit too abstract.\nHopefully you can appreciate how different the frequentist and Bayesian approaches are: while frequentist statistics focusses on the rejection of a null/nil hypothesis based on the probability of the data given the hypothesis, or \\(P(d|h)\\), Bayesian statistics is about obtaining the probability of any hypothesis given the data, or \\(P(h|d)\\). You might also realise now that \\(P(d|h)\\) appears in Bayes’ Theorem. But the theorem also includes the prior, \\(P(h)\\). This is totally missing in the frequentist approach.\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhat is the main conceptual difference between the frequentist and Bayesian interpretations of probability?\n\n Frequentist probability is about the likelihood of a hypothesis being true, while Bayesian probability is about repeating an experiment infinitely. Frequentist probability is about the frequency of outcomes in repeated trials, while Bayesian probability is about the degree of certainty in an event given conditions. Frequentist probability uses prior beliefs, while Bayesian probability ignores them. Frequentist and Bayesian probability are mathematically identical and differ only in terminology.\n\nIn the ‘globe-tossing’ example, what does a uniform prior (or flat prior) mean?\n\n All proportions of water are considered equally probable before any data is observed. The Earth is assumed to be exactly 50% water and 50% land. The probability of land is always higher than the probability of water. The prior distribution automatically adjusts after the first observation.\n\nIn the ‘globe-tossing’ example, what does a uniform prior (or flat prior) mean?\n\n The probability of the data given the hypothesis, \\(P(d|h)\\). The updating of prior beliefs in light of new data. The concept of hypothesis testing with a null hypothesis. The use of repeated replications of a study.\n\n\n\n\n\n\n\n\nGelman, Andrew, Deborah Ann Nolan, and Deborah Ann Nolan. 2011. Teaching statistics: a bag of tricks. Repr. Oxford: Oxford Univ. Press.\n\n\nKurtz, Solomon. 2023. Statistical Rethinking with Brms, Ggplot2, and the Tidyverse: Second Edition. Version 0.4.0. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bayesian inference</span>"
    ]
  },
  {
    "objectID": "ch-gauss-model.html",
    "href": "ch-gauss-model.html",
    "title": "21  Gaussian models",
    "section": "",
    "text": "21.1 Gaussian models\nIn the context of a quantitative research study, a simple objective is to figure out the values of the parameters of the probability distribution of the variable of interest: Voice Onset Time, number of telic verbs, informativity score, acceptability ratings, reaction times, and so on. Let’s imagine we are interested in understanding more about the nature of reaction times in auditory lexical decision tasks (lexical decision tasks in which the target is presented aurally rather than in writing). We can revisit the RT data from Tucker et al. (2019) to try and address the following research question:\nNow, you might wonder why the mean and the standard deviation? This is because we are assuming that reaction times (i.e the population of reaction times, rather than our specific sample) are distributed according to a Gaussian probability distribution. It is usually the onus of the researcher to assume a probability distribution family. You will learn some heuristics for picking a distribution family later depending on the general type of the variable of interest, but for now the Gaussian family will be a safe assumption to make. In statistical notation, we can write:\n\\[\n\\text{RT} \\sim Gaussian(\\mu, \\sigma)\n\\]\nwhich you can read as: “reaction times are distributed according to a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)”. So the research question above is about finding the values of \\(\\mu\\) and \\(\\sigma\\).\nFor illustration’s sake, let’s assume the sample mean and standard deviation are also the population \\(\\mu\\) and \\(\\sigma\\): \\(Gaussian(\\mu = 1010, \\sigma = 318)\\) (we calculated these in Chapter 11). Figure 21.1 shows the empirical probability distribution (in grey, this is a density curve calculated with kernel density estimation) and the theoretical probability distribution (in purple) based on the sample mean and SD: in other words, the purple curve is the density curve of the theoretical probability distribution \\(Gaussian(1010, 318)\\). We know by now that any sample mean and SD is biased, due to uncertainty and variability. What we are really after is the values of \\(\\mu\\) and \\(\\sigma\\) which are the mean and standard deviation of the Gaussian distribution of the population of RTs in auditory lexical decision tasks. In other words, we want to make inference from the sample to the population of RTs.\nA statistical tool we can use to obtain an estimate of \\(\\mu\\) and \\(\\sigma\\) is a Gaussian model. A Gaussian model is a statistical model that estimates the values of the parameters of a (theoretical) Gaussian distribution, i.e. \\(\\mu\\) and \\(\\sigma\\). We can provisionally describe the model using formulae, like this:\n\\[\n\\begin{align}\n\\text{RT} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = ...\\\\\n\\sigma & = ...\\\\\n\\end{align}\n\\]\nNow, here is where things get interesting. Bayesian approaches to statistics assume uncertainty in the parameters of the distribution one is estimating. So not only the observed values of RT are uncertain because they come for a probability distribution, but the parameters of the distribution are themselves uncertain. You can think of the mean and SD are uncertain variables that need to be estimated from the data. When we say a varible is uncertain, we describe it using a probability distribution. So the aim of a Gaussian model is to estimate the probability distributions of the parameters from the data (and the priors), rather than just their values.\n\\[\n\\begin{align}\n\\text{RT} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & \\sim P(\\mu_1, \\sigma_1)\\\\\n\\sigma & \\sim P_+(\\mu_2, \\sigma_2)\\\\\n\\end{align}\n\\]\nWe say that \\(\\mu\\) comes from a probability distribution \\(P(\\mu_1, \\sigma_1)\\). We use a subscript \\(1\\) to differentiate the mean and SD of the main \\(Gaussian(\\mu, \\sigma)\\) distribution from the mean and SD of the probability distribution of the mean \\(\\mu\\). Similarly, we say that \\(\\sigma\\) comes from a probability distribution \\(P_+(\\mu_2, \\sigma_2)\\): \\(P_+()\\) is a (non-technical) way to indicate that the probability should include only positive values. Why? Because SDs can only be positive. By specifying \\(P_+()\\) we are constraining the probability distribution of \\(\\sigma\\) to have positive values only. In sum, we need to estimate two probability distributions, \\(P(\\mu_1, \\sigma_1)\\) and \\(P_+(\\mu_2, \\sigma_2)\\). These are posterior probability distributions. You will learn more about posterior probability distributions in the next chapter. For now, just keep in mind that they are called posterior because the come from the combination of priors and data.\nIn the rest of this book, you will be using the default prior probability distributions, or priors for short, as set by brms, the R package we will use to fit Bayesian models. This means that you will not have to worry about priors while you step your toes into the ocean of Bayesian statistics. However, it is helpful to learn a bit of context in relation to priors. After all, one of the big differences between frequentist and Bayesian statistics are indeed the priors (\\(P(h)\\) in Bayes’ Theorem in Chapter 21). After learning about priors in this chapter, you can safely assume that priors are handled by brms for you and you should not worry until after you completed this course. Note that in actual research, thinking about priors is a necessary step, even if one ends up using the default brms priors. Check out the Spotlight box to learn a bit more about priors.\nIn the next chapter you will fit the Gaussian model of RTs using brms, with the default priors as set by the package.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-gauss-model.html#gaussian-models",
    "href": "ch-gauss-model.html#gaussian-models",
    "title": "21  Gaussian models",
    "section": "",
    "text": "NoteQuiz 1\n\n\n\n\nWhy is the distribution of \\(\\sigma\\) constrained to positive values only?\n\n Because \\(\\sigma\\) is always zero. Because \\(\\sigma\\) represents variability, which cannot be negative. Because \\(\\mu\\) is also constrained to be positive.\n\nWhy have we assumed that reaction times are Gaussian?\n\n Because RTs are Gaussian. Because it is easier to run the model. Because a Gaussian distribution is a safe assumption to make.\n\nIn Bayesian modeling, posterior probability distributions are described as:\n\n The data alone. Theoretical distributions chosen by the researcher before collecting data. The combination of priors and observed data.\n\n\n\n\n\n\n\n\n\n\nImportantPrior probability distributions\n\n\n\n\n\nHow do we go about choosing priors for the model above? Once you know that you are trying to estimate the (posterior) probability distribution of \\(\\mu\\) and \\(\\sigma\\) you also know that you should choose a prior for each parameter. In other words, each parameter in the model gets its own prior. But what is a prior exactly? It is just a probability distribution! With Gaussian models, it is common to use Gaussian probability distributions as the priors for the mean and SD of the Gaussian distribution. Yes, you read right: we use Gaussian distributions as priors for the parameters of the Gaussian distribution. Note that priors should be chosen before seeing the data. Here, we have seen the data many times, so let’s just pretend we haven’t. For example, let’s say that we believe that, prior to seeing the data, the mean RT is a value from a Gaussian distribution with mean 900 ms and SD 200 ms: \\(Gaussian(900, 200)\\). Do not worry as to how I came up with those numbers. Since you will not need to choose priors yourself, for now just focus on understanding how priors fit in Gaussian models. The \\(Gaussian(900, 200)\\) distribution is shown in Figure 21.2.\n\n\nCode\nxseq &lt;- seq(0, 2000)\n\nggplot() +\n  aes(x = xseq, y = dnorm(xseq, 900, 200)) +\n  geom_path(colour = \"darkgreen\", linewidth = 1) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nFigure 21.2: The prior for \\(\\mu\\).\n\n\n\n\n\nThe prior distribution we have chosen for \\(\\mu\\) says that values around 900 ms are more probable than values away from 900. We can now pick a prior for \\(\\sigma\\), the overall standard deviation. Let’s say the SD can be described by a half-Gaussian prior probability with mean 0 and SD 200. Why half? Because as we said earlier, SDs can only be positive and with a Gaussian distribution with mean 0 we can just take the positive half to constrain the distribution to positive values. It is also common for priors on standard deviations to set the mean to 0 (to understand the reason, you will have to learn more about priors, so we won’t delve into this). Our half-Gaussian prior distribution is shown in Figure 21.3.\n\n\nCode\nxseq &lt;- seq(0, 750)\n\nggplot() +\n  aes(x = xseq, y = dnorm(xseq, 0, 200)) +\n  geom_path(colour = \"darkorange\", linewidth = 1) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nFigure 21.3: The prior for \\(\\sigma\\).\n\n\n\n\n\nThe prior for \\(\\sigma\\) indicates that we expect values closer to zero to be more probable that larger values. We can rewrite the model formulae above as:\n\\[\n\\begin{align}\n\\text{RT}  & \\sim \\text{Gaussian}(\\mu, \\sigma) \\\\\n\\mu        & \\sim \\text{Gaussian}(900, 200)   & \\text{[Prior for $\\mu$]} \\\\\n\\sigma     & \\sim \\text{HalfGaussian}(0, 200) & \\text{[Prior for $\\sigma$]}\n\\end{align}\n\\]\n\n\n\n\n\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html",
    "href": "ch-fit-model.html",
    "title": "22  Fitting Gaussian models with brms",
    "section": "",
    "text": "22.1 Posterior probability distributions\nIn the previous chapter, I have introduced the theory behind Bayesian Gaussian models. In this chapter, you will learn how to fit Gaussian models in R. You can fit a Gaussian model to data in R using the brms package (the name is an initialism of “Bayesian Regression Models using Stan”; Gaussian models are a special type of regression models, which will be introduced in Chapter 23).\nThe brms package can run a variety of Bayesian (regression) models. It is a very flexible package that allows you to model a lot of different types of variables. You don’t really need to understand all of the technical details to be able to effectively use the package and interpret the results, so this textbook will focus on how to use the package in the context of research. We will cover some of the technicalities, but if you are are particularly interested in the inner workings of the package, feel free to find materials on specific aspects by searching online. One useful thing to know is that brms is a bridge between R and the statistical programming software Stan. Stan is a powerful piece of software that can run any type of Bayesian model, not just regressions. What brms does is that it allows you to write Bayesian models in R, which are translated into Stan models and run with Stan under the hood. You can safely use brms without learning Stan, but if you are interested, you can check Ch 8-10 of Nicenboim, Schad, and Vasishth (2025) and the Stan documentation.\nYou can run a Bayesian Gaussian model with the brm() function, short for “Bayesian Regression Model” (a Gaussian model is a special type of regression model). The mandatory arguments of brm() are a model formula, a distribution family (of the outcome variable), and the data you want to run the model with. Running a model with data is also formally known as fitting the model to the data. We want to fit a Gaussian model to reaction times from Tucker et al. (2019). Let’s revisit the mathematical formula of the model from Chapter 21 (let’s discard the priors; see the R Note box below):\n\\[\nRT \\sim Gaussian(\\mu, \\sigma)\n\\]\nIt would be nice if brms() allowed you to write the formula like that.\nAlas, due to historical and technical reasons of how other R packages write model formulae, you need to use a special way of specifying the model. As mentioned, you need three arguments: a model formula, the distribution family of the outcome, and the data. So the mathematical formula is split in two parts (corresponding to two arguments of the brm() function): formula and family.\nAs with other R functions, we want to assign the output of brm() to a variable, here rt_bm. We will then be able to inspect the output in rt_bm. Now run the Gaussian model of RTs (don’t forget to attach the brms package and read the data, like in the following code).\nWhen you run the code, some text will be printed below the code in your Quarto document. This is what it looks like.\nThe messages in the text are related to Stan and the statistical algorithm used by Stan to estimate the parameters of the model (in this model, these are the mean and the standard deviation of RTs). Compiling Stan program... tells you that brms has instructed Stan to compile the model specified in R and that Stan is now compiling the model to be run on the data (don’t worry if this does not make sense). Start sampling tells us that the statistical algorithm used for estimation has started. This algorithm is the Markov Chain Monte Carlo algorithm, or MCMC for short. The algorithm is run by default four times; in technical terms, four MCMC chains are run. This is why information on Chain 1, 2, 3, and 4 is printed. We will treat MCMC in more details in Chapter 25.\nNow that the model has finished running and that the output has been saved in rt_bm, we can inspect it with the summary() function (note that this is different from summarise()!).\nLet’s break down the summary bit by bit:\nThe main characteristic of Bayesian models is that they don’t just provide you with a single numeric estimate for the model parameters. As mentioned in Chapter 21, the model estimates a full probability distribution for each parameter/coefficient. These probability distributions are called posterior probability distributions (or posteriors for short). They are called posterior because they are derived from the data and the prior probability distributions. The model we fitted has two parameters: the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). For reasons that will become clear in Chapter 23, the \\(\\mu\\) parameter is called Intercept in the summary: you can find it in the Regression Coefficients table. The standard deviation \\(\\sigma\\) is in the Further distributional parameters table.\nThe Regression Coefficients table reports, for each estimated coefficient, a few summary measures of the posterior distributions of the estimated coefficients. Here, we only have the summary measures of one posterior: the posterior of the model’s mean \\(\\mu\\). The table also has three diagnostic measures, which you can ignore for now.\nHere’s a breakdown of the table’s columns:\nThe Further distributional parameters table has the same structure. In this model, the posterior mean of \\(\\sigma\\) is 317.88 ms and the posterior SD of \\(\\sigma\\) is 3.17 ms.\nPutting all this together in mathematical notation:\n\\[\n\\begin{align}\nRT & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & \\sim P(1010.5, 4.45)\\\\\n\\sigma & \\sim P(317.88, 3.17)\n\\end{align}\n\\]\nIn other words, according to the model and data, the mean or RTs is a value from the distribution \\(P(1010.5, 4.45)\\) and the SD of RTs is a value from the distribution \\(P(317.88, 3.17)\\). Here, \\(P()\\) stands for a generic posterior probability distribution. The model has quantified the uncertainty around the value of the \\(\\mu\\) and \\(\\sigma\\) parameters and this uncertainty is reflected by the fact that we get a full posterior probability distribution (summarised by its mean and SD) for each of the parameters. We don’t know exactly the values of the parameters of the Gaussian distribution assumed to have generated the sampled RTs.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Fitting Gaussian models with brms</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#posterior-probability-distributions",
    "href": "ch-fit-model.html#posterior-probability-distributions",
    "title": "22  Fitting Gaussian models with brms",
    "section": "",
    "text": "TipPosterior probability distribution\n\n\n\nA posterior probability distribution is a probability distribution of a model parameter estimated by a Bayesian model from the prior probability distribution and the data.\n\n\n\n\n\nEstimate, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient). In our model, this is the mean of the posterior distribution of the mean \\(\\mu\\) (Intercept). Yes, you read correctly: the mean of the mean! Remember, Bayesian models estimate a full posterior probability distribution and the posterior can be summarised by a mean and a standard deviation. In this model, the posterior mean (short for mean of the posterior probability distribution) of \\(\\mu\\) is 1010.5 ms.\nEst.error, the error of the mean estimate, or estimate error. The estimate error is the standard deviation of the posterior distribution of the coefficient (here the mean \\(\\mu\\)). Yes, the standard deviation of the mean! Again, since we are estimating a full probability distribution for \\(\\mu\\), we can summarise it with a mean and SD as we do for any Gaussian distribution. In this model, the posterior SD of \\(\\mu\\) is 4.45 ms. Be careful: this SD is not \\(\\sigma\\). It is the standard deviation of the posterior distribution of the mean \\(\\mu\\).\nl-95% CI and u-95% CI, the lower and upper limits of the 95% Bayesian Credible Interval (more on these below).\nFinally, Rhat, Bulk_ESS, Tail_ESS are diagnostics of the MCMC chains, which you can ignore for now.\n\n\n\n\n\n\n\n\n\n\n\nImportantR Note: Default priors in brms\n\n\n\n\n\nOne major aspect of Bayesian modelling is the combination of prior knowledge with evidence from the observed data. As introduced in Chapter 21, choosing priors is an important step in conducting Bayesian analyses. However, in this textbook we will not deal with prior specification given the quite tight schedule of the course.\nIf prior specification is a necessary step, how come we are not doing that? When fitting Bayesian models with brms, you are in fact using brms default priors. These are generic priors that work in most circumstances and they are equivalent to prior probability distributions that are almost flat (like in the first prior of Figure 20.1). In other words, they are so generic that they have very little influence on the posterior distribution, but still they help with the computational aspects of model fitting (which you will learn more about in Chapter 25).\nIf you want to inspect brms default priors, you can use the get_prior() function. Let’s do this for the model we fitted above. The function requires the model formula, the family and the data, much like the brm() function. It returns a data frame with one prior per row. The columns of interest are prior and class.\n\nget_prior(\n  RT ~ 1,\n  family = gaussian,\n  data = mald\n)\n\n\n  \n\n\n\nThere are two priors, one for the intercept (\\(\\mu\\)) and one for \\(\\sigma\\) (see class column). For both \\(\\mu\\) and \\(\\sigma\\), brms sets a Student-t prior probability distribution. The Student-t distribution is a continuous probability distribution with three parameters: the degrees of freedom, the mean and the standard deviation. You will encounter the Student-t distribution in Chapter 29, where you will learn about frequentist p-values. For now, it will suffice to say that a Student-t distribution (also simply called a t-distribution) is similar to a Gaussian distribution (hence the two parameters, mean and SD).\nNormally, you would choose priors before collecting/seeing the data. Here, brms actually uses the data to come up with generic priors that cover a wide range of values without including very unlikely values. Yet, the priors set by brms are so generic that, as said above, they bear very little effect on the posterior. So they are safe to use, even if they are based on the data itself. Just remember, thought, that if you do want to specify your own priors, you must do so independent of the data (ideally, even before you have access to the data, whether you collect it yourself or whether it is pre-existing).\nFor this model and data, brms sets a t-distribution for the prior of the mean \\(\\mu\\), with mean 935.5 and SD 220.2, and a t-distribution for the SD \\(\\sigma\\), with mean 0 and SD 220.2. You will notice that the SDs of both priors are the same. The following figures illustrate the density curve of the priors.\nx_mu &lt;- seq(0, 2000)\n\nggplot() +\n  aes(x = x_mu, y = extraDistr::dlst(x_mu, 3, 935.5, 220.2)) +\n  geom_path(linewidth = 1) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n\nx_sigma &lt;- seq(0, 1500)\n\nggplot() +\n  aes(x = x_sigma, y = extraDistr::dlst(x_sigma, 3, 0, 220.2)) +\n  geom_path(linewidth = 1) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Prior probability distribution for the mean.\n\n\n\n\n\n\n\n\n\n\n\n(b) Prior probability distribution for the SD.\n\n\n\n\n\n\n\nFigure 22.1: Default brms priors for this chapter’s model and data.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Fitting Gaussian models with brms</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#plotting-the-posterior-distributions",
    "href": "ch-fit-model.html#plotting-the-posterior-distributions",
    "title": "22  Fitting Gaussian models with brms",
    "section": "22.2 Plotting the posterior distributions",
    "text": "22.2 Plotting the posterior distributions\nWhile the model summary reports summaries of the posterior distributions, it is always helpful to plot the posteriors. We can easily do so with the base R plot() function, like in Figure 22.2. The density plots of the posteriors distributions of the two parameters estimated in the model are shown on the left of the figure: b_Intercept which corresponds to Intercept from the summary and sigma (the reason for why it’s b_Intercept will become clear in Chapter 23).\n\nplot(rt_bm, combo = c(\"dens\", \"trace\"))\n\n\n\n\n\n\n\nFigure 22.2: Posterior plots of the rt_bm Gaussian model\n\n\n\n\n\nFor the b_Intercept coefficient, i.e. the mean \\(\\mu\\), the posterior probability encompasses values between 1000 and 1020 ms, approximately. But some values are more probable than others: the values in the centre of the distribution have a higher probability density then the values on the sides. The mean of that posterior probability distribution is the Estimate value in the model summary: 1010.5 ms. Its standard deviation is the Est.error: 4.45 ms. The mean indicates the value with the highest probability density, which corresponds to the value on the horizontal axis of the density plot below the highest peak of the density curve. Based on these properties, values around 1010.5 ms are more probable than values further away from it.\nFor sigma, i.e. \\(\\sigma\\), the posterior probability covers values between 310 and 325. What is the mean of the posterior probability of \\(\\sigma\\)? The answer is in the summary, in Further distributional parameters. There you will also find the standard deviation of the posterior of \\(\\sigma\\). That’s a standard deviation of a standard deviation! As before, this is because we are not estimating a simple value for \\(\\sigma\\) but a full (posterior) probability distribution and we can summarise this distribution with a mean and a standard deviation. Again, the highest peak in the distribution corresponds to the Estimate value.\nNow, looking at a full probability distribution like that is not very straightforward and summary measures can be even less straightforward. Credible Intervals (CrIs) help summarise the posterior distributions so that interpretation is more straightforward.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Fitting Gaussian models with brms</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#interpreting-credible-intervals",
    "href": "ch-fit-model.html#interpreting-credible-intervals",
    "title": "22  Fitting Gaussian models with brms",
    "section": "22.3 Interpreting Credible Intervals",
    "text": "22.3 Interpreting Credible Intervals\nThe model summary reports the Bayesian Credible Intervals (CrIs) of the posterior distributions. Another way of returning summaries of the coefficients is to use the posterior_summary() function which returns a table (technically, a matrix, another type of R objects; we print only the first two rows with [1:2,] because we can ignore the other ones).\n\nposterior_summary(rt_bm)[1:2,]\n\n             Estimate Est.Error      Q2.5     Q97.5\nb_Intercept 1010.5015  4.452312 1001.6555 1019.2559\nsigma        317.8845  3.171714  311.7382  324.1688\n\n\nA Bayesian CrI is simply the central probability interval of a posterior probability distribution. You have encountered intervals in Chapter 19. By default, brms prints the 95% CrIs: these are the 95% central probability intervals of the posterior probability of each coefficient. The 95% CrI of the b_Intercept is between 1002 and 1019. This means that there is a 95% probability, or (equivalently) that we can be 95% confident, that the Intercept, i.e. \\(\\mu\\), is within that range, given the model and data. For sigma, i.e. \\(\\sigma\\), the 95% CrI is between 312 and 324. So, again, there is a 95% probability that the sigma value is between those values, given the model and data. So, to summarise, a 95% CrI tells us that we can be 95% confident, or in other words that there is a 95% probability, that the value of the coefficient is between the values of the CrI.\nThere is nothing special about 95% CrI and in fact it is recommended to calculate and report a few of them. Personally, I use 90, 80, 70 and 60% CrIs. You can get any CrI with the summary() and the posterior_summary() functions, but you will also learn an alternative and more succinct way in Chapter 24. Here is how to get an 80% CrI with summary().\n\nsummary(rt_bm, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 1 \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept  1010.50      4.45  1004.74  1016.14 1.00     3628     2476\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma   317.88      3.17   313.84   321.89 1.00     4064     2571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor posterior_summary(), you specify the percentiles rather than the probability level.\n\nposterior_summary(rt_bm, probs = c(0.1, 0.9))[1:2,]\n\n             Estimate Est.Error       Q10       Q90\nb_Intercept 1010.5015  4.452312 1004.7400 1016.1351\nsigma        317.8845  3.171714  313.8399  321.8941\n\n\nHere you have the results from the regression model. Really, the results of the model are the full posterior probabilities, but it makes things easier to focus on the CrIs. If you are used to frequentist statistics (either from learning how to run them or from reading frequentist analyses in academic papers) this might seem a bit underwhelming. But this is the core thinking of Bayesian statistics: inference is based on full probability distributions for the model’s parameters, rather than on a single value (like a p-value).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Fitting Gaussian models with brms</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#reporting",
    "href": "ch-fit-model.html#reporting",
    "title": "22  Fitting Gaussian models with brms",
    "section": "22.4 Reporting",
    "text": "22.4 Reporting\n\nWhat about reporting the model in writing? We could report the model and the results like this (for simplicity, we report only the 95% CrIs here. You will learn how to report multiple CrIs in tables later).1\n\nWe fitted a Bayesian Gaussian model of reaction times (RTs) using the brms package (Bürkner 2017) in R (R Core Team 2024). The model estimates the mean and standard deviation of RTs.\nBased on the model results, there is a 95% probability that the mean is between 1002 and 1019 ms (mean = 1011, SD = 4) and that the standard deviation is between 312 and 324 ms (mean = 318, SD = 3).\n\nThe example used in this chapter is quite trivial: we are just estimating a mean and SD from the data, but in order to understand how to answer more involved questions it is fundamental that you understand what was covered in this chapter. So make sure to have grasped the basics of Gaussian models before moving onto regressions in Chapter 23. On the other hand, if you will ever be asked what we can expect RTs in a auditory lexical decision task to look like you might be able to say that, based on the model and data in this chapter, we can be quite confident that they will have a mean of about 1010 ms and an SD of about 320 ms. You might think this was a lot of work to just learn about what we knew directly from the sample, and it is, but be reassured that in normal research contexts things are always much more complex than this, so it is indeed worth the effort.\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhich function is used to fit Bayesian models?\n\n brms() brm() bm()\n\nThe Estimate column in further distributional parameters is:\n\n The mean of the posterior distributino of the mean. The standard deviation of the posterior distribution of the mean. The mean of the posterior distribution of the standard deviation.\n\nThe posterior of each parameter is the value in the Estimate column. TRUEFALSE\nOnly 95% CrI should be used for inference. TRUEFALSE\nTo fit a Gaussian model, the formula has the form y ~ 1. TRUEFALSE\nThe intercept corresponds to the parameter \\(\\mu\\). TRUEFALSE\n\n\n\n\n\n\n\nNicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025. Introduction to Bayesian Data Analysis for Cognitive Science. https://bruno.nicenboim.me/bayescogsci/.\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Fitting Gaussian models with brms</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#footnotes",
    "href": "ch-fit-model.html#footnotes",
    "title": "22  Fitting Gaussian models with brms",
    "section": "",
    "text": "To know how to add a citation for any R package, simply run citation(\"package\") in the R Console, where \"package\" is the package name between double quotes.↩︎",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Fitting Gaussian models with brms</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html",
    "href": "ch-regression-intro.html",
    "title": "23  Introduction to regression",
    "section": "",
    "text": "23.1 A straight line\nIn the previous chapters, you have learned the basics of probability and how to run Gaussian models to estimate the mean and standard deviation (\\(\\mu\\) and \\(\\sigma\\)) of a variable. This chapter extends the Gaussian model to what is commonly called a Gaussian regression model (or simply regression model). Regression models (including the Gaussian) are models based on the equation of a straight line. This is why regression models are also called linear models. Regression models allow you to model the relationship between two or more variables. This textbook introduces you to regression models of increasing complexity which can model variables frequently encountered in linguistics. Note that regression models are very powerful and flexible statistical models which can deal with a great variety of types of variables. Appendix A has a regression cheat sheet which you will be able to consult, after completing this course, as a guide for building a regression model based on a set of questions. For now, let’s dive into the basics of a regression model.\nA regression model is a statistical model that estimates the relationship between an outcome variable and one or more predictor variables (more on outcome/predictor below). Regression models are based on the equation of a straight line.\n\\[\ny = mx + c\n\\]\nAn alternative notation of the equation is:\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\n\\(\\beta\\) is the Greek letter beta [ˈbiːtə]: you can read \\(\\beta_0\\) as “beta zero” and \\(\\beta_1\\) “beta one”. In this formula, \\(\\beta_0\\) corresponds to \\(c\\) and \\(\\beta_1\\) to \\(m\\) in the first notation. We will use the second notation (with \\(\\beta_0\\) and \\(\\beta_1\\)) in this book, since using \\(\\beta\\)’s with subscript indexes will help understand the process of extracting information from regression models later.1",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html#back-to-school",
    "href": "ch-regression-intro.html#back-to-school",
    "title": "23  Introduction to regression",
    "section": "23.2 Back to school",
    "text": "23.2 Back to school\nYou might remember from school when you were asked to find the values of \\(y\\) given certain values of \\(x\\) and specific values of \\(\\beta_0\\) and \\(\\beta_1\\). For example, you were given the following formula (the dot \\(\\cdot\\) stands for multiplication; it can be dropped so \\(2 \\cdot x\\) and \\(2x\\) are equivalent):\n\\[\ny = 3 + 2 \\cdot x\n\\]\nand the values \\(x = (2, 4, 5, 8, 10, 23, 36)\\). The homework was to calculate the values of \\(y\\) and maybe plot them on a Cartesian coordinate space.\n\n\nCode\nlibrary(tidyverse)\n\nline &lt;- tibble(\n  x = c(2, 4, 5, 8, 10, 23, 36),\n  y = 3 + 2 * x\n)\n\nggplot(line, aes(x, y)) +\n  geom_point(size = 4) +\n  geom_line(colour = \"red\") +\n  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))\n\n\n\n\n\n\n\n\n\nUsing the provided formula, we are able to find the values of \\(y\\). Note that in \\(y = 3 + 2 * x\\), \\(\\beta_0 = 3\\) and \\(\\beta_1 = 2\\). Importantly, \\(\\beta_0\\) is the value of \\(y\\) when \\(x = 0\\). \\(\\beta_0\\) is commonly called the intercept of the line. The intercept is the value where the line crosses the y-axis (the value where the line “intercepts” the y-axis).\n\\[\n\\begin{align}\ny & = 3 + 2 x\\\\\n& = 3 + 2 \\cdot 0\\\\\n& = 3\\\\\n\\end{align}\n\\]\nAnd \\(\\beta_1\\) is the number to add to the intercept for each unit increase of \\(x\\). \\(\\beta_1\\) is commonly called the slope of the line.2 Figure 23.1 should clarify this. The dashed line indicates the increase in \\(y\\) for every unit increase of \\(x\\) (i.e., every time \\(x\\) increases by 1, \\(y\\) increases by 2).\n\n\nCode\nline &lt;- tibble(\n  x = 0:3,\n  y = 3 + 2 * x\n)\n\nggplot(line, aes(x, y)) +\n  geom_point(size = 4) +\n  geom_line(colour = \"red\") +\n  annotate(\"path\", x = c(0, 0, 1), y = c(3, 5, 5), linetype = \"dashed\") +\n  annotate(\"path\", x = c(1, 1, 2), y = c(5, 7, 7), linetype = \"dashed\") +\n  annotate(\"path\", x = c(2, 2, 3), y = c(7, 9, 9), linetype = \"dashed\") +\n  annotate(\"text\", x = 0.25, y = 4.25, label = \"+2\") +\n  annotate(\"text\", x = 1.25, y = 6.25, label = \"+2\") +\n  annotate(\"text\", x = 2.25, y = 8.25, label = \"+2\") +\n  scale_y_continuous(breaks = 0:15) +\n  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))\n\n\n\n\n\n\n\n\nFigure 23.1: Illustration of the meaning of the slope: with a slope of 2, for each unit increase of \\(x\\), \\(y\\) increases by 2.\n\n\n\n\n\n\n\n\n\n\n\nTipEquation of a line\n\n\n\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\nwhere \\(\\beta_0\\) is the line intercept and \\(\\beta_1\\) is the line slope.\n\n\nOf course, we can plug in any value of \\(x\\) in the formula to obtain \\(y\\). The following equations show \\(y\\) when \\(x\\) is 1, 2, and 3. You see that when you go from \\(x = 1\\) to \\(x = 2\\) we go from \\(y = 5\\) to \\(y = 7\\): \\(7 - 5 =2\\), our slope \\(\\beta_1\\).\n\\[\n\\begin{align}\ny & = 3 + 2 \\cdot 1\\\\\n& = 3 + 2\\\\\n& = 5\\\\\ny & = 3 + 2 \\cdot 2\\\\\n& = 3 + 4\\\\\n& = 7\\\\\ny & = 3 + 2 \\cdot 3\\\\\n& = 3 + 6\\\\\n& = 9\\\\\n\\end{align}\n\\]\nNow, in the context of research, you usually start with a sample of measures (values) of \\(x\\) (the predictor variable) and \\(y\\) (the outcome variable), rather than having to calculate \\(y\\). Then you have to estimate (i.e. to find the values of) \\(\\beta_0\\) and \\(\\beta_1\\) of the model formula \\(y = \\beta_0 + \\beta_1 x\\) . This is what regression models are for: given the sampled values of \\(y\\) and \\(x\\), the model estimates \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\n\n\n\nWarningExercise\n\n\n\n\nGo to the web app Linear Models Illustrated.\nIn the first tab, “Continuous”, you will find instructions on the left and a plot on the right. The plot on the right is the plot resulting from the parameters specified to the left.\nPlay around with the intercept and slope parameters to see what happens to the line with different values of the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\).\n\n\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nUse the Linear Models Illustrated app to answer the following questions.\n\nWhat happens to the line when you increase the intercept \\(\\beta_0\\)?\n\n The whole line shifts downwards. The line becomes steeper. The whole line shifts upwards. The line becomes flat.\n\nWhat happens to the line when you set the slope \\(\\beta_1\\) to a negative number?\n\n The line decreases from left to right. The whole line becomes flat. The whole line shifts downwards. The line increases from left to right.\n\nWhat happens to the line when you set the slope \\(\\beta_1\\) to 0 zero?\n\n The whole line shifts downwards. The line decreases from left to right. The line becomes steeper. The line becomes flat.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html#add-error",
    "href": "ch-regression-intro.html#add-error",
    "title": "23  Introduction to regression",
    "section": "23.3 Add error",
    "text": "23.3 Add error\nMeasurements are noisy: they usually contain errors. Error can have many different causes (for example, measurement error due to technical limitations or variability in human behaviour), but we are usually not that interested in learning about what causes the error. Rather, we just want our model to be able to deal with error. Let’s see what errors looks like. Figure 23.2 shows values of \\(y\\) simulated with the equation \\(y = 1 + 1.5x\\) (with \\(x\\) equal 1 to 10), to which the random error \\(\\epsilon\\) (the Greek letter epsilon [ˈɛpsɪlɒn]) was added. Due to the added error, the points are almost on the straight line defined by \\(y = 1 + 1.5x\\), but not quite. The vertical distance between the observed points and the expected line, called the regression line, is the residual error (red lines in the plot).\n\n\nCode\nset.seed(4321)\nx &lt;- 1:10\ny &lt;- (1 + 1.5 * x) + rnorm(10, 0, 2)\n\nline &lt;- tibble(\n  x = x,\n  y = y\n)\n\nm &lt;- lm(y ~ x)\nyhat &lt;- m$fitted.values\ndiff &lt;- y - yhat\nggplot(line, aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = y, yend = yhat), colour = \"red\") +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(title = bquote(italic(y) == 1 + 1.5 * italic(x) + epsilon))\n\n\n\n\n\n\n\n\nFigure 23.2: Illustration of residual error.\n\n\n\n\n\nWhen taking into account error, the equation of a regression model becomes the following:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is the error. In other words, \\(y\\) is the sum of \\(\\beta_0\\), \\(\\beta_1 x\\) and some error. In regression modelling, the error \\(\\epsilon\\) is assumed to come from a Gaussian distribution with mean 0 and standard deviation \\(\\sigma\\) when \\(y\\) is assumed to be generated by a Gaussian distribution: \\(\\epsilon \\sim Gaussian(\\mu = 0, \\sigma)\\). We can substitute \\(\\epsilon\\) for the distribution.\n\\[\ny = \\beta_0 + \\beta_1 x + Gaussian(0, \\sigma)\n\\]\nFurthermore, this equation can be rewritten like so (since the mean of the Gaussian error is 0):\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 x\\\\\n\\end{align}\n\\]\nYou can read those formulae like so: “The variable \\(y\\) is distributed according to a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The mean \\(\\mu\\) is equal to the intercept \\(\\beta_0\\) plus the slope \\(\\beta_1\\) times the variable \\(x\\).” This is a Gaussian regression model, because the assumed family of the outcome \\(y\\) is Gaussian. Now, the goal of a (Gaussian) regression model is to estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) from the data (i.e. from the values of \\(x\\) and \\(y\\)). In other words, regression models find the regression line based on the observations of \\(y\\) and \\(x\\). We do not know what is the true regression line that has generated \\(y\\) and \\(x\\), we just have \\(y\\) and \\(x\\) values.\nThe \\(y \\sim Gaussian(\\mu, \\sigma)\\) line in the formulae above is exactly the formula you saw in Chapter 21. It is no coincidence: this is Gaussian regression model. The outcome variable \\(y\\) is assumed to be Gaussian. The new building block we have added now is that \\(\\mu\\) depends on \\(x\\): this is because \\(x\\) appears in the formula of \\(\\mu\\). In other words, we are allowing the mean \\(\\mu\\) to vary with \\(x\\). This is expressed by the so-called regression equation (also linear equation): \\(\\mu = \\beta_0 + \\beta_1 x\\). This is the core concept of regression models.\n\n\n\n\n\n\nTipRegression model\n\n\n\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma) & \\text{[Distribution of y]}\\\\\n\\mu & = \\beta_0 + \\beta_1 x & \\text{[Regression equation]}\\\\\n\\end{align}\n\\]\n\n\nYou perhaps realised this by now, but the regression equation implies that both \\(y\\) and \\(x\\) are numeric variables. However, regression models can also be used with variables that are categorical. You will learn how to use categorical predictors (categorical \\(x\\)’s) in regression models in Week 7’s chapters. Moreover, regression models are not limited to the Gaussian distribution family and in fact regression models can be fit with virtually any other distribution family. The chapters of Week 8 will teach you how to fit two other useful distribution families: the log-normal family and the Bernoulli family. You will be able to learn about other families by checking the resources linked in Appendix A.\n\n\n\n\n\n\nImportantSpotlight: Regression, eugenics and racism\n\n\n\n\n\nGalton and regression\nThe basic logic of regression models is attributed to Francis Galton (1822–1911). Galton studied the relationship between the heights of parents and their children (Galton 1980, 1886). He noticed that while tall parents tended to have tall children, the children’s heights were often closer to the average height of the population. This phenomenon, which he called “regression toward mediocrity” (now known as “regression to the mean”), showed that extreme values (e.g., very tall or very short parents) were less likely to be perfectly transmitted to the next generation.\n\n\n\n\n\nThe core idea of Galton’s framework can be expressed as a regression model:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot x + \\epsilon\n\\]\nwhere:\n\n\\(y\\): the child’s height (response variable),\n\\(x\\): the average of the parents’ heights (predictor variable),\n\\(\\beta_0\\): the intercept (the expected height of a child when the parents’ height is at the mean),\n\\(\\beta_1\\)​: the slope, representing the rate of change in the child’s height with respect to the parents’ height,\n\\(\\epsilon\\): the error term, accounting for random variability.\n\nGalton found that the slope \\(\\beta_1\\) was less than 1, meaning that the children’s heights were not as extreme as their parents’ heights. For example, if tall parents (above the mean) had an average child height increase of \\(\\beta_1 &lt; 1\\), it indicated a “regression” toward the population mean. The intercept \\(\\beta_0\\) ensured the line passed through the mean of both parents’ and children’s heights.\nGalton, eugenics and racism\nGalton is considered one of the founders of modern statistics and is widely recognized for his contributions to fields such as regression, correlation, and the study of heredity. However, his work is also deeply intertwined with controversial and now discredited views on race and eugenics. Galton coined the term eugenics in 1883, defining it as the “science of improving the genetic quality of the human population”. His goal was to encourage the reproduction of individuals he deemed “fit” and discourage that of those he considered “unfit”. He promoted selective breeding among humans, drawing inspiration from animal breeding practices.\nGalton believed in a hierarchy of intelligence and ability among “races”, a belief that was common among many European intellectuals of his time. In works like Hereditary Genius (1869), he argued that intelligence and other traits were hereditary and that Europeans were superior to other racial groups. These conclusions were based on flawed assumptions and biased interpretations of data. His ideas contributed to the spread of pseudo-scientific racism, which attempted to justify inequality and colonialism.\nGalton’s eugenic ideas were later used to justify discriminatory policies, including forced sterilization programs and racial segregation in various countries. While Galton himself did not directly advocate for many of the extreme measures implemented in the 20th century, his work laid the groundwork for such abuses. His promotion of eugenics and racial hierarchies has left a damaging legacy.\n\n\n\n\n\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246. https://doi.org/10.2307/2841583.\n\n\n———. 1980. “Kinship and Correlation.” The North American Review 150 (401): 419431.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html#footnotes",
    "href": "ch-regression-intro.html#footnotes",
    "title": "23  Introduction to regression",
    "section": "",
    "text": "Yet other notations are \\(y = a + bx\\) and \\(y = \\alpha + \\beta x\\).↩︎\nMathematically, it is called the gradient, but in regression modelling the word slope is commonly used.↩︎",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression.html",
    "href": "ch-regression.html",
    "title": "24  Regression models",
    "section": "",
    "text": "24.1 Vowel duration in Italian: the data\nIn Chapter 23 you were introduced to regression models. Regression is a statistical model based on the equation of a straight line, with added error.\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\n\\(\\beta_0\\) is the regression line’s intercept and \\(\\beta_1\\) is the slope of the line. We have seen that \\(\\epsilon\\) is assumed to be from a Gaussian distribution with mean 0 and standard deviation \\(\\sigma\\).\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 x\\\\\n\\end{align}\n\\]\nFrom now on, we will use the latter way of expressing regression models, because it makes it clear which distribution we assume the variable \\(y\\) to be generated by (here, a Gaussian distribution). Note that in the wild, variables very rarely are generated by Gaussian distributions. It is just pedagogically convenient to start with Gaussian regression models (i.e. regression models with a Gaussian distribution as the distribution of the outcome variable \\(y\\)) because the parameters of the Gaussian distribution, \\(\\mu\\) and \\(\\sigma\\) can be interpreted straightforwardly on the same scale as the outcome variable \\(y\\): so for example if \\(y\\) is in centimetres, then the mean and standard deviation are in centimetres, if \\(y\\) is in Hz, then the mean and SD are in Hz, and so on. Similarly, the regression \\(\\beta\\) coefficients will be on the same scale as the outcome variable \\(y\\). You will be introduced later to regression models with distributions other than the Gaussian, where the regression parameters are estimated on a different scale than that of the outcome variable \\(y\\).\nThe goal of the Gaussian regression model expressed in the formulae above is to estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) from observed data. Now, since truly Gaussian data is difficult to come by, especially in linguistics, for the sake of pedagogical simplicity we will start the learning journey on fitting regression models using vowel durations, i.e. data for which a Gaussian regression is generally not appropriate. You will learn in Week 8 more appropriate distribution families for this type of data.\nWe will analyse the duration of vowels in Italian from Coretta (2019) and how speech rate affects vowel duration. Vowel duration should be pretty straightforward, and speech rate is simply the number of syllables per second, calculated from the frame sentence the vowel was uttered in. An expectation we might have is that vowels get shorter with increasing speech rate. You will notice how this is a very vague hypothesis: how shorter do they get? Is the shortening the same across all speech rates, or does it get weaker with higher speech rates? Our expectation/hypothesis simply states that vowels get shorter with increasing speech rate. Maybe we could do better and use what we know from speech production and come up with something more precise, but this type of vague hypothesis are very common, if not standard, in linguistic research, so we will stick to it for practical and pedagogical reasons. Remember, however, that robust research should strive for precision. In short, we will try to answer the following research question:\nLet’s load the R data file coretta2018a/ita_egg.rda. It contains several phonetic measurements obtained from audio and electroglottographic recordings. You can find the information on the data in the related entry on the QM Data website: Electroglottographic data on Italian.\nload(\"data/coretta2018a/ita_egg.rda\")\n\nita_egg\nLet’s plot vowel duration and speech rate in a scatter plot. The relevant columns in the tibble are v1_duration and speech_rate. The points in the plot are the individual observations (measurements) of vowels in the 19 speakers of Italian.\nita_egg |&gt; \n  ggplot(aes(speech_rate, v1_duration)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Speech rate (syllables per second)\",\n    y = \"Vowel duration (ms)\"\n  )\n\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 24.1: Scatter plot of speech rate (as number of syllables per second) and vowel duration (in milliseconds) in 19 Italian speakers.\nYou might be wondering what is the warning about missing values. This is because some observations of vowel duration (v1_duration) in the data are missing (i.e. they are NA, “Not Available”). We can drop them from the tibble using drop_na(). This function takes column names as arguments: each row that has NA in any of the columns listed in the function will be dropped, so be careful when using drop_na() without listing columns because any NA value in any column with make the row be removed.\nita_egg_clean &lt;- ita_egg |&gt; \n  drop_na(v1_duration)\nWe will use ita_egg_clean for the rest of the tutorial. Let’s reproduce the plot, but let’s add a regression line. This is the straight line we have been talking about, the line that is reconstructed by regression models. It is sometimes useful to add the regression line to the scatter plots to show the linear relationship of the two variables in the plot. We can quickly add regression lines to scatter plots with the smooth geometry: geom_smooth(method = \"lm\"). The method argument lets us pick the type of method to create the “smooth”: here, we want a regression line so we choose lm for linear model (remember, linear model is another term for regression model). Under the hood, geom_smooth() fits a regression model to estimate the regression line and plots it. We will fit our own regression model below, so for now the regression line is just for show. Figure 24.2 shows the scatter plot with the regression line. You can ignore the message about the formula.\nita_egg_clean |&gt; \n  ggplot(aes(speech_rate, v1_duration)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Speech rate (syllables per second)\",\n    y = \"Vowel duration (ms)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 24.2: Relationship between speech rate (as number of syllables per second) and vowel duration (in milliseconds) in 19 Italian speakers.\nBy glancing at the individual points, we can see a negative relationship between speech rate and vowel duration: vowels get shorter with greater speech rate. This is reflected by the regression line too, which has a negative slope. A negative slope means that when the values on the x-axis increase, the values on the y-axis decrease. When the opposite is true, i.e. when with increasing x-axis values you observe increasing y-axis values, we say the regression line has positive slope. Positive and negative slope correspond to what some call a direct and inverse relationship. In terms of the equation of the line \\(y = \\beta_0 + \\beta_1 x\\), a positive slope means \\(\\beta_1\\) is a positive number and, conversely, a negative slope means \\(\\beta_1\\) is a negative number. When \\(\\beta_0\\) is zero, then the regression line is flat and we say that the two variables are independent: changing one, does not systematically change the other. You explored these features of the regression slope in Chapter 23, when playing around with the Regression Models Illustrated app and when answering the quiz.\nFigure 24.2 looks very nice, but the plot doesn’t tell us much about the estimates for \\(\\beta_0\\) and \\(\\beta_1\\). For that, we need to actually fit the regression model.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#vowel-duration-in-italian-the-data",
    "href": "ch-regression.html#vowel-duration-in-italian-the-data",
    "title": "24  Regression models",
    "section": "",
    "text": "What is the relationship between vowel duration and speech rate?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDirect/positive and inverse/negative relationship\n\n\n\nWhen the slope \\(\\beta_1\\) is positive, the regression line has positive slope and \\(x\\) and \\(y\\) have a direct relationship.\nWhen the slope \\(\\beta_0\\) is negative, the regression line has negative slope and \\(x\\) and \\(y\\) have an inverse relationship.\nWhen the slope \\(\\beta_0\\) is 0 zero, the regression line is flat and \\(x\\) and \\(y\\) are independent.\n\n\n\n\n24.1.1 The model\nLet’s move on onto fitting a Gaussian regression model to vowel duration as the outcome variable and speech rate as the predictor. We are assuming that vowel duration follows a Gaussian distribution (although as mentioned at the beginning of this chapter, this is not the case, but it will do for now). Here is the model we will fit, in mathematical notation.\n\\[\n\\begin{align}\nvdur & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 \\cdot sr\\\\\n\\end{align}\n\\]\nYou can read that as:\n\nVowel duration (\\(vdur\\)) is distributed (\\(\\sim\\)) according to a Gaussian distribution (\\(Gaussian(\\mu, \\sigma)\\)).\nThe mean \\(\\mu\\) is equal to the sum of \\(\\beta_0\\) (the intercept) and the product of \\(\\beta_1\\) and speech rate (\\(\\beta_1 \\cdot sr\\)). The formula of \\(\\mu\\) is regression equation of the model.\n\nThe regression model estimates the parameters in the mathematical formulae: the parameters to be estimated in regression models are usually represented with Greek letters (hence why we adopted this notation for the linear equation). The regression model in the formulae above has to estimate the following three parameters:\n\nThe regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nThe standard deviation of the Gaussian distribution, \\(\\sigma\\).\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are called the regression coefficients because they are coefficients of the regression equation. In maths, a coefficient is simply a constant value that multiplies a “basis” in the equation, like the variable \\(sr\\). In the regression equation of the model, \\(\\beta_1\\) is a multiplier of the variable \\(sr\\), but what about \\(\\beta_0\\)? Well, it is implied that \\(\\beta_0\\) is a multiplier of the constant basis 1, because \\(\\beta_0 \\cdot 1 = \\beta_0\\). Knowing this should now reveal the reason behind the strange formula that R uses in Gaussian models like the ones we fitted in Chapter 22: the 1 in the formula stands for the constant basis of the intercept, meaning that the model estimates the coefficient of the intercept, \\(\\beta_0\\). Gaussian models without predictors are in fact also called intercept-only regression models, because only an intercept is estimated. There is no slope in the model because there is not variable \\(x\\) to multiply the slope with.\nGoing back to our regression model of vowel duration and speech rate, we can rewrite the model formula to make the constant basis 1 explicit, thus:\n\\[\n\\begin{align}\nvdur & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 \\cdot 1 + \\beta_1 \\cdot sr\\\\\n\\end{align}\n\\]\nTo instruct R to model vowel duration as a function of the numeric predictor speech rate you simply add it to the 1 we have used in the right-hand side of the tilde in Chapter 22 (i.e. v1_duration ~ 1): so v1_duration ~ 1 + speech_rate. The R formula is based on the bases you multiply the coefficients with in the mathematical formula: \\(1\\) and \\(sr\\). In R parlance, the 1 and sr in the R formula are called predictor terms, or terms for short. While the predictor \\(sr\\) can take different values, the \\(1\\) is constant so it is also called the constant term, or the intercept term (because it is the basis of the intercept \\(\\beta_0\\)). In the R formula, you don’t explicitly include the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), just the bases. Put all this together and you get the 1 + speech_rate part of the formula. There is more: in R, since the \\(1\\) is a constant, you can omit it! So v1_duration ~ 1 + speech_rate can also be written as v1_duration ~ speech_rate. They are equivalent.\nThat was probably a lot! But now that we have clarified how the R formula is set up, we can proceed and fit the model. Here is the full code to fit a Gaussian regression model of vowel duration with brms.\n\nlibrary(brms)\n\nvow_bm &lt;- brm(\n  # `1 +` can be omitted.\n  v1_duration ~ 1 + speech_rate,\n  # v1_duration ~ speech rate,\n  family = gaussian,\n  data = ita_egg_clean\n)\n\n\n\n\n\n\n\nImportantR Note: The rethinking package\n\n\n\n\n\nMcElreath’s textbook Statistical Rethinking (McElreath 2020) comes with an R package, rethinking, that lets you fit data using R formulae that resemble the mathematical formulae more closely. For example, in rethinking the model formula of our model of vowel duration would look like the code below. The package requires you to include all the formulae in a list. Note that the rethinking package does not set default priors, so we have included them below.\nalist(\n  v1_duration ~ dnorm(mu, sigma),\n  mu &lt;- b0 + b2 * speech_rate,\n  # Priors\n  a ~ dt(80, 25), \n  b ~ dt(0, 1),\n  sigma ~ dt(0, 25)\n)\nIf you look closely at the the first two lines in the list, you should recognise the mathematical formulae of the model we have seen above.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#interpret-the-model-summary",
    "href": "ch-regression.html#interpret-the-model-summary",
    "title": "24  Regression models",
    "section": "24.2 Interpret the model summary",
    "text": "24.2 Interpret the model summary\nAs we has seen in Chapter 22, to obtain a summary of the model, we use the summary() function.\n\nsummary(vow_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: v1_duration ~ speech_rate \n   Data: ita_egg_clean (Number of observations: 3253) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     198.47      3.33   191.76   204.97 1.00     3681     2274\nspeech_rate   -21.73      0.62   -22.93   -20.49 1.00     3623     2421\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    21.66      0.27    21.14    22.19 1.00     3855     2615\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s focus on the Regression Coefficients table of the summary. It should now be clear why in the summary of the model in Chapter 22, the summaries for the mean \\(\\mu\\) (i.e. \\(\\beta_0\\)) were in the regression coefficients table. The table of the regression model we just fit has two coefficients. To understand what they are, just remember the equation of the line and the model formula above, repeated here.\n\\[\n\\begin{align}\nvdur & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 \\cdot 1 + \\beta_1 \\cdot sr\\\\\n\\end{align}\n\\]\n\nIntercept is \\(\\beta_0\\): this is the mean vowel duration, when speech rate is 0.\nspeech_rate is \\(\\beta_1\\): this is the change in vowel duration for each unit increase of speech rate.\n\nThis should make sense, if you understand the equation of a line: \\(y = \\beta_0 + \\beta_1 x\\). Remember, the intercept \\(\\beta_0\\) is the \\(y\\) value when \\(x\\) is 0. In our regression model, \\(y\\) is vowel duration \\(vdur\\) and \\(x\\) is speech rate \\(sr\\). So the Intercept is the mean vowel duration when speech rate is 0. Recall that the Estimate and Est.Error column are simply the mean and standard deviation of the posterior probability distributions of the estimate of Intercept and speech_rate respectively. In this model we just have two coefficients instead of one. Looking at the 95% Credible Intervals (CrIs), we can say that based on the model and data:\n\nThe mean vowel duration, when speech rate is 0 syl/s, is between 192 and 205 ms, at 95% confidence.\nWe can be 95% confident that, for each unit increase of speech rate (i.e. for each increase of one syllable per second), the duration of the vowel decreases by 20.5-23 ms.\n\nTo answer our research question (what is the relationship between vowel duration and speech rate?) we can say that with increasing speech rate, vowel duration decreases. We can be more precise than that and say that for each increase of one syllable per second, the vowel becomes 20.5 to 23 ms shorter, at 95% confidence (that is our 95% CrI). Since this is a regression model, it doesn’t matter if we are comparing \\(vdur\\) when \\(sr = 0\\) vs when \\(sr = 1\\), or when \\(sr = 6.5\\) vs when \\(sr = 7.5\\). The slope coefficient \\(\\beta_1\\) tells us what to add to \\(vdur\\) when you increase \\(sr\\) by one. For example, assume you want to obtain from the model an estimate of mean \\(vdur\\) when speech rate is 5. You would just plug in \\(sr = 5\\) in the formula:\n\\[\n\\begin{align}\n\\mu & = \\beta_0 + \\beta_1 \\cdot 5\\\\\n\\end{align}\n\\]\nSo that is the intercept \\(\\beta_0\\) plus \\(\\beta_1\\) times the speech rate, i.e. 5. The only complication is that here \\(\\beta_0\\) and \\(\\beta_1\\) are full probability distributions, rather than single values like you would have in the simple case of the equation of a line. Since the coefficients are posterior probability distributions, any operation like addition and multiplication will lead to a posterior probability distribution, i.e. the posterior probability distribution of \\(\\mu\\). You will learn how do to these operations in the next chapter, Chapter 25.\nFor now, let’s focus on the posterior distributions of the coefficients. To see what the posterior probability densities of \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) look like, you can quickly plot them with the plot() function, as we did in Chapter 22. There is also another way: we can use the mcmc_dens() function from the bayesplot package (why it’s mcmc_dens() will become clear in Chapter 25). By default the function plots the posterior densities of all parameters in the model, plus other internal parameters that we usually don’t care about. We can conveniently specify which parameters to plot with the pars argument, which takes a character vector. The names of the parameters for the regression coefficients are slightly different than what you see in the summary: b_Intercept and b_speech_rate. These are just the names you see in the summary, with a prefixed b_. The b_ stands for beta coefficient, which makes sense, since these are the \\(\\beta_0\\) and \\(\\beta_1\\) coefficients. Figure 24.3 shows the output of the mcmc_dens() function.\n\nlibrary(bayesplot)\n\nmcmc_dens(vow_bm, pars = c(\"b_Intercept\", \"b_speech_rate\", \"sigma\"))\n\n\n\n\n\n\n\nFigure 24.3: Posterior density plots of a regression model fitted to vowel duration.\n\n\n\n\n\nThese are the results of the regression model: the full posterior probability distributions of the three parameters \\(\\beta_0\\), \\(\\beta_1\\), \\(\\sigma\\). The posteriors can be described, as with any other probability distribution, by the values of their parameters. It is common to use the mean and standard deviation, the parameters of the Gaussian distribution. This is independent from the fact that we fitted a Gaussian distribution: posterior distributions tend to be bell-shaped, i.e. Gaussian. This is why the Regression Coefficients table of the summary reports mean and SD, i.e. Estimate and Est.Err, as mentioned earlier.\nYou should always also plot the model predictions, i.e. the predicted values of vowel duration based on the model predictors (here just speech_rate). You will learn more advanced methods later on, but for now you can use conditional_effects() from the brms package.\n\nconditional_effects(vow_bm, effects = \"speech_rate\")\n\n\n\n\n\n\n\nFigure 24.4: Posterior predictions of vowel duration based on speech rate from a regression model.\n\n\n\n\n\nIf you wish to include the raw data in the plot, you can wrap conditional_effects() in plot() and specify points = TRUE. Any argument that needs to be passed to geom_point() (these are all ggplot2 plots!) can be specified in a list as the argument point_args. Here we are making the points transparent.\n\nplot(\n  conditional_effects(vow_bm, effects = \"speech_rate\"),\n  points = TRUE,\n  point_args = list(alpha = 0.1)\n)\n\n\n\n\n\n\n\nFigure 24.5: Posterior predictions of vowel duration based on speech rate from a regression model (repr.).\n\n\n\n\n\nThis plot looks basically the same as Figure 24.2. Indeed, in Figure 24.2 we used geom_smooth() to add a regression line from a regression model. A warning told us that this formula was used: y ~ x. In the context of that plot, that means the smooth function fitted a regression model with speech rate as x and vowel duration as y. This is because the aesthetics are exactly aes(x = speech_rate, y = v1_duration) (we didn’t write the x = and y = because they are implied). So geom_smooth() has fitted exactly the same model we have fitted with brms. It might look trivial to fit a full model when you can just look at the regression line of geom_smooth(). This is not the case for two reasons: first, you just see a regression line, but you don’t know what the posterior distributions of the parameters are; second, with more complex scenarios, geom_smooth() falls short and can only produce regression lines based on very simple formulae like y ~ x.1",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#reporting",
    "href": "ch-regression.html#reporting",
    "title": "24  Regression models",
    "section": "24.3 Reporting",
    "text": "24.3 Reporting\nYou have seen an example of reporting in Chapter 22. We can use that as a template for reporting a regression model, by reworking a few parts and adding information related to the numeric predictor in the regression. You could report the vowel duration regression model like so:\n\nWe fitted a Bayesian regression model using the brms package (Bürkner 2017) in R (R Core Team 2025). We used a Gaussian distribution for the outcome variable, vowel duration (in milliseconds). We included speech rate (measured as syllables per second) as the regression predictor.\nBased on the model results, there is a 95% probability that the mean vowel duration, when speech rate is 0, is between 192 and 205 ms (mean = 198, SD = 3). For each unit increase of speech rate (i.e. for each one syllable per second added), vowel duration decreases by 20 to 23 ms (mean = -22, SD = 1). The residual standard deviation is between 21 and 22 ms (mean = 22, SD = 0).\n\nNote the wording of the speech rate coefficient: “vowel duration decreases by 20 to 23 ms”. The speech rate coefficient 95% CrI is fully negative (i.e. both lower and upper limit are negative) so we can say that vowel duration decreases. Furthermore, since we say “decreases” then we should report the CrI limits as positive numbers. Think about it: we say “decrease X by 2” to mean “X - 2”, rather than “decrease X by -2”. Finally, given we flipped the signs of the CrI limits, it is clearer to write “20 to 23 ms”, rather than the other way round as you would if you reported the interval as is: 95% CrI [-23, -20].\nAnother point to note is that in the reporting style I am using in this book, we place more emphasis on the posterior CrI than on the posterior mean and SD. So the CrI is in the main text, while mean and SD are between parentheses. Other researchers might in fact do it the other way round. Whatever you decide to do, be consistent. Finally, it is unusual to report the coefficients of \\(\\sigma\\): I have done it here for completeness, since it doesn’t hurt to do so.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#whats-next",
    "href": "ch-regression.html#whats-next",
    "title": "24  Regression models",
    "section": "24.4 What’s next",
    "text": "24.4 What’s next\nIn this chapter you have learned the very basics of Bayesian regression models. As mentioned above, regression models with brms are very flexible and you can easily fit very complex models with a variety of distribution families (for a list of available families, see ?brmsfamily; you can even define your own distributions!). The perk of using brms is that you can just learn the basics of one package and one approach and use it to fit a large variety of regression models. This is different from the standard frequentist approach, where different models require different packages or functions, with their different syntax and quirks. In the following weeks, you will build your understanding of Bayesian regression models, which will enable you to approach even the most complex models! However, due to time limits you won’t learn everything there is to learn in this course. Developing conceptual and practical skills in quantitative methods is a long-term process and unfortunately one semester will not be enough. So be prepared to continue your learning journey for years to come!",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#summary",
    "href": "ch-regression.html#summary",
    "title": "24  Regression models",
    "section": "24.5 Summary",
    "text": "24.5 Summary\n\n\n\n\n\n\n\nGaussian regression models have the following mathematical form:\n\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 x\\\\\n\\end{align}\n\\]\n\nA regression model estimates an intercept \\(\\beta_0\\) and a slope \\(\\beta_1\\) from the data (\\(x\\) and \\(y\\)).\nRegression models in brms are fit with the R formula y ~ 1 + x. We can omit the constant/intercept term: y ~ x.\nThe intercept \\(\\beta_0\\) is the mean \\(y\\) when \\(x\\) is 0 zero.\nThe slope \\(\\beta_1\\) is the change in \\(y\\) for every unit increase of \\(x\\).\n\n\n\n\n\n\n\n\nBürkner, Paul-Christian. 2017. “Brms: An r Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 128. https://doi.org/10.18637/jss.v080.i01.\n\n\nCoretta, Stefano. 2019. “Vowel Duration, Voicing Duration, and Vowel Height: Acoustic and Articulatory Data from Italian [Research Compendium].” https://doi.org/10.17605/OSF.IO/XDGFZ.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing [Version 4.5.0].\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#footnotes",
    "href": "ch-regression.html#footnotes",
    "title": "24  Regression models",
    "section": "",
    "text": "Technically, geom_smooth() uses lm() under the hood. This is a base R function that fits regression models using maximum likelihood estimation (MLE). This way of estimating regression coefficients is common in frequentist approaches to regression modelling and we will not treat it here. If you are interested about lm() and MLE, you can learn about these in Winter (2020).↩︎",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html",
    "href": "ch-regression-draws.html",
    "title": "25  Wrangling MCMC draws",
    "section": "",
    "text": "25.1 MCMC what?\nBayesian regression models fitted with brms/Stan use the Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate the probability distributions of the model’s parameters. You have first encountered MCMC in Chapter 22: the text printed when running brm() is about the MCMC. Bayesian models reconstruct the posterior probability distribution of a set of parameters. More precisely, they reconstruct (i.e. estimate) the joint probability distribution of the parameters. In other words, all parameters are estimated at the same time: think of this as a multidimensional space of probability, with one dimension per parameter. With two parameters, like intercept and slope, this is a 3-dimensional space, something we can imagine: think of a landscape with hills and valleys, where the x-axis are values for the intercept, the y-axis are values for the slope and the z-axis (the vertical axis) are probability densities. The hills represent high probability areas and valleys represent low probability areas. This landscape of hills and valleys is determined by the two components of the Bayes’ Theorem: the prior probability distribution \\(P(h)\\) and the probability of the data given the prior, \\(P(d | h)\\).\nConstructing the landscape from the prior and data analytically (i.e. solving the mathematical equation of Bayes’ Theorem) is very often very hard or even impossible. The MCMC algorithm allows us to sample points in the landscape without actually mathematically reconstruct the landscape. In the Stan software, which brms uses to fit Bayesian models, the MCMC algorithm uses a specific implementation called Hamiltonian Monte Carlo (HMC). The HMC version of MCMC simulates physical particles rolling through the posterior landscape, using equations from physical mechanics. At each iteration, the algorithm “flicks” a particle (like a pinball ball) and then stops it in its tracks: the place on the posterior landscape where the particle stops is taken as a “draw”. In the 3-dimensional landscape of intercept and slope, the intercept and slope values corresponding to the spot the particle was stopped are recorded. So the draw from one iteration holds one value for the intercept and one for the slope. The algorithm proceeds for several iterations, thus creating a list of draws. These draws can be used to plot and summarise the posterior distributions of the parameters. The draws are called posterior draws, because they come from the posterior probability distribution.\nThis is a bit abstract and if you want to learn more about MCMC, I recommend McElreath (2020), Ch 9 and Nicenboim, Schad, and Vasishth (2025), Ch 3. Note that to be a proficient user of brms and Bayesian regression models, you don’t need to fully understand the mathematics behind MCMC algorithms, as long as you understand them conceptually.\nWhen you run a model with brms, the draws (i.e. the sampled values) are stored in the model object. All operations on a model, like obtaining the summary(), are actually operations on those draws. We normally fit regression models with four MCMC chains. The sampling algorithm within each chain runs for 2000 iterations by default. The first half (1000 iterations) are used to “warm up” the algorithm (i.e. tune certain parameters related to the mechanics equations that make the particles move) while the second half (1000 iterations) are the ones included in the actual posterior draws. Since four chains run with 2000 iterations of which 1000 are kept as posterior draws, we end up with 4000 draws we can use to learn details about the posterior. The rest of this chapter will teach you how to extract and manipulate the model’s draws. We will do so by revisiting the model fitted in Chapter 24.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#reproducible-model-fit",
    "href": "ch-regression-draws.html#reproducible-model-fit",
    "title": "25  Wrangling MCMC draws",
    "section": "25.2 Reproducible model fit",
    "text": "25.2 Reproducible model fit\nBefore we move to the model, it is worth making a couple practical considerations. Fitting simple models with brms is relatively quick. However, more complex model using larger data sets can take some time for the MCMC to efficiently sample the posterior distribution (sometimes even hours!). It is useful to save the model fit to a file so that, once the model is fit once, you don’t have to fit it again. This can be done by specifying a file path in the file argument in the brm() function. I suggest developing the habit of having a dedicated cache/ in your Quarto project to save all of the brms model objects in. Go ahead and create a cache/ folder in you project. Then, in your week-05.qmd document, rewrite the model from the previous chapter like so:\n\nvow_bm &lt;- brm(\n  # `1 +` can be omitted.\n  v1_duration ~ speech_rate,\n  family = gaussian,\n  data = ita_egg_clean,\n  cores = 4,\n  seed = 20912,\n  file = \"cache/vow_bm\"\n)\n\nThe file argument tells brms to save the model output to the cache/ folder in a file called vow_bm.rds. The extension .rds is appended automatically (this is the same file type you encountered where reading data, like glot_status.rds). What about cores and seed? When the model is fit, four MCMC chains are run. By default, these are run sequentially: the first chain, then the second, then the third and finally the fourth. But we can speed things up a bit by running them in parallel on separate computer cores. Virtually all computers today have at least four cores, so we can run the four chains using four cores. This is what cores = 4 does: it tells brms to run each chain on one core so they are run in parallel. Since the MCMC algorithm contains a random component (physical particles are randomly flicked across the landscape), every time you refit the model, a different set of draws are drawn (because the particles stop at random places in the landscape). One way to make the model reproducible (meaning, obtaining exactly the same draws every time) is to set a “seed”. In computing, a seed is a number used for random number generation: when set, the same list of “random” numbers is produced. The MCMC algorithm uses random number generation to run itself, so by setting the seed we are in fact “fixing” the randomness of the algorithm. The seed number can be any number: here I set it to 20912.\nNow run the model. The model will be fitted and the model object will be saved in cache/ with the file name vow_bm.rds. If you now re-run the same code again, you will notice that brm() does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works!). This saves time: you fit the model once but you can read the output multiple times. This is also good for reproducibility: an independent researcher with access to your code and the cache folder can run your code and get exactly the same results as yours.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you save the model fit to a file, R does not keep track of changes in the model specification (like changes in formula or data and so on), so if you make changes to model, you need to delete the saved model file before re-running the code for the changes to have effect!",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#extract-mcmc-posterior-draws",
    "href": "ch-regression-draws.html#extract-mcmc-posterior-draws",
    "title": "25  Wrangling MCMC draws",
    "section": "25.3 Extract MCMC posterior draws",
    "text": "25.3 Extract MCMC posterior draws\nThere are different ways to extract the MCMC posterior draws from a fitted model. In this book, we will use the as_draws_df() function from the posterior package. The function extracts the draws from a Bayesian regression model and outputs them as a data frame. Before we extract the draws from the vow_bm model, let’s revisit the summary.\n\nsummary(vow_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: v1_duration ~ speech_rate \n   Data: ita_egg_clean (Number of observations: 3253) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     198.47      3.33   191.76   204.97 1.00     3681     2274\nspeech_rate   -21.73      0.62   -22.93   -20.49 1.00     3623     2421\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    21.66      0.27    21.14    22.19 1.00     3855     2615\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe Draws information in the summary are exactly that: information on the MCMC draws of the model. It says 4 chains were run each with 2000 iterations of which 1000 used for warm-up. thin = 1 just tells brms to keep all the post-warm up draws, and it’s fine as is so you can just ignore it. Then the summary tells us that there are 4000 total post-warm-up draws. We are good to go! We can extract the MCMC draws from the model using the as_draws_df() function. This function returns a data frame (more specifically a tidyverse tibble with class draws_df) with values from each draw of the MCMC algorithm. Since there are 4000 post-warm-up draws, the tibble has 4000 rows.\n\nvow_bm_draws &lt;- as_draws_df(vow_bm)\nvow_bm_draws\n\n\n  \n\n\n\nIgnore the Intercept, lprior and lp__ columns, they are for internal safekeeping. Open the data frame in the RStudio viewer. You will see three extra column: .chain, .iteration and .draw (which are mentioned in the message printed with the tibble). They indicate:\n\n.chain: The MCMC chain number (1 to 4).\n.iteration: The iteration number within chain (1 to 1000).\n.draw: The draw number across all chains (1 to 4000).\n\nMake sure that you understand these columns in light of the MCMC algorithm. The following columns contain the drawn values at each draw for three parameters of the model: b_Intercept, b_speech_rate and sigma. To remind yourself what these mean, let’s have a look at the mathematical formula of the model.\n\\[\n\\begin{align}\nvdur & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\mu        & = \\beta_0 + \\beta_1 \\cdot sr \\\\\n\\end{align}\n\\]\nSo:\n\nb_Intercept is \\(\\beta_0\\). This is the mean vowel duration when speech rate is zero.\nb_speech_rate is \\(\\beta_1\\). This is the change in vowel duration for each unit increase of speech rate.\nsigma is \\(\\sigma\\). This is the overall standard deviation of vowel duration (the standard deviation of the residual error).\n\nAny inference made on the basis of the model are inferences derived from the draws. One could say that the model “results” are, to put it simply, these draws and that the draws can be used to make inferences about the population one is investigating.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#summary-measures-of-the-posterior-draws",
    "href": "ch-regression-draws.html#summary-measures-of-the-posterior-draws",
    "title": "25  Wrangling MCMC draws",
    "section": "25.4 Summary measures of the posterior draws",
    "text": "25.4 Summary measures of the posterior draws\nThe Regression Coefficients table from the summary() of the model reports summary measures calculated from the drawn values of b_Intercept and b_speech_rate. These summary measures are the mean (Estimate), the standard deviation (Est.error) of the draws and the lower and upper limits of the 95% Credible Interval (CrI). We can obtain those same measures ourselves from the data frame with the draws. Let’s calculate the mean and SD of b_Intercept and b_speech_rate (we round to the second digit with round(2)).\n\n# Intercept\nmean(vow_bm_draws$b_Intercept) |&gt; round(2)\n\n[1] 198.47\n\nsd(vow_bm_draws$b_Intercept) |&gt; round(2)\n\n[1] 3.33\n\n# Speech rate\nmean(vow_bm_draws$b_speech_rate) |&gt; round(2)\n\n[1] -21.73\n\nsd(vow_bm_draws$b_speech_rate) |&gt; round(2)\n\n[1] 0.62\n\n\nCompare the values obtained now with the values in the model summary above. They are the same, because the summary measures in the model summary are simply summary measures of the draws. What if we want to calculate the Credible Intervals (CrIs)? In Chapter 19, you learned about the quantile function (the inverse CDF) to calculate intervals from theoretical distributions. However, here we need to calculate intervals from a sample of posterior draws: the MCMC draws. Note that central probability intervals of posterior draws are called Credible Intervals in Bayesian statistics, so CrI is just a specific type of interval. To obtain intervals from samples we can use the quantile2() function from the posterior package. This function takes two arguments: x, a vector of values to calculate the interval of, and probs, a vector of probabilities, like qnorm(). By default, probs = c(0.05, 0.95). This gives you a 90% CrI interval, but the model summary returns by default a 95% CrI. For a 95% CrI, we need the 2.5th percentile and the 97.5th percentile: c(0.025, 0.975). Here’s the code:\n\nlibrary(posterior)\n\n# Intercept\nquantile2(vow_bm_draws$b_Intercept, c(0.025, 0.975)) |&gt; round(2)\n\n  q2.5  q97.5 \n191.76 204.97 \n\n# Speech rate\nquantile2(vow_bm_draws$b_speech_rate, c(0.025, 0.975)) |&gt; round(2)\n\n  q2.5  q97.5 \n-22.93 -20.49 \n\n\nCompare these values with the ones in summary: again they are the same. Remember: a 95% CrI tells us that there is a 95% probability, given the model and data, that the value of the parameters is between the lower and upper limit of the interval. So a 90% CrI tells us that there is an 90% probability that the value is between the lower and upper limit, a 60% interval that there is a 60% probability and so on. We can also say that we are 95% confident that the value lies between the limits. Intervals at lower level of probability are narrower (they span a smaller range of values) than intervals at higher level of probability: so a 95% CrI is always wider than an 80% CrI, which is wider than a 60% CrI and so on. A narrower CrI means more precision: we have a more precise expectation of which range the parameter lies in. But with more precision comes more uncertainty: a 60% CrI is more precise than a 95% CrI because it is narrower, but it is also more uncertain because we go from a 95% probability to a 60% probability. This is the precision/confidence trade-off that we have to live with when doing research. Vasishth and Gelman (2021) say (in the context of frequentist statistics): “[we have to learn] to accept the fact that—in almost all practical data analysis situations—we can only draw uncertain conclusions from data.”\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nCalculate the 90%, 80% and 60% CrIs of b_Intecept and b_speech_rate.\n\n\nWith this model, vow_bm, getting all of these CrIs might look trivial: the 95% CrIs are quite narrow, giving us quite a precise range of values for both the intercept and the coefficient of speech rate. This is because there is quite a lot of data and the model is quite simple, there is only one predictor. With more complex model and smaller data sets, uncertainty is greater and the intervals will span a large range of values. We will see examples later in the book. In those cases, it is helpful to be able to discuss CrIs at different levels of probability, since a lower-probability CrI might tells us something clearer about what we are investigating, while warning us of the increased uncertainty that comes with it.\nIn the previous chapter, we mentioned that one can calculate the posterior probability of mean vowel duration \\(vdur\\) based on a specific value of speech rate \\(sr\\), using the model. We also noted that since we are working with MCMC it is not just like plugging in numbers according to the model’s equation, but rather we need to use the entire probability distributions. In fact, we can use the MCMC draws and plug them in directly, as you would with a single number. However, with MCMC draws the operations in the equation are applied draw-wise. Let’s see what this means by means of code:\n\nvow_bm_draws &lt;- vow_bm_draws |&gt; \n  mutate(\n    vdur_sr5 = b_Intercept + b_speech_rate * 5\n  )\nhead(vow_bm_draws$vdur_sr5)\n\n[1] 90.46033 89.10209 90.58991 90.38785 90.02444 89.54223\n\n\nThe mutate code vdur_sr5 = b_Intercept + b_speech_rate * 5 is based on \\(\\mu = \\beta_0 + \\beta_1 sr\\). In the code, \\(sr\\) is set to 5 (syllables per second). So the mean vowel duration when speech rate is 5 syl/s is the intercept \\(\\beta_0\\) plus the slope \\(\\beta_1\\) times 5. Since we are mutating a data frame where each row is one of the 4000 total draws, we are summing and multiplying the values within each row. This gives us a new column vdur_sr5 with 4000 predicted values of vowel duration, one per draw. You can then get summary measures, CrIs and even plot the values of the predicted column, like you would with the coefficients columns. The following code calculates the 95% CrI of the predicted vowel duration when speech rate is 5 syl/s (note we took 5 syl/s just as an example, but you can get prediction for any value of speech rate).\n\nquantile2(vow_bm_draws$vdur_sr5, c(0.025, 0.975)) |&gt; round()\n\n q2.5 q97.5 \n   89    91 \n\n\nBased on the model and data, when speech rate is 5 syl/s, the predicted vowel duration is 80-91 ms, at 95% probability.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#plotting-posterior-draws",
    "href": "ch-regression-draws.html#plotting-posterior-draws",
    "title": "25  Wrangling MCMC draws",
    "section": "25.5 Plotting posterior draws",
    "text": "25.5 Plotting posterior draws\nPlotting posterior draws is as straightforward as plotting any data. You already have all of the tools to understand plotting draws with ggplot2. To plot the reconstructed posterior probability distribution of any parameter, we plot the probability density (with geom_density()) of the draws of that parameter. Let’s plot b_speech_rate. This will be the posterior probability density of the change in vowel duration for each increase of one syllable per second. Figure 25.1 shows the posterior probability density of b_speech_rate. If you compare this plot with the central panel of Figure 24.3, the density curves are identical.\n\nvow_bm_draws |&gt;\n  ggplot(aes(b_speech_rate)) +\n  geom_density() +\n  geom_rug(alpha = 0.2)\n\n\n\n\n\n\n\nFigure 25.1: Posterior probability distribution of b_speech_rate.\n\n\n\n\n\nThe ggdist package has some convenience ggplot geometries and stats for plotting posterior densities with CrIs. The stat_halfeye() can shade the area under the curve depending on the specified interval levels, like in the code for Figure 25.2 below, which shows 50%, 80% and 95% CrIs. Below the density curve there are error bars of increasing thickness, each corresponding to a CrI. The large dot represents the median of the draws (rather than the mean, like in the model summary). The median is another acceptable summary measure for posteriors.\n\nlibrary(ggdist)\n\nvow_bm_draws |&gt;\n  ggplot(aes(x = b_speech_rate)) +\n  stat_halfeye(\n    .width = c(0.5, 0.8, 0.95),\n    aes(fill = after_stat(level))\n  ) +\n  scale_fill_brewer(na.translate = FALSE) +\n  geom_rug(alpha = 0.2)\n\n\n\n\n\n\n\nFigure 25.2: Posterior probability distribution of b_speech_rate with credible intervals.\n\n\n\n\n\nThe aes(fill = after_stat(level)) requires a bit of explanation. We are using aes() because we are mapping the fill of the shaded areas to some data, i.e. the level of the CrIs: 0.5, 0.8, 0.95. These are specified in the .width argument. The CrI are calculated by stat_halfeye() from the supplied vow_bm_draws, and the function creates, under the hood, a data frame with the interval limits and a column level which specifies the interval level. So after_stat(level) is simply telling ggplot to use the level column for the fill from the data frame that is available after the stat (the halfeye) has been computed (if you want to know more, you can check the aes_eval documentation from ggplot2). You can learn more about ggdist visualisation tools on the ggdist website.\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nPlot the half-eyes of b_Intercept and sigma.\n\n\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press.\n\n\nNicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025. Introduction to Bayesian Data Analysis for Cognitive Science. https://bruno.nicenboim.me/bayescogsci/.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. “How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis.” Linguistics 59 (5): 13111342. https://doi.org/10.1515/ling-2019-0051.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-interim-summ.html",
    "href": "ch-interim-summ.html",
    "title": "26  Interim summary",
    "section": "",
    "text": "Week 6 of the QML course is “catch-up” week. There are no classes and no new contents are introduced, for you to be able to catch up with things if you need to and/or revise the contents of Week 1 to 5.\nIf you look back at what you knew when starting in Week 1 and what you have learned up until now, especially if you were a beginner, you should me very proud of yourself! We have covered a lot of concepts and skills in a very short period of time. You have learned about research methods, research questions and research hypotheses, the perils of the research cycle and questionable research practices, Bayesian inference, the basics of R, how to read, summarise and plot data, and how to fit and interpret simple Gaussian models, including regression models like \\(y \\sim x\\).\nThe contents of Week 1 to 5 are really the bare minimum one needs to know to be able to work towards being proficient in quantitative methods, but the road to proficiency is a long one. Week 7 to 10 continue the learning journey of regression models and it will provide you with background knowledge in frequentist inference, another approach to statistical inference. We will scratch the surface of regression modelling, and to really become functional you will need to learn beyond this course. There is so much we can fit in a one-semester course and to be able to get to a point where you can confidently navigate all there is to quantitative methods will take years. This is no different from studying linguistics: you don’t just do one semester-long course in linguistics and then you can write a full dissertation.\nHopefully you enjoyed the learning process so far and you look forward for the second part!",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Interim summary</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html",
    "href": "ch-regression-cat.html",
    "title": "27  Regression with categorical predictors",
    "section": "",
    "text": "27.1 Revisiting reaction times\nIn Chapter 24 you learned how to fit regression models of the following form in R using the brms package.\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 \\cdot x\\\\\n\\end{align}\n\\]\nIn these models, \\(x\\) was a numeric predictor, like speech rate in the chapter’s example. Numeric predictors are not the only type of predictors that a regression model can handle. Regression predictors can also be categorical variables: gender, age group, place of articulation, mono- vs bi-lingual, etc. However, regression model cannot handle categorical predictors directly: think about it, what would it mean to multiply \\(\\beta_1\\) by “female” or by “old”? Categorical predictors have to be re-coded as numbers.\nIn this chapter we will revisit the MALD reaction times (RTs) data from Chapter 22, this time with the following research question:\nThis chapter will teach you the default way of including categorical predictors in regression models: numerical coding with treatment contrasts. This is the most common way to code categorical predictors.\nLet’s read the MALD data (Tucker et al. 2019). The data contains reaction times from an auditory lexical decision task with English listener: the participants would hear a word (real or not) and would have to press a button to say if the word was a real English word or not.\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\nmald\nThe relevant columns are RT with the RTs in milliseconds and IsWord, the type of target word: it tells if the target word the listeners heard is a real English word (TRUE) or not (a nonce word, FALSE). Figure 27.1 shows the density plot of RTs, grouped by whether the target word is real or not. We can notice that the distribution of RTs with nonce (non-real) words is somewhat shifted towards higher RTs, indicating that more time is needed to process nonce words than real words.\nCode\nmald |&gt; \n  ggplot(aes(RT, fill = IsWord)) +\n  geom_density(alpha = 0.8) +\n  geom_rug(alpha = 0.1) +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nFigure 27.1: Density plot of reaction times from the MALD data (Tucker et al. 2019).\nYou might also notice that the “tails” of the distributions (the left and right sides) are not symmetric: the right tail is heavier that the left tail. This is a very common characteristics of RT values and of any variable that can only be positive (like vowel duration or the duration of any other phonetic unit). These variables are “bounded” to only positive numbers. You will learn later on that the values in these variables are generated by a log-normal distribution, rather than by a Gaussian distribution (which is “unbounded”). For the time being though, we will model the data as if they were generated by a Gaussian distribution, so that we can focus on the categorical predictor part.\nAnother way to present a numeric variable like RTs depending on categorical variables is to use a jitter plot, like Figure 27.2. A jitter plot places dots corresponding to the values in the data on “strips”. The strips are created by randomly jittering dots horizontally, so that they don’t all fall on a straight line. The width of the strips, aka the jitter, can be adjusted with the width argument. It’s subtle, but you can see how in the range 1 to 2 seconds there are a bit more dots in nonce words (right, orange) than in real words (left, green). In other words, the density of dots in that range is greater in nonce words than real words. If you compare again the densities in Figure 27.1 above, you will notice that the orange density in the 1-2 seconds range is higher in nonce words. These are just two ways of visualising the same thing.\nmald |&gt;\n  ggplot(aes(IsWord, RT, colour = IsWord)) +\n  # width controls the width of the strip with jittered points\n  geom_jitter(alpha = 0.15, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nFigure 27.2: Jitter plot of reactions times for real and nonce words.\nWe can even overlay density plots on jitter plots using “violins”, like in Figure 27.3. The violins are simply mirrored density plots, placed vertically on top of the jittered strips. The width of the violins can be adjusted with the width argument, like with the jitter.\nmald |&gt;\n  ggplot(aes(IsWord, RT, fill = IsWord)) +\n  geom_jitter(alpha = 0.15, width = 0.1) +\n  geom_violin(width = 0.2) +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nFigure 27.3: Jitter and violin plot of reactions times for real and nonce words.\nIt is good practice to show the raw data and the density using violin and jitter plots, because, so this should be your go-to choice for plotting numeric data by categorical groups, like RTs and type of word here. Now we can obtain a few summary measures. The following code groups the data by IsWord and then calculates the mean, median and standard deviation of RTs. After summarise(), we are using a trick to round all columns that are numeric, namely the columns with the mean, median and SD. The trick is to use the across() function in combination of where(). across(where(is.numeric), round) means “across columns where the type is numeric, round the value”.\nmald_summ &lt;- mald |&gt; \n  group_by(IsWord) |&gt; \n  summarise(\n    mean(RT), median(RT), sd(RT)\n  ) |&gt; \n  mutate(\n    # round all numeric columns\n    across(where(is.numeric), round)\n  )\n\nmald_summ\nThe mean and median RTs for nonce words (IsWord = FALSE) are about 100 ms higher than the mean and median of real words. We could stop here and call it a day, but we would make the mistake of not considering uncertainty and variability: this is just one (admittedly large) sample of all the RT values that could be produced by the entire English-speaking population. So we should apply inference from the sample to the population to obtain an estimate of the difference in RTs that accounts for that uncertainty and variability. We can use regression models to do that. The next sections will teach you how to model the RTs with regression models in brms.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#revisiting-reaction-times",
    "href": "ch-regression-cat.html#revisiting-reaction-times",
    "title": "27  Regression with categorical predictors",
    "section": "",
    "text": "TipJitter and violin plots\n\n\n\nData that combines one numeric and one or more categorical variables can be represented with jitter and violin plots.\nA jitter plot shows the numeric data in a strip of jittered points (one point per observation in the data). A violin is a mirrored density curve.\n\n\n\n\n\n\n\n\n\n\n\nImportantR Note: Tables with kable()\n\n\n\n\n\nWith Quarto, you can output the summaries as a table, using knitr::kable(). You can learn more about this here.\n\nknitr::kable(\n  mald_summ,\n  digits = 0,\n  col.names = c(\"Is word?\", \"mean\", \"median\", \"SD\")\n)\n\n\n\nTable 27.1: Mean, median and standard deviation of RTs for real and nonce words.\n\n\n\n\n\n\nIs word?\nmean\nmedian\nSD\n\n\n\n\nTRUE\n953\n888\n291\n\n\nFALSE\n1069\n994\n333",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#treatment-contrasts",
    "href": "ch-regression-cat.html#treatment-contrasts",
    "title": "27  Regression with categorical predictors",
    "section": "27.2 Treatment contrasts",
    "text": "27.2 Treatment contrasts\nWe can model RTs with a Gaussian distribution (although as mentioned above, this family of distribution is generally not appropriate for variables liker RTs) with a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\): \\(RT \\sim Gaussian(\\mu, \\sigma)\\). This time though we want to model a different mean \\(\\mu\\) depending on the word type in IsWord. How do we make the model estimate a different mean depending on IsWord? There are several ways of setting this up. The default method is to use so-called treatment contrasts which involves numerical coding of categorical predictors (the coding takes the same naming as the contrasts, so the coding for treatment contrasts is treatment coding). Let’s work by example with IsWord. This is a categorical predictor with two levels: TRUE and FALSE. With treatment contrasts, one level is chosen as the reference level and the other is compared to the reference level. The reference level is automatically set as the first level in the predictor in alphabetical order. This would mean that FALSE would be the reference level, because “F” comes before “T”.\nHowever, in the mald data, the column IsWord is a factor column and the levels have been ordered so that TRUE is the first level and FALSE the second. You can see this in the Environment panel, if you click on the arrow next to mald and look next to IsWord: it will say Factor w/ 2 levels \"TRUE\",\"FALSE\". You can also check the order of the levels of a factor with the levels() function.\n\nlevels(mald$IsWord)\n\n[1] \"TRUE\"  \"FALSE\"\n\n\nYou can set the order of the levels with the factor() function. If you don’t specify the order of the levels, the alphabetical order will be used. For example:\n\nfac &lt;- tibble(\n  fac = c(\"a\", \"a\", \"b\", \"b\", \"a\"),\n  fac_1 = factor(fac),\n  fac_2 = factor(fac, levels = c(\"b\", \"a\"))\n)\n\nlevels(fac$fac_1)\n\n[1] \"a\" \"b\"\n\nlevels(fac$fac_2)\n\n[1] \"b\" \"a\"\n\n\nWe will use IsWord with the order TRUE and FALSE. This means that TRUE will be the reference level and the RTs when IsWord is FALSE will be compared to the RTs of when IsWord is TRUE. In other words, now the mean \\(\\mu\\) varies depending on the level of IsWord. We need to numerically code IsWord (i.e. use numbers to refer to the levels of the categorical predictor) to be able to allow the mean to vary by the word type in the model formula. With treatment contrasts, treatment coding is used to numerically code categorical predictors. Treatment coding uses indicator variables which take the values 0 or 1. Let’s make an indicator variable \\(w\\) that is 0 when IsWord is TRUE, or 1 when IsWord is FALSE. We only need one indicator variable because there are only two levels and they can be coded with a 0 and a 1 respectively. This way of setting up indicator variables (using 0/1) is called dummy coding. See Table 27.2 for the correspondence between the predictor IsWord and the value of the indicator variable \\(w\\).\n\n\n\nTable 27.2: Treatment contrasts coding of the categorical predictor IsWord.\n\n\n\n\n\nIsWord\n\\(w\\)\n\n\n\n\nIsWord = TRUE\n0\n\n\nIsWord = FALSE\n1\n\n\n\n\n\n\nWe can now update the equation of \\(\\mu\\):\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot w_i\\\\\n\\end{align}\n\\]\nThe subscript \\(i\\) is an index of each observation in the data. There are 5000 observations in mald so \\(i = [1..5000]\\) (\\(i\\) is goes from 1 to 5000). \\(RT_i\\) then is indexing the specific observations in the data, \\(RT_1\\), \\(RT_2\\), \\(RT_3\\), …, \\(RT_{5000}\\). Each observation is from a real or nonce word: this is coded by \\(w_i\\). The subscript \\(i\\) in \\(w_i\\) is indexing the IsWord value of the \\(i\\)th observation. In the data, \\(RT_1 = 617\\) and \\(w_1 = 0\\) (i.e. TRUE). The mean \\(mu_i\\) also has a subscript \\(i\\). The \\(i\\) is there to say that now \\(\\mu\\) depends on the specific observation \\(i\\), so that we can model a different mean depending on \\(w_i\\). In fact, in the model in Chapter 24, \\(i\\) was implied to keep things simple, but technically it should be added, because the mean does depend on the value of \\(sr\\):\n\\[\n\\begin{align}\nvdur_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot sr_i\\\\\n\\end{align}\n\\]\nThese are just technicalities of the notation, so you shouldn’t be too tripped up about it, the substance of the model is the same.\nGoing back to our model of RTs as a function of word type (“as a function of” is another way of saying that you are modelling an outcome variable depending on a predictor), \\(\\beta_0\\) and \\(\\beta_1\\) are the regression coefficients, exactly like in the regression model with vowel duration and speech rate. You can think of \\(\\beta_0\\) as the model’s intercept and of \\(\\beta_1\\) as the model’s slope. Can you figure out why? Recall that a regression model with a numeric predictor is basically the formula of a line. In Chapter 24, we modelled vowel duration as a function of speech rate. The intercept of the regression line estimated by the model indicates where the line crosses the y-axis when the x value is 0: in that model, that was the vowel duration when speech rate is 0. In the model above, with RTs as a function of word type, the numeric predictor is the indicator variable \\(w\\), which stands for the categorical predictor IsWord. The formula of a line really works just with numbers so for the formula to make sense, we had to make the categorical predictor IsWord into a number and treatment contrasts is such one way of doing so.\nNow, \\(\\beta_0\\) is the model’s intercept because that is the mean RT when \\(w\\) is 0 (like with vowel duration when speech rate is 0; can you see the parallel?). And \\(\\beta_1\\) is the model’s slope because \\(\\beta_1\\) is the change in RT for a unit increase of \\(w\\), which is when \\(w\\) goes from 0 to 1. This in turn corresponds to the change in level in the categorical predictor. Do you see the connection? It’s a bit contorted, but once you get this, it should explain several aspects of regression models with categorical predictors and treatment contrasts. So \\(\\mu_i\\) is the sum of \\(\\beta_0\\) and \\(\\beta_1 \\cdot w_i\\). \\(w_i\\) is 0 (when the word is real) or 1 (the the word is a nonce word). That is why we write \\(RT_i\\): the subscript \\(i\\) allows us to pick the correct value of \\(w_i\\) for each specific observation. The result of plugging in the value of \\(w_i\\) is laid out in the following formulae.\n\\[\n\\begin{align}\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot w_i\\\\\n\\mu_{\\text{T}} & = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\\\\n\\mu_{\\text{F}} & = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\n\\end{align}\n\\]\n\nWhen IsWord is TRUE, the mean RT is equal to \\(\\beta_0\\).\nWhen IsWord is FALSE, the mean RT is equal to \\(\\beta_0 + \\beta_1\\).\n\nIf \\(\\beta_0\\) is the mean RT when IsWord is TRUE, what is \\(\\beta_1\\) by itself? Simple.\n\\[\n\\begin{align}\n\\beta_1 & = \\mu_\\text{F} - \\mu_\\text{T}\\\\\n& = (\\beta_0 + \\beta_1) - \\beta_0\n\\end{align}\n\\]\n\\(\\beta_0\\) is the difference between the mean RT when IsWord is TRUE and the mean RT when IsWord is FALSE. As mentioned above, with treatment contrasts you are comparing the second level to the first. So one regression coefficient will be the mean of the reference level, and the other coefficient will be the difference between the mean of the two levels. I know this is not particularly beginner-friendly, but this is the default in R when using categorical predictors and it is the most common way to code categorical predictors, so you will frequently find tables in academic articles with regression coefficients that have to be interpreted that way.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#model-rts-by-word-type",
    "href": "ch-regression-cat.html#model-rts-by-word-type",
    "title": "27  Regression with categorical predictors",
    "section": "27.3 Model RTs by word type",
    "text": "27.3 Model RTs by word type\nThat was a lot of theory, so let’s move onto the application of that theory. Fitting a regression model with a categorical predictor like IsWord is as simple as including the predictor in the model formula, to the right of the tilde ~. From now on we will stop including 1 + since an intercept term is always included by default.\n\nrt_bm_1 &lt;- brm(\n  # equivalent: RT ~ 1 + IsWord\n  RT ~ IsWord,\n  family = gaussian,\n  data = mald,\n  seed = 6725,\n  file = \"cache/ch-regression-cat-rt_bm_1\"\n)\n\nTreatment contrasts are applied by default: you do not need to create an indicator variable yourself or tell brms to use that coding (which is both a blessing and a curse). The following code chunk returns the summary of the model rt_bm_1.\n\nsummary(rt_bm_1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     953.14      6.12   941.17   965.17 1.00     4590     2897\nIsWordFALSE   116.34      8.80    98.74   134.14 1.00     4815     3235\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   312.47      3.11   306.44   318.60 1.00     4720     3456\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe first few lines should be familiar: they report information about the model, like the distribution family, the formula, the MCMC draws and so on. Then we have the Regression Coefficients table, just like in the model of Chapter 24. The estimates of the coefficients and the 95% Credible Intervals (CrIs) are reported in Table 27.3 (the table was generated with knitr::kable() from the R Note above! Expand the code to see it).\n\n\nCode\nfixef(rt_bm_1) |&gt; knitr::kable()\n\n\n\n\nTable 27.3: Regression coefficients from a model of RTs by word type.\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nIntercept\n953.1412\n6.120423\n941.16925\n965.1697\n\n\nIsWordFALSE\n116.3413\n8.800872\n98.73942\n134.1424\n\n\n\n\n\n\n\n\nJust to remind you: the Estimate column is the mean of the posterior distribution of the regression coefficients, and Est.Error is the standard deviation of the posterior distribution of the regression coefficients. Q2.5 and Q97.5 are the lower and upper limit of the 95% CrI of the posterior distribution of the regression coefficients. Let’s plot the posterior distributions of the two coefficients.\n\n\nCode\nmcmc_dens(rt_bm_1, pars = vars(starts_with(\"b_\")))\n\n\n\n\n\n\n\n\nFigure 27.4: Density plots of the posterior probability distributions of the regression coefficients of model rt_bm_1.\n\n\n\n\n\n\nb_Intercept (Intercept in the model summary) is the mean RT when IsWord is TRUE. This is \\(\\beta_0\\) in the model’s mathematical formula.\nb_IsWordFALSE (IsWordFALSE in the model summary) is the difference in mean RT between nonce and real words. This is \\(\\beta_1\\) in the model’s mathematical formula.\n\nMoving onto the 95% CrIs:\n\nThe 95% CrI of Intercept \\(\\beta_0\\) is [941, 965] ms: this means that there is a 95% probability that the mean RT when IsWord is TRUE is between 941 and 965 ms.\nThe 95% CrI of IsWordFALSE \\(\\beta_0\\) is [99, 134] ms: this means that there is a 95% probability that the difference in mean RT between nonce and real words is between 99 and 134 ms.\n\nSo here we have our answer: at 95% confidence (another way of saying at 95% probability), based on the model and data, RTs for nonce words are 99 to 134 ms longer than RTs for real words. What about the predicted RTs for nonce words?",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#sec-reg-cat-pred",
    "href": "ch-regression-cat.html#sec-reg-cat-pred",
    "title": "27  Regression with categorical predictors",
    "section": "27.4 Posterior predictions",
    "text": "27.4 Posterior predictions\nThe coefficients are just telling us the difference in RT between nonce and real words, but not the predicted mean RT for nonce words. As in Chapter 25, we can simply plug in the draws from the model according to the model formulae to obtain any estimand of interest, like the posterior predictions of RTs when the word is not a real word. Remember, for nonce words (\\(w_i = 1\\)):\n\\[\n\\begin{align}\n\\mu_{\\text{F}} & = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\n\\end{align}\n\\]\nTo get the predicted RTs for nonce words we need to sum b_Intercept and b_IsWordFALSE. Since we are at it, we also create a column for real words (that is just b_Intercept so we are basically just copying it that column; remember, \\(\\mu_T = \\beta_1\\)).\n\nrt_bm_1_draws &lt;- as_draws_df(rt_bm_1) |&gt; \n  mutate(\n    real = b_Intercept,\n    nonce = b_Intercept + b_IsWordFALSE\n  )\n\nquantile2(rt_bm_1_draws$real, c(0.025, 0.975)) |&gt; round()\n\n q2.5 q97.5 \n  941   965 \n\nquantile2(rt_bm_1_draws$nonce, c(0.025, 0.975)) |&gt; round()\n\n q2.5 q97.5 \n 1057  1082 \n\n\nThe CrI for nonce is the same as the one you see in the model summary for Intercept. when the word is not a real word, the RTs are between 1057 and 1080 ms at 95% probability. Compare this with the mean RTs when the word is a real word: 95% CrI [941, 965] ms. Reaction times are longer with nonce words than with real words, as b_IsWordFALSE indicated. The conditional_effect() function from brms plots predicted values of the outcome variable depending on specified predictors. We can use it to plot the 95% CrIs (and mean) of the predicted RTs depending on word type.\n\nconditional_effects(rt_bm_1, effects = \"IsWord\")\n\n\n\n\n\n\n\n\nI always find plotting the full posterior distributions to be more informative (and I find the plots with error bars and dots to be potentially misleading, since they are just showing summaries of the posteriors). We have already the posterior draws of RTs with real words (b_Intercept; and those with nonce words, but to make plotting more straightforward we need to pivot the data.\nPivoting is the process of changing the shape of the data from a long format to a wide format and vice versa. You can find nice animations on Garrick Aden-Buie’s website that illustrate the process visually. What we need is pivoting from a wide to a long format: rt_bm_1_draws has two columns we are interested in, real and nonce, but to plot we need a column like word_type which says if the posterior prediction is for real or nonce words and a column like pred for the posterior prediction. We can achieve this with the pivot_longer() function from tidyr (another tidyverse package). You can learn more about pivoting in the package vignette Pivoting. We first select the two columns we want to pivot, real and nonce with select(), then we pivot with pivot_longer(): the function needs to know which columns to pivot and here we are saying to pivot all columns with the special tidyverse function everything() (note that this only works with certain tidyverse functions). Then we need to tell pivot_longer() what we want to name the column with the original column names and what name we want for the column with the values of the original columns. Check the result and play around with the code to understand how pivoting works.\n\nrt_bm_1_long &lt;- rt_bm_1_draws |&gt; \n  select(real, nonce) |&gt; \n  pivot_longer(everything(), names_to = \"word_type\", values_to = \"pred\")\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\nrt_bm_1_long\n\n\n  \n\n\n\nNow we can plot the data with ggplot and ggdist’s stat_halfeye(). We are setting the error bars to show the 50% and 95% CrI (.width = c(0.5, 0.95)). I have also added code on the last line to manually set the “breaks” of the x-axis using the breaks argument in scale_x_continuous().\n\nrt_bm_1_long |&gt; \n  ggplot(aes(pred, word_type)) +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  scale_x_continuous(breaks = seq(920, 1100, by = 20)) +\n  labs(\n    x = \"Predicted RTs (ms)\", y = \"Word type\"\n  )\n\n\n\n\n\n\n\nFigure 27.5: Posterior predictions of RT by word type.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#reporting",
    "href": "ch-regression-cat.html#reporting",
    "title": "27  Regression with categorical predictors",
    "section": "27.5 Reporting",
    "text": "27.5 Reporting\nReporting regression models with categorical predictors is not too different from reporting models with a numeric predictor, but there are a couple of things that you need to keep in mind. With a categorical predictor you should report the estimates for the intercept, the difference from the intercept and the predicted value for the second level in the categorical predictor. You should also tell the reader how the categorical predictor was coded. Here is the full report for our model of reaction times.\n\nWe fitted a Bayesian regression model using the brms package (Bürkner 2017) in R (R Core Team 2025). We used a Gaussian distribution for the outcome variable, reaction times (in milliseconds). We included word type (real vs nonce word) as the regression predictor. Word type was coded using the default R treatment contrasts, with real word set as the reference level.\nBased on the model results, there is a 95% probability that the mean RT with real words is between 941 and 965 ms (mean = 953, SD = 6). When the word is a nonce word, the RTs are between 1057 and 1082 ms at 95% confidence (mean = 1069, SD = 6). When comparing RTs for nonce vs real words, there is an 95% probability that the difference is between 99 to 134 ms (mean = 116, SD = 9). The residual standard deviation is between 306 and 319 ms (mean = 312, SD = 3).",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#conclusion",
    "href": "ch-regression-cat.html#conclusion",
    "title": "27  Regression with categorical predictors",
    "section": "27.6 Conclusion",
    "text": "27.6 Conclusion\nTo summarise, the model suggests that RTs with nonce words are 99-134 ms longer than RTs with real words. Now, is a difference of 99-134 ms a linguistically meaningful one? Statistics cannot help with that: only a well developed mathematical model of lexical retrieval and related neurocognitive processes would allow us to make any statement regarding the linguistic relevance of a particular result. This aspect applies to any statistical approach, whether frequentist or Bayesian. Within a frequentist framework (which you will learn about in Chapter 29), “statistical significance” is not “linguistic significance”. A difference between two groups can be “statistically significant” and not be linguistically meaningful and vice versa. Within the Bayesian framework, getting posterior distributions that are very certain (like the ones we have gotten so far) doesn’t necessarily mean that we have a better understanding of the processes behind the phenomenon we are investigating. After having obtained estimates from a model, always ask yourself: what do those estimates mean, based on our current understanding of the linguistic phenomenon under investigation?",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#summary",
    "href": "ch-regression-cat.html#summary",
    "title": "27  Regression with categorical predictors",
    "section": "27.7 Summary",
    "text": "27.7 Summary\n\n\n\n\n\n\n\nCategorical predictors can be included in regression models by coding them as numbers.\nThe most common coding system uses treatment contrasts. The reference level is coded as 0 and the other level is coded as 1.\nThe intercept is the mean outcome for the reference level of the categorical predictor. The slope is the difference in mean outcome between the second level and the reference.\n\n\n\n\n\n\n\n\nBürkner, Paul-Christian. 2017. “Brms: An r Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 128. https://doi.org/10.18637/jss.v080.i01.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing [Version 4.5.0].\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression with categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html",
    "href": "ch-regression-more.html",
    "title": "28  More than two levels",
    "section": "",
    "text": "28.1 Mixean Basque VOT\nChapter 27 showed you how to fit a regression model with a categorical predictor. We modelled reaction times as a function of word type (real or nonce) from the MALD dataset (Tucker et al. 2019). The categorical predictor IsWord (word type: real or nonce) was included in the model using treatment contrasts: the model’s intercept is the mean of the first level and the “slope” is the difference between the second level and the first. In this chapter we will look at new data, Voice Onset Time (VOT) of Mixean Basque (Egurtzegi and Carignan 2020), to illustrate how to fit and interpret a regression model with a categorical predictor that has three levels, phonation (voiced, voiceless unaspirated, aspirated).\nThe data egurtzegi2020/eu_vot.csv contains measurements of VOT from 10 speakers of Mixean Basque (Egurtzegi and Carignan 2020). Mixean Basque contrasts voiceless unaspirated, voiceless aspirated and voiced stops. Let’s read the data.\nlibrary(tidyverse)\n\neu_vot &lt;- read_csv(\"data/egurtzegi2020/eu_vot.csv\")\neu_vot\nBased on our general knowledge of VOT, the VOT should increase from voiced to voiceless unaspirated to voiceless aspirated stops. We can use a Gaussian regression model to assess whether our expectations are compatible with the data. The eu_vot data has a voicing column that tells only if the stop is voiceless or voiced, but we need a column that further differentiates between unaspirated and aspirated voiceless stops. We can create a new column, phonation depending on the phone, using the case_when() function inside mutate().\ncase_when() works like an extended ifelse() function: while ifelse() is restricted to two conditions (i.e. when something is TRUE or FALSE), chase_when() allows you to specify many conditions. The general syntax for the conditions in case_when() is condition ~ replacement where condition is a matching statement and replacement is the value that should be returned when there is a match. In the following code, we use case_when() to match specific phones in the phone column and based on that we return voiceless, voiced or aspirated. These values are saved in the new column phonation. We also convert the VOT values from seconds to milliseconds by multiply the VOT by 1000 in a new column VOT_ms.\neu_vot &lt;- eu_vot |&gt; \n  mutate(\n    phonation = case_when(\n      phone %in% c(\"p\", \"t\", \"k\") ~ \"voiceless\",\n      phone %in% c(\"b\", \"d\", \"g\") ~ \"voiced\",\n      phone %in% c(\"ph\", \"th\", \"kh\") ~ \"aspirated\"\n    ),\n    # convert to milliseconds\n    VOT_ms = VOT * 1000\n  )\nFigure 28.1 shows the densities of the VOT values for voiced, voiceless (unaspirated) and (voiceless) aspirated stops separately. Do the densities match our expectations about VOT?\nCode\neu_vot |&gt; \n  drop_na(phonation) |&gt; \n  ggplot(aes(VOT_ms, fill = phonation)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\nFigure 28.1",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#mixean-basque-vot",
    "href": "ch-regression-more.html#mixean-basque-vot",
    "title": "28  More than two levels",
    "section": "",
    "text": "WarningExercise 1\n\n\n\nRecreate the following plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nThe fill legend is not really needed, since the x-axis already separates the different phonations types, but different fill colours can help with the overall legibility of the violins since they highlight the area covered by the violin shape.\nTo remove the legend, you should use the theme() function. Check the documentation of ?theme and search online for the argument value that hides the legend.\n\n\n\n\n\n\n\n\n\nImportantSolution\n\n\n\n\n\nHave you tried legend.position in theme()?\n\n\nShow me\n\n\n\neu_vot |&gt; \n  drop_na(phonation) |&gt; \n  ggplot(aes(phonation, VOT_ms, fill = phonation)) +\n  geom_jitter(alpha = 0.1, width = 0.2) +\n  geom_violin(alpha = 0.8, width = 0.2) +\n  labs(x = \"Phonation\", y = \"VOT (ms)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nWarningExercise 2\n\n\n\nCalculate appropriate measures of central tendency and dispersion of VOT depending on the phonation type.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#treatment-contrasts-with-three-levels",
    "href": "ch-regression-more.html#treatment-contrasts-with-three-levels",
    "title": "28  More than two levels",
    "section": "28.2 Treatment contrasts with three levels",
    "text": "28.2 Treatment contrasts with three levels\nLet’s proceed with modelling VOT. We will assume that VOT values follow a Gaussian distribution: as with reaction times, this is just a pedagogical step for you to get familiar with fitting models with categorical predictors, but other distribution families might be more appropriate. While for reaction times there are some recommended distributions (which you will learn about in ?sec-lognorm), there are really no recommendations for VOT, so a Gaussian distribution will have to do for now.\nBefore fitting the model, it is important to go through the model’s mathematical formula and to pay particular attention to how phonation type is coded using treatment contrasts. The phonation predictor has three levels: aspirated, voiced and voiceless. The order of the levels follows the alphabetical order. You will remember from Chapter 27 that the mean of the first level of a categorical predictor ends up being the intercept of the model while the difference of the second level relative to the first is the slope. With a third level, the model estimates another “slope”, which is the difference between the third level and the first. With treatment contrasts, the second and higher levels of a categorical predictor are compared (or contrasted) with the first level. With the default alphabetical order, this means that the intercept of the model will tell us the mean VOT of aspirated stops, and the mean of voiced and voiceless stops will be compared to that of aspirated stops.\nAn other important aspect of treatment coding of categorical predictors that we haven’t discussed is the number of indicator variables needed: the number of indicator variables is always the number of the levels of the predictor minus one (\\(N - 1\\), where \\(N\\) is the number of levels). It follows that a predictor with three levels needs two indicator variables (\\(N = 3\\), \\(3 - 1 = 2\\)). This is illustrated in Table 28.1. For each observation, \\(ph_\\text{VD}\\) indicates if the observation is from a voiced stop (1) or not (0), and \\(ph_\\text{VL}\\) indicates if the observation is from a voiceless (unaspirated) stop (1) or not (0). Of course, if the observation if from an aspirated stop, that is neither a voiced nor a voiceless (unaspirated) stop, so both \\(ph_\\text{VD}\\) and \\(ph_\\text{VL}\\) are 0.\n\n\n\nTable 28.1: Treatment contrasts coding of the categorical predictor phonation.\n\n\n\n\n\nphonation\n\\(ph_\\text{VD}\\)\n\\(ph_\\text{VL}\\)\n\n\n\n\nphonation = aspirated\n0\n0\n\n\nphonation = voiced\n1\n0\n\n\nphonation = voiceless\n0\n1\n\n\n\n\n\n\nNow that we know how phonation is coded, we can look at the model formula.\n\\[\n\\begin{align}\n\\text{VOT}_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot ph_{\\text{VD}[i]} + \\beta_2 \\cdot ph_{\\text{VL}[i]}\\\\\n\\end{align}\n\\]\nThe formula states that each observation of VOT come from a Gaussian distribution with a mean and standard deviation and that the mean depends on the value of the indicator variables \\(ph_\\text{VD}\\) and \\(ph_\\text{VL}\\): that is what the subscript \\(i\\) is for.\n\n\n\n\n\n\nWarningExercise 3\n\n\n\nWork out the formula of the mean VOT for each level of phonation by substituting the correct value for \\(ph_\\text{VD}\\) and \\(ph_\\text{VL}\\).\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nFor example, for phonation = aspirated:\n\\[\n\\begin{align}\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot ph_{\\text{VD}[i]} + \\beta_2 \\cdot ph_{\\text{VL}[i]}\\\\\n& = \\beta_0 + \\beta_1 \\cdot 0 + \\beta_2 \\cdot 0\\\\\n& = \\beta_0\n\\end{align}\n\\]\nSo the mean VOT with aspirated stops is \\(\\beta_0\\).\n\n\n\nThe code below fits a Gaussian regression model, with VOT (in milliseconds) as the outcome variable and phonation type as the (categorical) predictor. Phonation type (phonation) is coded with treatment contrasts. Before fitting the model, answer the question in Quiz 1 below.\n\nlibrary(brms)\n\nvot_bm &lt;- brm(\n  VOT_ms ~ phonation,\n  family = gaussian,\n  data = eu_vot,\n  seed = 6725,\n  file = \"cache/ch-regression-more-vot_bm\"\n)\n\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nHow many regression coefficients are there in the model above?\n\n 2 3 4\n\n\n\nWhen you run the model you will see this message: Warning: Rows containing NAs were excluded from the model.. This is really nothing to worry about: it just warns you that rows that have NAs were dropped before fitting the model. Of course, you could also drop them yourself in the data and feed the filtered data to the model. This is probably a better practice because it gives you the opportunity to explicitly find out which rows have NAs (and why).\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\nBased on the density plots of VOT you made above, which of the following sets of expectations makes sense?\n\n \\(\\beta_0, \\beta_1, \\beta_2\\) should have negative values. \\(\\beta_0\\) should have positive values and \\(\\beta_1, \\beta_2\\) should have negative values. \\(\\beta_0, \\beta_1\\) should have positive values, \\(\\beta_2\\) should have negative values.\n\n\n\n\n\n\n\n\n\nImportantSolution\n\n\n\n\n\nYou can just check the summary of the model. Does the summary meet your expectations?\nsummary(vot_bm)\n\n\n\nCarefully look through the Regression Coefficients of the model and make sure you understand what each row corresponds to. It should be clear by now that while the first coefficient, the “intercept”, is the mean VOT with aspirated stops, the second and third coefficients are the difference between the mean VOT of aspirated stops and the mean VOT of voiced and voiceless stops respectively. This falls out of the formulae you worked out in Exercise 3.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#posterior-predictions",
    "href": "ch-regression-more.html#posterior-predictions",
    "title": "28  More than two levels",
    "section": "28.3 Posterior predictions",
    "text": "28.3 Posterior predictions\nWe can obtain the posterior predictions for the VOT of voiced and voiceless stops as we did in Section 27.4. If you have completed Exercise 3 above, the following code should have no surprises.\n\nvot_bm_draws &lt;- as_draws_df(vot_bm) |&gt; \n  mutate(\n    aspirated = b_Intercept,\n    voiced = b_Intercept + b_phonationvoiced,\n    voiceless = b_Intercept + b_phonationvoiceless\n  )\n\nLet’s also pivot the draws to a longer format with pivot_longer(). Ignore the warning about dropping the draws_df class.\n\nvot_bm_long &lt;- vot_bm_draws |&gt; \n  select(aspirated:voiceless) |&gt; \n  pivot_longer(everything(), names_to = \"phonation\", values_to = \"pred\")\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\nvot_bm_long\n\n\n  \n\n\n\nThis time I will show you how to use the long draws tibble to create a summary table. This is what the following code does: the only new bit is the paste(..., collapse = \", \") part. This is needed because quantile2() returns a vector with two elements (one with the lower limit and one with the upper limit of the CrI) but we want to collapse that into a single string, with the two limits separated by a comma and space. The resulting string is wrapped between square brackets, a typical format for reporting CrIs.\n\nvot_bm_tab &lt;- vot_bm_long |&gt; \n  group_by(phonation) |&gt; \n  summarise(\n    mean = mean(pred), sd = sd(pred),\n    `99%` = paste0(\"[\", paste(quantile2(pred, c(0.005, 0.995)) |&gt; round(), collapse = \", \"), \"]\"),\n    `60%` = paste0(\"[\", paste(quantile2(pred, c(0.2, 0.8)) |&gt; round(), collapse = \", \"), \"]\")\n  )\n\nvot_bm_tab\n\n\n  \n\n\n\nThe function kable() from the knitr package provides us with a convenient way to output the table in vot_bm_tab as a nicely formatted table in a rendered Quarto file. Table 28.2 is indeed the output of R code using knitr::kable(). Unfold the code to see it (click on the little triangle to the left of Code below). Check the documentation of kable() to learn about its arguments.\n\n\nCode\nvot_bm_tab |&gt; \n  knitr::kable(\n    col.names = c(\"\", \"Mean\", \"SD\", \"99% CrI\", \"60% CrI\"),\n    digits = 1, align = c(\"rcccc\")\n  )\n\n\n\n\nTable 28.2: Posterior summaries from a Bayesian regression model of VOT.\n\n\n\n\n\n\n\nMean\nSD\n99% CrI\n60% CrI\n\n\n\n\naspirated\n32.6\n1.5\n[29, 37]\n[31, 34]\n\n\nvoiced\n-44.4\n1.1\n[-47, -42]\n[-45, -43]\n\n\nvoiceless\n19.8\n0.5\n[19, 21]\n[19, 20]\n\n\n\n\n\n\n\n\nIn Table 28.2, I reported the posterior mean, SD and 99% and 60% CrIs of the posterior distributions of the predicted VOT of aspirated, voiced, and voiceless stops. Why 99% and 60%? There is nothing special with 95% CrIs and as mentioned in previous chapters you should report multiple levels. I admit that in this case, as in the models from previous chapters reporting more than one CrI is overkill, because our posteriors are so certain. So take this as just an example of how you could report results in a table. Probably, a 99% CrI would have sufficed.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#reporting",
    "href": "ch-regression-more.html#reporting",
    "title": "28  More than two levels",
    "section": "28.4 Reporting",
    "text": "28.4 Reporting\nThe following paragraph shows you how you could write up the model and results in a paper.\n\nWe fitted a Bayesian regression model to Voice Onset Time (VOT) of Mixean Basque stops. We used a Gaussian distribution for the outcome and phonation (aspirated, voiced, voiceless) as the only predictor. Phonation was coded using the default treatment coding.\nAccording to the model, the mean VOT of aspirated stops is between 29 and 37 ms, of voiced stops is between -47 and -42 ms, of voiceless stops is between 19 and 21 ms, at 99% probability. Table 28.2 reports mean, SD, 99% and 60% CrIs. When comparing voiced and voiceless stops to aspirated stops, at 99% confidence, the VOT of voiced stops is 72-82 ms shorter than that of aspirated stops (mean = -77, SD = 1.86), while the VOT of voiceless stops is 9-17 ms shorter than that of aspirated stops (mean = -13, SD = 1.58).\n\nYou might find that certain authors use \\(\\beta\\) instead of “mean” for the posterior mean reported between parentheses: for example, (\\(\\beta\\) = 33, SD = 1.5). The \\(\\beta\\) stands for \\(\\hat{\\beta}\\), which is a notation to indicate that the \\(\\beta\\) is estimated (that’s what the little hat on top of the letter signifies). This practice is probably more common in the frequentist approach than the Bayesian approach to inference. I find it more straightforward to say “mean” since that is what the estimand is: the mean of the posterior distribution of the coefficient. Similarly, some might use “SE” for standard error instead of SD. As long as one is consistent, it doesn’t matter much since there are indeed different traditions (and even different fields within linguistics might prefer different ways of reporting).\nNote that readers not used to Bayesian statistics might find the choice of CrI levels questionable or at least surprising. As said many times, there is nothing special about 95% CrI: it is a misguided historical accident from frequentist approaches to default to 95%. The next chapter is dedicated precisely to frequentist statistics and how this approach is misused in research. Most of the current research in linguistics is conducted using problematic frequentist methods, so the only thing you can do is learn where the problems lie and do your best to avoid them in your own research.\n\n\n\n\nEgurtzegi, Ander, and Christopher Carignan. 2020. “An Acoustic Description of Mixean Basque.” The Journal of the Acoustical Society of America 147 (4): 27912802. https://doi.org/10.1121/10.0000996.\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html",
    "href": "ch-null-ritual.html",
    "title": "29  Frequentist statistics, the Null Ritual and p-values",
    "section": "",
    "text": "29.1 Frequentist statistics, feuds and eugenics: a brief history\nIn Chapter 20, you were introduced to the Bayesian approach to inference, and we only briefly mentioned frequentist statistics. It is very likely that you had heard of p-values before, especially when reading research literature. You will have also heard of “statistical significance”, which is based on the p-value obtained in a statistical test performed on data. This section will explain what p-values are, how they are often misinterpreted and misused (Cassidy et al. 2019; Gigerenzer 2004), and how the “Null Ritual”, a degenerate form of statistics derived from different frequentist frameworks (yes, you can do frequentist statistics in different ways!) that has become the standard in research, despite not being a coherent way of doing frequentist statistics (Gigerenzer, Krauss, and Vitouch 2004).\nA lot of current research is carried out with frequentist methods. This is a historical accident, based on both an initial misunderstanding of Bayesian statistics (which is, by the way, older than frequentist statistics) and the fact that frequentist maths was much easier to deal with (and personal computers did not exist at the time). Bayesian statistics, rooted in Thomas Bayes‘s (1701-1771) work on updating probabilities with new evidence based on a specific interpretation of Bayes’ Theorem, remained largely dormant until the 20th century due to computational difficulties and philosophical debates. It gained widespread traction in the 1990s with advances in computational methods, especially the development of Markov Chain Monte Carlo ( Chapter 25). However, modern frequentist statistics is attributed to three statisticians whose lives span several decades before the time Bayesianism found a place in statistical debates: Ronald Fisher (1890-1926), Jerzy Neyman (1894-1981) and Egon Pearson (1895-1980, the son of another statistician, Karl Pearson 1857-1936).\nThe history of frequentist statistics is a tale of heated debates, feuds and the now discredited field of eugenics (a colonial movement with the aim of improving the genetic “quality” of human populations). Francis Galton (1822-1911) is considered the originator of eugenics: this is the same Galton who came up with the idea of “regression toward mediocrity” which gives the name to regression models (see Spotlight box in Chapter 23). Karl Pearson was deeply influenced by Galton and saw statistics as a tool for improving racial fitness. Pearson’s Biometric School in Britain was explicitly tied to eugenics research. Ronald Fisher was another British statistician who strongly believed in eugenics and racial differences. While both K. Pearson and Fisher were proponents of eugenics, they differed in their understanding of statistics. Fisher criticised K. Pearson’s approach as too mechanic and simplistic (thus earning the elder statistician’s lasting disapproval). Fisher came up with the concept of statistical significance and devised the famous p-value as a way to quantify statistical significance based on data, against a hypothesis (to be nullified) of how the researcher thought the data were produced.\nFisher’s contemporaries Jerzy Neyman and Egon Pearson (the son of K. Pearson), who both rejected eugenics, found Fisher’s critiques to K. Pearson’s (the father) approach well-founded, but they themselves thought Fisher’s significance testing fell short. While Fisher’s significance testing was based on quantifying significance in light of a single hypothesis, Neyman and E. Pearson (the son) argued that one should contrast two opposing hypotheses and control for error rates in rejecting one and accepting the other. They thus introduced the idea of “significance level”, a threshold which determined if one accepted a result as statistical significant or not. In sum, while Fisher’s approach was focused on estimating the degree of statistical significance, Neyman and Pearson’s approach was more interested in decision-making under uncertainty. “Fisherian frequentism” and “Neyman–Pearson frequentism”, as they later became to be known, are incompatible approaches, despite both being based on the same statistical concept of the p-value, because they entail two very different interpretations of statistical significance (and the objective of research more generally).",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html#null-hypothesis-significance-testing",
    "href": "ch-null-ritual.html#null-hypothesis-significance-testing",
    "title": "29  Frequentist statistics, the Null Ritual and p-values",
    "section": "29.2 Null Hypothesis Significance Testing",
    "text": "29.2 Null Hypothesis Significance Testing\nMoving forward in time to the last three decades, the commonly accepted approach to frequentist inference is the so-called Null Hypothesis Significance Testing, or NHST. As practised by researchers, the NHST approach is an incoherent mix of Fisherian and Neyman–Pearson frequentism (Perezgonzalez 2015). The main tenet of the NHST is that you set a null hypothesis and you try to reject it (as Fisher expected statistical significance to be used). A null hypothesis is, in practice, always a nil hypothesis: in other words, it is the hypothesis that there is no difference between two estimands (these usually being means of two or more groups of interest). This aspect is a great departure from both Fisherian and Neyman–Pearson frequentism, since neither says anything about the null hypothesis having to necessarily be a nil hypothesis. Then, using a variety of numerical techniques, one obtains a p-value, i.e. a frequentist probability. The p-value is used for inference: if the p-value is smaller than a threshold (Neyman–Pearson’s significance level), you can reject the nil hypothesis; if the p-value is equal to or greater than the threshold, you cannot reject the null hypothesis.\nThe following section explains p-values within the NHST approach, since that is the approach researchers adopt (knowingly or less knowingly) when using p-values. Note however that the NHST approach has been heavily criticised by frequentist and Bayesian statisticians alike and has resulted in the proposal of alternative, stricter, versions of NHST, like the frequentist Statistical Inference as Severe Testing (SIST, Mayo 2018; for a critique see Gelman et al. 2019). The inconsistent nature of NHST has led to the elaboration of the concept and label “Null Ritual” (Gigerenzer 2004, 2018; Gigerenzer, Krauss, and Vitouch 2004) and the slogan-titled paper The difference between “significant” and “not significant” is not itself statistically significant (Gelman and Stern 2006). p-values are very commonly mistaken for Bayesian probabilities (Cassidy et al. 2019) and this results in various misinterpretations of reported results. Section 29.4 explains the main issues with the Null Ritual and invites you to always think critically when reading results and discussions in published research.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html#the-p-value",
    "href": "ch-null-ritual.html#the-p-value",
    "title": "29  Frequentist statistics, the Null Ritual and p-values",
    "section": "29.3 The p-value",
    "text": "29.3 The p-value\nTo illustrate p-values, we will compare simulated durations of vowels when followed by voiceless consonants vs voiced consonants. It is a well-known phenomenon that vowels followed by voiced consonants tend to be longer than vowels followed by voiceless ones (see review in Coretta 2019). Let’s simulate some vowel duration observations: we do so with the rnorm() function, which takes three arguments: the number of observations to sample and the mean and standard deviation of the Gaussian distribution to sample observations from. We use a \\(Gaussian(80, 10)\\) for durations before voiceless consonants, and a \\(Gaussian(90, 10)\\) for durations before voiced consonants. In other word, there is a difference of 10 milliseconds between the two means. Since the rnorm() function randomly samples from the given distribution, we have set a “seed” so that the code will return the same numbers every time it is run, for reproducibility.\n\nset.seed(2953)\n# Vowel duration before voiceless consonants\nvdur_vls &lt;- rnorm(15, mean = 80, sd = 10)\nvdur_voi &lt;- rnorm(15, mean = 90, sd = 10)\n\nNormally, you don’t know what the underlying means are, we do here because we set them. So let’s get the sample mean of vowel duration in the two conditions, and take the difference.\n\nmean_vls &lt;- mean(vdur_vls)\nmean_voi &lt;- mean(vdur_voi)\n\ndiff &lt;- mean_voi - mean_vls\ndiff\n\n[1] 7.43991\n\n\nThe difference in the sample means is about 7.4. Now, a NHST researcher would define the following two hypotheses:\n\n\\(H_0\\): the difference between means is 0.\n\\(H1\\): the difference between means is not 0.\n\n\\(H_0\\) simply states that there is no difference between the mean of vowel duration when followed by voiced or voiceless consonants. This is the “null” hypothesis. \\(H_1\\), called the “alternative” hypothesis, states that the difference between means is different from exactly 0. We could have decided to go with “greater than 0”, rather than just “not 0”, because we know about the trend of longer vowels before voiced consonants, so the difference should be positive. But this is not how NHST is usually set up: it is always assumed that the \\(H_1\\) is “the difference between means is not 0”.\nHere is where things get tricky: if \\(H_0\\) is correct, then we should observe a difference as close to 0 as possible. Why not exactly 0? Because it is impossible for two samples (even if they come exactly from the same distribution) to have exactly the same mean for the difference to be 0. But how do we define “as close as possible”? The frequentist solution is to define a probability distribution of the difference between means centred around 0. This means that 0 has the highest probability, but that negative and positive differences around 0 are also possible.\nWilliam Sealy Gosset (1876-1937), a statistician, chemist and brewer who worked for Guinness the brewery, proposed the t-distribution for this purpose. Gosset, though employed at Guinness, was sent to University College London in 1906–1907 to study statistics under Karl Pearson (the father), who at the time was the leading authority in mathematical statistics. Guinness allowed this because Gosset needed advanced statistical tools to handle small experimental data sets in brewing and agriculture. K. Pearson’s Biometrika journal became the outlet where Gosset published his famous 1908 paper “The probable error of a mean” (Student 1908). K. Pearson encouraged Gosset to publish it, though Guinness insisted that he use a pseudonym to protect trade secrets, so Gosset published under the pseudonym “Student”. Because of this, the t-distribution is also called the Student-t distribution. Gosset argued that the t-distribution was an appropriate probability distribution for differences between means, especially with small sample sizes.\nThe t-distribution is similar to a Gaussian distribution, but the probability on either side of the mean declines more gently than with the Gaussian. As the Gaussian, the t-distribution has a mean and a standard deviation. It has an extra parameter: the degrees of freedom, or \\(df\\). The \\(df\\) affect how quickly the probability declines moving away from zero: the higher the \\(df\\) the more quickly the probability gets lower. This is illustrated in Figure 29.1. The figure shows four different t-distributions: what they all have in common is that the mean is 0 and the standard deviation is 1. These are called “standard” t-distributions. Where they differ is in their degrees of freedom. When the degrees of freedom are infinite (Inf), the t-distribution is equivalent to a Gaussian distribution.\n\n\nCode\n# Degrees of freedom to compare\ndfs &lt;- c(1, 2, 5, Inf)\n\n# Create data\ndata &lt;- tibble(df = dfs) |&gt;\n  mutate(data = map(df, ~ {\n    tibble(\n      x = seq(-4, 4, length.out = 500),\n      y = if (is.infinite(.x)) dnorm(seq(-4, 4, length.out = 500))\n          else dt(seq(-4, 4, length.out = 500), df = .x)\n    )\n  })) |&gt;\n  unnest(data)\n\n# Plot\nggplot(data, aes(x = x, y = y, color = as.character(df))) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"t-Distributions\",\n    x = \"t-statistic\", y = \"Density\",\n    color = \"DFs\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nFigure 29.1: Example t-distributions with different degrees of freedom and fixed mean and standard deviation (mean = 0, sd = 1).\n\n\n\n\n\nWhy mean 0 and standard deviation 1? Because we can standardise the difference between the two means and always use the same standard t-distribution, so that the scale of the difference doesn’t matter: we could be comparing milliseconds, or Hertz, or kilometres. To standardise the difference between two means, we calculate the t-statistic. The t-statistic is a standardised difference. Here’s the formula:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\\(t\\) is the t-statistic.\n\\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means of the first and second group (the order doesn’t really matter).\n\\(s^2_1\\) and \\(s^2_2\\) are the variance of the first and second group. The variance is simply the square of the standard deviation (expressed with \\(s\\) here).\n\\(n_1\\) and \\(n_2\\) are the number of observations for the first and second group (sample size).\n\nWe have the means of vowel duration before voiced and voiceless consonants and we know the sample size (15 observations per group), so we just need to calculate the variance.\n\nvar_vls &lt;- sd(vdur_vls)^2 # also var(vdur_vls)\nvar_voi &lt;- sd(vdur_voi)^2 # also var(vdur_voi)\n\ntstat &lt;- (mean_voi - mean_vls) / sqrt((var_voi / 15) + (var_vls / 15))\n\ntstat\n\n[1] 2.437442\n\n\nSo the t-statistic for our calculated difference is 2.4374424. Gosset introduced the t-statistics as a tool for quantile interval estimation and for comparing means, but hypothesis testing wouldn’t be “invented” until Fisher’s work on the p-value one or two decades later. Fisher took Gosset’s t-statistic and embedded it into his broader significance testing framework. Fisher popularized the use of the t-distribution for calculating p-values: the probability, under the null hypothesis, of obtaining a t-statistic as extreme or more extreme than the observed value. This re-framing changed the purpose of Gosset’s work from estimation to a frequentist test of significance.\nNow, after having obtained the t-statistic, the NHST researcher would ask: what is the probability of finding a t-statistic (and the difference in means it represents) this large or larger, assuming that the t-statistic (and the difference) is 0. This probability is Fisher’s p-value. You should note two things:\n\nFirst, the part about the real difference being 0. This is \\(H_0\\) from above, our null hypothesis that the difference is 0. For a p-value to work, we must assume that \\(H_0\\) is always true. Otherwise, the frequentist machinery does not work.\nAnother important aspect is the “difference this large or larger”: due to how probability density functions work, we cannot obtain the probability of a specific value, but only the probability of an interval of values (Chapter 19). The NHST story goes that, if \\(H_0\\) is true, you should not get very large differences, let alone larger differences than the one found.\n\nThe next step is thus to obtain the probability of \\(t \\geq\\) 2.4374424 (t being equal or greater than 2.4374424), given a standard t-distribution. Before we can do this we need to pick the degrees of freedom of the distribution, because of course these affect the probability. The degrees of freedom are calculated based on the data with the following, admittedly complex, formula:\n\\[\n\\nu = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}}\n\\]\n\n\nCode\ndf &lt;- ( (var_voi/15 + var_vls/15)^2 ) / \n      ( ((var_voi/15)^2 / (15 - 1)) + ((var_vls/15)^2 / (15 - 1)) )\n\n\nThe degrees of freedom for our data are approximately 28. Figure 29.2 shows a t-distribution with those degrees of freedom and a dashed line where our t-statistic falls. Now, since we set \\(H_1\\) to be “the difference is not 0”, we are including both positive and negative differences between the means. The sign of the t-statistic reflects the sign of the difference: positive t means a positive difference, while negative t indicates a negative difference between the two means. Since \\(H_1\\) is non-directional (it does not specify whether the difference is positive or negative), the p-value must account for extreme values of the t-statistic in both directions. This is called a two-tailed t-test: the p-value is the probability of observing a t-statistic at least as extreme as the one obtained, in either tail of the t-distribution.\n\n\nCode\n# Create data\ndata &lt;- tibble(df = df) |&gt;\n  mutate(data = map(df, ~ {\n    tibble(\n      x = seq(-4, 4, length.out = 500),\n      y = if (is.infinite(.x)) dnorm(seq(-4, 4, length.out = 500))\n          else dt(seq(-4, 4, length.out = 500), df = .x)\n    )\n  })) |&gt;\n  unnest(data)\n\n# Plot\nggplot(data, aes(x = x, y = y)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = tstat, linetype = \"dashed\") +\n  geom_vline(xintercept = -tstat, linetype = \"dotted\") +\n  geom_area(\n    data = subset(data, x &gt;= tstat),\n    aes(x = x, y = y),\n    fill = \"purple\", alpha = 0.3\n  ) +\n  geom_area(\n    data = subset(data, x &lt;= -tstat),\n    aes(x = x, y = y),\n    fill = \"purple\", alpha = 0.3\n  ) +\n  annotate(\n    \"text\", x = 3, y = 0.05, label = \"t-statistic\"\n  ) +\n  labs(\n    title = glue::glue(\"Standard t-Distribution with df = {round(df)}\"),\n    x = \"t-statistic\", y = \"Density\",\n    caption = \"The sum of the shaded areas is the p-value.\"\n  )\n\n\n\n\n\n\n\n\nFigure 29.2: Standard t-distribution and obtained t-statistic.\n\n\n\n\n\nIn Figure 29.2, the dotted line to the left of the distribution marks the t-statistic but with negative sign. The shaded purple area on both tails of the distribution marks the area under the density curve with t values as extreme or more extreme than the obtained t-statistic. The size of this area is the probability that we get a t value as extreme or more extreme than the obtained t-statistic, given that \\(H_0\\) is true. This probability is the p-value! The part “given that \\(H_0\\) is true” shows that the p-value is a conditional probability, conditional on \\(H_0\\) being true: we could write this as \\(p = P(d | H_0)\\), where the vertical bar \\(|\\) indicates that the probability of the t-statistic is conditional on \\(H_0\\) and \\(d\\) stands for “data”, or more precisely for “data as extreme or more extreme than the one observed”. You have already encountered conditional probabilities in Chapter 20, in Bayes’ Theorem.\nYou can get the p-value in R using the pt() function. You need the t-value and the degrees of freedom. These are saved in tstat and df from previous code. We also need to set another argument, lower.tail: this argument, when set to TRUE, states that we want the probability of getting a t value that is equal or less than the specified t value, but we want the probability of getting a t value that is equal or greater than the specified t value, since the t-value is positive, so we set lower.tail to FALSE. Since this is a two-tailed t-test, we also need the probability for the negative tail. Since the distribution is symmetric around 0, the upper and lower-tail probabilities given a t-statistic are the same, so we can simply multiply the upper-tail probability by 2.\n\n# pt() times 2 to get the two-tailed prob\npvalue &lt;- pt(tstat, df, lower.tail = FALSE) * 2\npvalue\n\n[1] 0.02150441\n\n\nIn other words, assuming \\(H_0\\) is true and there is not difference between the two groups of vowel duration, the probability of obtaining a t-statistic as extreme or more extreme than \\(\\pm\\) 2.44 is approximately 0.02. In other words, there is approximately a 2% probability that the difference between durations of vowels followed by voiced or voiceless consonants is \\(\\pm\\) 7 ms or larger. Of course, we want the p-value to be as small as possible: if \\(H_0\\) is true and the true difference is 0, finding a large difference should be very unlikely (think about the t-distribution: values away from 0 are less likely than 0 and values closer to 0). This was the original formulation of Fisher’s statistical testing: the p value could be taken as the degree of significance. However, Neyman and Pearson argued that a threshold should be decided and that a binary decision regarding significance should be taken.\nHow do we choose how small a p-value is small enough? Is 1% small enough? What about 0.5%? 5%?, maybe 10%? This is what the so-called \\(\\alpha\\)-level is for (read “alpha level”, from the Greek letter \\(\\alpha\\)). We will get back to the issue of setting an \\(\\alpha\\)-level in the next section, but for now know that in social research it has become standard to set it to 0.05. In other words, if the p-value is lower than \\(\\alpha = 0.05\\) then we take the p-value to be small enough, otherwise we don’t. When the p-value is smaller than 0.05, we say we found a statistically significant difference, when it is equal or greater than 0.05, we say we found a statistically non-significant difference. When we find a statistical significant difference, the NHST story goes, we say that we reject the null hypothesis \\(H_0\\). In our simulated example of vowel duration, the p-value is smaller than 0.05, so we say the mean vowel durations before voiced vs voiceless consonants are (statistically) significantly different from each other.\n\n\n\n\n\n\nTipp-value\n\n\n\nA p-value is the probability of obtaining a result as extreme or more extreme than the one obtained, assuming the null hypothesis is true.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html#sec-null-ritual",
    "href": "ch-null-ritual.html#sec-null-ritual",
    "title": "29  Frequentist statistics, the Null Ritual and p-values",
    "section": "29.4 The Null Ritual",
    "text": "29.4 The Null Ritual\nGigerenzer (2004) calls the way researchers perform NHST the “null ritual”. He defines the null ritual as the following procedure (Gigerenzer 2004, 588):\n\nSet up a statistical null hypothesis of “no mean difference” or “zero correlation.” Don’t specify the predictions of your research hypothesis or of any alternative substantive hypotheses.\nUse 5% as a convention for rejecting the null. If signiﬁcant, accept your research hypothesis. Report the result as p &lt; 0.05, p &lt; 0.01, or p &lt; 0.001 (whichever comes next to the obtained p-value).\nAlways perform this procedure.\n\nGigerenzer (2004) explains how the null ritualistic approach to frequentism, or NHST, is an inconsistent hybrid of Fisher’s statistical significance testing and Neyman–Pearson decision theory. Fisherian frequentism did not have an \\(\\alpha\\)-level: the p-value was used as a degree of statistical significance. Neyman and Pearson rejected the idea of significance degree and argued for the use an \\(\\alpha\\)-level, but they in no way proposed a fixed level and, quite the opposite, urged researchers to set the \\(\\alpha\\)-level on a case-by-case basis. Moreover, Fisher’s null hypothesis didn’t have to be a nil hypothesis: you could set your null hypothesis (the hypothesis to be “nullified”) to anything it made sense, so you could have for example \\(H_0: \\delta = +2.5\\) (where \\(\\delta\\) “delta” is the difference between means) and the p-value would be about nullifying that hypothesis, not \\(\\delta = 0\\).\nThere is another level that is relevant to Neyman–Pearson statistics that is very often glossed over: the \\(\\beta\\)-level, also known as statistical power. Statistical power is the probability of finding a significant difference when there is a real difference. In social sciences, this is arbitrarily set to 0.8: i.e., there should be an 80% probability of finding a significance difference where there is one. Statistical power is in large part determined by the magnitude of the difference and the variance of the groups being compared. With small and very noisy differences, you need a larger sample size to reach a high statistical power. As with the \\(\\alpha\\)-level, a researcher should set the \\(\\beta\\)-level on a case-by-case basis. Once a statistical power level is chose, a researcher is supposed to run a prospective power analysis (Brysbaert and Stevens 2018; Brysbaert 2020): this is a procedure that, given the chosen statistical power and hypotehsised difference magnitude and variance, helps you determine a specific sample size needed to reach that statistical power. Calculating p-values without prospective power analysis is incorrect and yet has become the norm.\nFurthermore, researchers consistently misinterpret p-values and frequentist inference more generally (Gigerenzer, Krauss, and Vitouch 2004; Gigerenzer 2018; Perezgonzalez 2015; Cassidy et al. 2019). There is a very common tendency to confuse the p-value for the probability that the null hypothesis \\(H_0\\) is true. This is incorrect because the p-value is the probability \\(P(d|H_0)\\): it is conditional on \\(H_0\\) being true and it is clearly not \\(P(H_0)\\). Another misinterpretation takes the p-value as the inverse of the probability that \\(H_1\\) is true: again, this must be false because \\(P(d|H_0)\\) is the probability of the “data” given \\(H_0\\), not the probability of \\(H_0\\) (in which case, assuming contrasting hypotheses, then we would indeed have \\(P(H_1) = -P(H_0)\\)). Finally, something dubbed “Bayesian wishful thinking” by Gigerenzer (2018), is the belief that the p-value is the probability of \\(H_0\\) given the data (\\(P(H_0|d)\\)) or worse the inverse of the probability of \\(H_1\\) given the data (\\(P(H_1|d)\\)). You might see why it is called Bayesian wishful thinking: a Bayesian posterior probability, as per Bayes’ Theorem, is precisely the probability of a hypothesis given the data: \\(P(h|d)\\) (Chapter 20). However, a p-value is not \\(P(h|d)\\) but rather \\(P(d|H_0)\\).\nThese are just the main issues and misinterpretation of the null ritual NHST and if you’d like to learn more about this, I strongly recommend you to read Gigerenzer (2004) and the other papers cited in this section. Gigerenzer, Krauss, and Vitouch (2004) highlights how the null ritual can indeed hurt research and anything that comes from it. Alas, as said at the beginning of this chapter, virtually all contemporary research (especially in the social sciences and hence linguistics) adopts the null ritual for statistical inference. This means, in practice, that we should always be very sceptical of statements regarding statistical significance and or strength of evidence when reading such literature and we should instead focus more on the actual estimates of the estimands of interest.\nMany students (and supervisors) worry that not learning how to run many different frequentist/null-ritualistic statistical tests and regression models could make it more difficult for you to understand previous literature, precisely because that is what most literature uses. This is in fact a unnecessary worry: all frequentist tests are just ways to obtain a p-value to test statistical significance and they tell you nothing else about the magnitude or importance of an estimate; frequentist regression models function on the same premise of using the equation of a line to estimate coefficients we’ve been discussing in the regression chapters, so that the structure of model coefficients and parameters is just the same. It is the interpretation that is different: in Bayesian regression models, you get a full posterior probability distribution for each parameter, while in frequentist regressions you only get a point-estimate (an estimate that is a single value). In other words, once you learn the basics of Bayesian regression models, you can still interpret frequentist/null-ritualistic regression models while avoiding the common pitfalls reviewed in this chapter.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html#why-prefer-bayesian-inference",
    "href": "ch-null-ritual.html#why-prefer-bayesian-inference",
    "title": "29  Frequentist statistics, the Null Ritual and p-values",
    "section": "29.5 Why prefer Bayesian inference?",
    "text": "29.5 Why prefer Bayesian inference?\nNow that you have a better understanding of frequentist statistics and more precisely the null ritualistic version of frequentism (or NHST) as practised by researchers, here are a few practical and conceptual reasons for why Bayesian statistics might be more appropriate in most research contexts.\n\n29.5.1 Practical reasons\n\nFitting frequentist models can lead to anti-conservative p-values (i.e. increased false positive rates, called Type-I error rates: there is no effect but yet you get a significant p-value). An interesting example of this for the non-technically inclined reader can be found in Dobreva (2024). Frequentist regression models fitted with lm()/lmer() tend to be more sensitive to small sample sizes than Bayesian models (with small sample sizes, Bayesian models return estimates with greater uncertainty, which is a more conservative approach).\nWhile very simple models will return very similar estimates whether they are frequentist or Bayesian, in most cases more complex models won’t converge if run with frequentist packages like lme4, especially without adequate enough sample sizes. Bayesian regression models always converge, while frequentist ones don’t always do.\nFrequentist regression models require as much work as Bayesian ones, although it is common practice to skip necessary steps when fitting the former, which gives the impression of it being a quicker process. Factoring out the time needed to run Markov Chain Monte Carlo chains in Bayesian regressions, in frequentist regressions you still have to perform robust perspective power analyses and post-hoc model checks.\nWith Bayesian models, you can reuse posterior distributions from previous work and include that knowledge as priors into your Bayesian analysis. This feature effectively speeds up the discovery process (getting to the real value estimate of interest faster). You can embed previous knowledge in Bayesian models while you can’t in frequentist ones.\n\n\n\n29.5.2 Conceptual reasons\n\nFrequentist regression models cannot provide evidence for a difference between groups, only evidence to reject the null (i.e. nil) hypothesis.\nA frequentist Confidence Interval (CI) like a 95% CI can only tell us that, if we run the same study multiple times, 95% of the time the CI will include the real value (but we don’t know whether the CI we got in our study is one from the 5% percent of CIs that do not contain the real value). On the other hand, a 95% Bayesian Credible Interval (CrI) always tells us that the real value is within a certain range, conditional on model and data. So, frequentist models really just give you a point estimate, while Bayesian models give you a range of values and their probability.\nWith Bayesian regressions you can compare any hypothesis, not just null vs alternative. (Although you can use information criteria with frequentist models to compare any set of hypotheses).\nFrequentist regression models are based on an imaginary set of experiments that you never actually carry out.\nBayesian regression models will converge towards the true value in the long run. Frequentist models do not.\n\nOf course, there are merits in fitting frequentist models, for example in corporate decisions, but you’ll still have to do a lot of work. The main conceptual difference then is that frequentist and Bayesian regression models answer very different questions and as (knowledge-oriented) researchers we are generally interested in questions that the latter can answer and the former cannot.\n\n\n\n\nBrysbaert, Marc. 2020. “Power Considerations in Bilingualism Research: Time to Step up Our Game.” Bilingualism: Language and Cognition 24 (5): 813818. https://doi.org/10.1017/s1366728920000437.\n\n\nBrysbaert, Marc, and Michaël Stevens. 2018. “Power Analysis and Effect Size in Mixed Effects Models: A Tutorial.” Journal of Cognition 1 (1). https://doi.org/10.5334/joc.10.\n\n\nCassidy, Scott A., Ralitza Dimova, Benjamin Giguère, Jeffrey R. Spence, and David J. Stanley. 2019. “Failing Grade: 89 Per-Cent of Introduction to Psychology Textbooks That Define/Explain Statistical Significance Do so Incorrectly.” Advances in Methods and Practices in Psychological Science. https://doi.org/10.1177/2515245919858072.\n\n\nCoretta, Stefano. 2019. “An Exploratory Study of Voicing-Related Differences in Vowel Duration as Compensatory Temporal Adjustment in Italian and Polish.” Glossa: A Journal of General Linguistics 4 (1): 1–25. https://doi.org/10.5334/gjgl.869.\n\n\nDobreva, Simona. 2024. “Bayesian Vs Frequentist Approach: Same Data, Opposite Results.” https://365datascience.com/trending/bayesian-vs-frequentist-approach/.\n\n\nGelman, Andrew, Daniel Lakeland, Brian Haig, Christian Hennig, Art Owen, Robert Cousins, Stan Young, et al. 2019. “Many Perspectives on Deborah Mayo’s “Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars”.” https://doi.org/10.48550/arXiv.1905.08876.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between “Significant” and “Not Significant” Is Not Itself Statistically Significant.” The American Statistician 60 (4): 328331. https://doi.org/10.1198/000313006X152649.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The Journal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\n———. 2018. “Statistical Rituals: The Replication Delusion and How We Got There.” Advances in Methods and Practices in Psychological Science 1 (2): 198218. https://doi.org/10.1177/2515245918771329.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual. What You Always Wanted to Know about Significance Testing but Were Afraid to Ask.” In, 391408.\n\n\nMayo, Deborah G. 2018. Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781107286184.\n\n\nPerezgonzalez, Jose D. 2015. “Fisher, Neyman-Pearson or NHST? A Tutorial for Teaching Data Testing.” Frontiers in Psychology 6 (223). https://doi.org/10.3389/fpsyg.2015.00223.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1. https://doi.org/10.2307/2331554.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html",
    "href": "ch-regression-bernoulli.html",
    "title": "30  Binary outcomes",
    "section": "",
    "text": "30.1 Probability and log-odds\nBinary outcome variables are very common in linguistics. These are categorical variable that have two levels, e.g.:\nSo far you have been fitting regression models in which the outcome variable was numeric and continuous. However, a lot of studies use binary outcome variables and it thus important to learn how to deal with those. This is what this chapter is about.\nWhen modelling binary outcomes, what the researcher is usually interested in is the probability of obtaining one of the two levels. For example, in a lexical decision task one might want to know the probability that real words were recognised as such (in other words, we are interested in accuracy: incorrect or correct response). Let’s say there is an 80% probability of responding correctly. So (\\(p()\\) stands for “probability of”):\n\\[\n\\begin{align}\np(\\text{correct}) & = 0.8\\\\\np(\\text{incorrect}) & = 1 - p(\\text{correct}) = 0.2\n\\end{align}\n\\]\nYou see that if you know the probability of one level (correct) you automatically know the probability of the other level, since there are only two levels and the total probability has to sum to 1. The distribution family for binary probabilities is the Bernoulli family. The Bernoulli family has only one parameter, \\(p\\), which is the probability of obtaining one of the two levels (one can pick which level). With our lexical decision task example, we can write:\n\\[\n\\begin{align}\nresp_{\\text{correct}} & \\sim Bernoulli(p) \\\\\np & = 0.8\n\\end{align}\n\\]\nYou can read it as:\nIf you randomly sampled from \\(Bernoulli(0.8)\\) you would get “correct” 80% of the times and “incorrect” 20% of the times. We can test this in R. In previous chapters we used the rnorm() function to generate random numbers from Gaussian distributions. R doesn’t have an rbern() function, so we have to use the rbinom() function instead. The function generates random observations from a binomial distribution: the binomial distributions is a more general form of the Bernoulli distribution. It has two parameters, \\(n\\) the number of trials and \\(p\\) the “success” probability of each trial. If we code each level in the binary variable as 0 and 1, \\(p\\) is the probability of getting 1 (that’s why it is called the “success” probability).\n\\[\nBinomial(n, p)\n\\]\nThink of a coin: you flip it 10 times so \\(n = 10\\) (ten trials). If this is a fair coin, then \\(p\\) should be 0.5: 50% of the times you get head (1) and 50% of the times you get tail (0). The rbinom() function takes three arguments: n number of observations (maybe confusingly, not the number of trials), size the number of trials, and p the probability of success. The following code simulates 10 flips of a fair coin with rbinom().\nThe output is 6, meaning 6 out of 10 flips had head (1). Note that the probability of success \\(p\\) is the mean probability of success. In any one observation, you won’t necessarily get 5 of 10 with \\(p = 0.5\\), but if you take many 10-trial observations, then on average you should get pretty close to 0.5. Let’s try this:\nHere we took 100 observations of 10 flips. On average, about 5 of 10 flips got head (it is not precisely 5, but very close).\nA Bernoulli distribution is simply a binomial distribution with a single trial. Imagine again a lexical decision task: each word presented to the participant is one trial and in each trial there is a probability \\(p\\) of getting it right (correctly identifying the type of the word). We can thus use rbinom() with size set to 1. Let’s get 25 observations of 1 trial each.\nFor each trial, we get a 1 for correct or a 0 for incorrect. If you take the mean of the trials it should be very close to 0.8 (with those random observations, it is 0.84). Again, \\(p\\) is the mean probability of success across trials.\nNow, what we are trying to do when modelling binary outcome variables is to estimate the probability \\(p\\) from the data. But there is a catch: probabilities are bounded between 0 and 1 and regression models don’t work with bounded variables out of the box! Bounded probabilities are transformed into an unbounded numeric variable. The following section explains how.\nAs we have just learned, probabilities are bounded between 0 and 1 but we need something that is not bounded because regression models don’t work with bounded numeric variables. This is where the logit function comes in: the logit function (from “logistic unit”) is a mathematical function that transforms probabilities into log-odds. The logit function is the quantile function (the function that returns quantiles, the value below which a given proportion of a probability distribution lies) of the logistic distribution. The logit function is the quantile of a logistic function with mean 0 and standard deviation 1 (a standard logistic distribution). In R, the logit function is qlogis(). The default mean and SD in qlogis() are 0 and 1 respectively, so you can just input the first argument, p which is the probability you want to transform into log-odds.\nqlogis(0.2)\n\n[1] -1.386294\n\nqlogis(0.5)\n\n[1] 0\n\nqlogis(0.8)\n\n[1] 1.386294\nFigure 30.1 shows the logit function (the quantile function of the standard logistic distribution). The probabilities on the x-axis are transformed into log-odds on the y axis. When you fit a regression model with a binary outcome and a Bernoulli family, the estimates of the regression coefficients are in log-odds.\nCode\ndots &lt;- tibble(\n  p = seq(0.1, 0.9, by = 0.1),\n  log_odds = qlogis(p)\n)\n\nlog_odds_p &lt;- tibble(\n  p = seq(0, 1, by = 0.001),\n  log_odds = qlogis(p)\n) %&gt;%\n  ggplot(aes(p, log_odds)) +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, colour = \"#8856a7\", linewidth = 1) +\n  geom_vline(xintercept = 1, colour = \"#8856a7\", linewidth = 1) +\n  geom_hline(yintercept = 0, alpha = 0.5) +\n  geom_line(linewidth = 2) +\n  geom_point(data = dots, size = 4) +\n  geom_point(x = 0.5, y = 0, colour = \"#8856a7\", size = 4) +\n  annotate(\"text\", x = 0.2, y = 3, label = \"logit(p) = log-odds\") +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = NULL, limits = c(0, 1)) +\n  scale_y_continuous(breaks = seq(-6, 6, by = 1), minor_breaks = NULL) +\n  labs(\n    x = \"Probability\",\n    y = \"Log-odds\"\n  )\nlog_odds_p\n\n\n\n\n\n\n\n\nFigure 30.1: The logit function (quantile function): from probabilities to log-odds.\nTo go back from log-odds to probabilities, we use the inverse logit function. This is the CDF of the standard logistic distribution. In R, you apply the inverse logit function (also called the logistic function, because it is the CDF of the standard logistic distribution) with plogis(). As with qnorm(), the default mean and SD are 0 and 1 respectively so you can just input the log-odds you want to transform into probabilities as the first argument q.\nplogis(-3)\n\n[1] 0.04742587\n\nplogis(0)\n\n[1] 0.5\n\nplogis(2)\n\n[1] 0.8807971\n\nplogis(6)\n\n[1] 0.9975274\n\n# To show that plogis is the inverse of qlogis\nplogis(qlogis(0.2))\n\n[1] 0.2\nFigure 30.1 shows the inverse logit transformation of log-odds (on the x-axis) into probabilities (on the y-axis). The inverse logit constructs the typical S-shaped curve (black thick line) of the CDF of the standard logistic distribution. Since probabilities can’t be smaller than 0 and greater than 1, the black line slopes in either direction and it approaches 0 and 1 on the y-axis without ever reaching them (in mathematical terms, it’s an asymptotic line). It is helpful to just memorise that log-odds 0 corresponds to probability 0.5 (and vice versa of course).\nCode\ndots &lt;- tibble(\n  p = seq(0.1, 0.9, by = 0.1),\n  log_odds = qlogis(p)\n)\n\np_log_odds &lt;- tibble(\n  p = seq(0, 1, by = 0.001),\n  log_odds = qlogis(p)\n) %&gt;%\n  ggplot(aes(log_odds, p)) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\") +\n  geom_hline(yintercept = 0, colour = \"#8856a7\", linewidth = 1) +\n  geom_hline(yintercept = 1, colour = \"#8856a7\", linewidth = 1) +\n  geom_vline(xintercept = 0, alpha = 0.5) +\n  geom_line(linewidth = 2) +\n  # geom_point(data = dots, size = 4) +\n  geom_point(x = 0, y = 0.5, colour = \"#8856a7\", size = 4) +\n  annotate(\"text\", x = -4, y = 0.8, label = \"inv_logit(log-odds) = p\") +\n  scale_x_continuous(breaks = seq(-6, 6, by = 1), minor_breaks = NULL, limits = c(-6, 6)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = NULL) +\n  labs(\n    x = \"Log-odds\",\n    y = \"Probability\"\n  )\np_log_odds\n\n\n\n\n\n\n\n\nFigure 30.2: The inverse logit function (CDF function): from log-odds to probabilities.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#sec-prob-log",
    "href": "ch-regression-bernoulli.html#sec-prob-log",
    "title": "30  Binary outcomes",
    "section": "",
    "text": "TipLogit and inverse logit functions\n\n\n\nThe logit function is the quantile function of a standard logistic distribution, used to convert probabilities into log-odds.\nThe inverse logit (aka logistic) function is the CDF of the standard logistic distribution, used to convert log-odds to probabilities.\n\n\n\n\n\n\n\n\nWarningExercise 1\n\n\n\nCalculate the following:\n\nThe log-odds corresponding to \\(p = 0.12, 0.66, 0.9999\\).\nThe probabilities corresponding to log-odds \\(= -6, 0.5, 15\\).",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#nicaraguan-sign-language-single-and-multi-verb-predicates",
    "href": "ch-regression-bernoulli.html#nicaraguan-sign-language-single-and-multi-verb-predicates",
    "title": "30  Binary outcomes",
    "section": "30.2 Nicaraguan Sign Language single and multi-verb predicates",
    "text": "30.2 Nicaraguan Sign Language single and multi-verb predicates\nTo illustrate how to fit a Bernoulli model, we will use data from Brentari et al. (2024) on the emergent Nicaraguan Sign Language (Lengua de Señas Nicaragüense, NSL).\n\nverb_org &lt;- read_csv(\"data/brentari2024/verb_org.csv\")\n\nverb_org\n\n\n  \n\n\n\nverb_org contains information on predicates as signed by three groups (Group): home-signers (homesign), first generation NSL signers (NSL1) and second generation NSL signers (NSL2). Specifically, the data coded in Num_Predicates whether the predicates uttered by the signer were single-verb predicates (single) or a multi-verb predicates (multiple). The hypothesis of the study is that use of multi-verb predicates would increase with each generation, i.e. that NSL1 signers would use more multi-verb predicates than home-signers and that NSL2 signers would use more multi-verb predicates than home-signers and NSL1 signers. (For the linguistic reasons behind this hypothesis, check the paper linked above). Figure 30.3 shows the proportion of single vs multiple predicates in the three groups.\n\n\nCode\nverb_org |&gt; \n  ggplot(aes(Group, fill = Num_Predicates)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    y = \"Proportion\"\n  )\n\n\n\n\n\n\n\n\nFigure 30.3: Proportion of single vs multiple predicates in three groups of NSL signers.\n\n\n\n\n\nWhat do you notice about the type of predicates in the three groups? Does it match the hypothesis put forward by the paper? We can calculate the proportion of multi-verb predicates by creating a column which codes Num_Predicates with 0 for single and 1 for multi-verbs predicates. This is the same dummy coding we encountered in Chapter 27 for categorical predictors, but now we apply that to a binary outcome variable. Then you just take the mean of the dummy column by group, to obtain the proportion of multi-verb predicates for each group. Note that since we code multi-verb predicates with 1, taking the mean gives you the proportion of multi-verb predicates and the proportion of single predicates is just \\(1 - p\\) where \\(p\\) is the proportion of multi-verb predicates.\n\nverb_org &lt;- verb_org |&gt; \n  mutate(\n    Num_Pred_dum = ifelse(Num_Predicates == \"multiple\", 1, 0)\n  )\n\nverb_org |&gt; \n  group_by(Group) |&gt; \n  summarise(\n    prop_multi = round(mean(Num_Pred_dum), 2)\n  )\n\n\n  \n\n\n\nOn average, home-signers produce 36% of multi-verb predicates, first generation signers (NSL1) 19% and second generation signers (NSL2) 50%. In other words, there is a decrease in production of multi-verb predicates from home-signers to NSL1 signers, while NSL2 signers basically just use either single or multi-verb predicates. Most times, it is also useful to plot the proportion for each participant separately. Unfortunately, this is an area were bad practices have become standard so it is worth spending some time on this, in a new section.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#plotting-proportions-percentages-and-accuracy-data",
    "href": "ch-regression-bernoulli.html#plotting-proportions-percentages-and-accuracy-data",
    "title": "30  Binary outcomes",
    "section": "30.3 Plotting proportions, percentages and accuracy data",
    "text": "30.3 Plotting proportions, percentages and accuracy data\nA common (but incorrect) way of plotting proportion/percentage data (like accuracy, or the single vs multi-verb predicates of the verb_org dataset) is to calculate the proportion of each participant and then produce a bar chart with error bars that indicate the mean proportion (i.e. the mean of the proportions of each participant) and the dispersion around the mean accuracy. You might have seen something like Figure 30.4 in many papers. This type of plot is called “bars and whiskers” because the error bars look like cat whiskers.\n\nverb_org |&gt; \n  group_by(Participant, Group) |&gt; \n  summarise(prop = sum(Num_Pred_dum) / n(), .groups = \"drop\") |&gt; \n  group_by(Group) |&gt; \n  add_tally() |&gt;\n  summarise(mean_prop = mean(prop), sd_prop = sd(prop)) |&gt; \n  ggplot(aes(Group, mean_prop)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(\n    aes(\n      ymin = mean_prop - sd_prop / sqrt(46),\n      ymax = mean_prop + sd_prop / sqrt(46)\n    ),\n    width = 0.2\n  )\n\n\n\n\n\n\n\nFigure 30.4: The bad way of plotting proportions.\n\n\n\n\n\nTHE HORROR. Alas, this is a very bad way of processing proportion data. You can learn why in Holtz (2019).1 The alternative (robust) way to plot proportion data is to show the proportion for individual participants. To do so we can use a combination of summarise(), geom_jitter() and stat_summary(). First, we need to compute the proportion of multi-verb predicates by participant. The procedure is the same as calculating the proportion for the three signer groups, but now we do it for each participant. Note that participants only have a unique ID within group, so we need to group by participant and group (we need the group information any way to be able to plot participants by group below). We save the output to a new tibble verb_part.\n\nverb_part &lt;- verb_org |&gt; \n  group_by(Participant, Group) |&gt; \n  summarise(\n    prop_multi = round(mean(Num_Pred_dum), 2),\n    .groups = \"drop\"\n  )\n\nNow we can plot the proportions and add mean and confidence intervals using geom_jitter() and stat_summary(). Before proceeding, you need to install the Hmisc package. There is no need to attach it (it is used by stat_summary() under the hood). Remember not to include the code for installation in your document; you need to install the package only once. After several years of teaching, I still see a lot of students having install.packages() in their scripts.\n\nggplot() +\n  # Proportion of each participant\n  geom_jitter(\n    data = verb_part,\n    aes(x = Group, y = prop_multi),\n    width = 0.1, alpha = 0.5\n  ) +\n  # Mean proportion by stimulus with confidence interval\n  stat_summary(\n    data = verb_org,\n    aes(x = Group, y = Num_Pred_dum, colour = Group),\n    fun.data = \"mean_cl_boot\", size = 0.5\n  ) +\n  labs(\n    title = \"Proportion of takete responses by participant and stimulus\",\n    caption = \"Mean proportion is represented by coloured points with 95% bootstrapped Confidence Intervals.\",\n    x = \"Stimulus\",\n    y = \"Proportion\"\n  ) +\n  ylim(0, 1) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 30.5: How to plot participants’ proportions and overall proportions.\n\n\n\n\n\nIn this data we don’t have many participants for group, but you can immediately appreciate the variability between participants. You will notice something new in the code: we have specified the data inside geom_jitter() and stat_summary() instead of inside ggplot(). This is because the two functions need different data: geom_jitter() needs the data with the proportion we calculated for each participant and group; stat_summary() needs to calculate the mean and CIs from the overall data, rather than from the proportion data we created. This is the aspect that a lot of researchers get wrong: you should not take the mean of the participant proportions, unless all participants have exactly the same number of observations. In the verb_org data specifically, the number of observations do indeed differ for each participant. You can check this yourself by getting the number of observations per participant per group with the count() function. We are also specifying the aesthetics within each geom/stat function, because while x is the same, the y differs! In stat_summary(), the fun.data argument lets you specify the function you want to use for the summary statistics to be added. Here we are using the mean_cl_boot function, which returns the mean proportion of Response_num and the 95% Confidence Intervals (CIs) of that mean. The CIs are calculated using a bootstrapping procedure (if you are interested in learning what that is, check the documentation of smean.sd from the Hmisc package).\nThis also makes a nice opportunity to mention one shortcoming of the models we are fitting, thinking a bit ahead of ourselves: the regression models we cover in Week 4 to 10 do not account for the fact that the data comes from multiple participants and instead they just assume that each observation in the data set is from a different participant. When you have multiple observations from each participant, we call this a repeated measure design. This is a problem because it breaks one assumption of regression models: that all the observations are independent from each other. Of course, if they come from the same participant, they are not independent. We won’t have time in this course to learn about the solution (i.e. hierarchical regression models, also known as multi-level, mixed-effects, nested-effects, or random-effects models; these are just regression models that are set up so they can account for hierarchical grouping in the data, like multiple observations from multiple participants, or multiple pupils from multiple classes from multiple schools), but I point you to further resources in ?sec-next (TBA).",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#bernoulli-model-of-nsl-predicates",
    "href": "ch-regression-bernoulli.html#bernoulli-model-of-nsl-predicates",
    "title": "30  Binary outcomes",
    "section": "30.4 Bernoulli model of NSL predicates",
    "text": "30.4 Bernoulli model of NSL predicates\nThe hypothesis of the study is that use of multi-verb predicates would increase with each generation. To statistically assess this hypothesis and obtain estimates of the proportion of multiple predicates, we can fit a Bernoulli model with Num_Predicates as the outcome variable and Group as a categorical predictor. Before we move on onto fitting the model, it is useful to transform Num_Predicates into a factor and specify the order of the levels so that single is the first level and multiple is the second level. Wee need this because Bernoulli models estimate the probability (the parameter \\(p\\) in \\(Bernoulli(p)\\)) of obtaining the second level in the outcome variable (remember, \\(p\\) is the probability of success, i.e. of obtaining \\(1\\) in a 0/1 coded binary variable). The first level in the factor corresponds to 0 and the second level to 1. We want to set this order because the hypothesis states that multi-verb predicates should increase, and by modelling the probability of multi-verb predicates we can more straightforwardly address the hypothesis (of course, if the probability of multi-verb predicates increases, the probability of single-verb predicates decreases, because \\(q = 1 - p\\)). Complete the following code to make Num_Predicates into a factor.\n\nverb_org &lt;- verb_org |&gt; \n  mutate(\n    ...\n  )\n\nIf you reproduce Figure 30.3 now, you will see that the order of Num_Predicates in the legend is “single” then “multiple” and that the order of the proportions in the bar chart have flipped.\n\n\nCode\nverb_org |&gt; \n  ggplot(aes(Group, fill = Num_Predicates)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    y = \"Proportion\"\n  )\n\n\n\n\n\n\n\n\nFigure 30.6: Proportion of single vs multiple predicates in three groups of NSL signers (repr).\n\n\n\n\n\nNow we can move on onto modelling. Here is the mathematical formula of the model we will fit.\n\\[\n\\begin{align}\nMVP_i & \\sim Bernoulli(p_i) \\\\\nlogit(p_i) & = \\beta_0 + \\beta_1 \\cdot NSL1_i + \\beta_2 \\cdot NSL2_i \\\\\n\\end{align}\n\\]\nThe probability of using an multi-verb predicate follows a Bernoulli distribution with probability \\(p\\), which is the only parameter. The logit function is applied to the parameter \\(p\\) as discussed in Section 30.1. Functions applied to model parameters are known as link functions. Bernoulli models use the logit link (i.e. the logit function). Because the logit link returns log-odds from probabilities, the log-odds of \\(p\\), rather than just the probability \\(p\\), are equal to the regression equation \\(\\beta_0 + \\beta_1 \\cdot NSL1_i + \\beta_2 \\cdot NSL2_i\\). This model has three regression coefficients: \\(\\beta_0, \\beta_1, \\beta_2\\). With one categorical predictor, regression models need one coefficient for each level in the predictor: since we have three levels in Group, we need three coefficients.\n\n\n\n\n\n\nNoteQuiz 1\n\n\n\n\nWhich of the following statements is correct?\n\n 1. The three coefficients are, respectively, the log-odds of MVPs in home-signers, in NSL1 and in NSL2. 2. The three coefficients are, respectively, the log-odds of MVPs in home-signers, the difference between NSL1 and home-signers, and the difference between NSL2 and home-signers. 3. The three coefficients are, respectively, the log-odds of MVPs in home-signers, the difference between NSL1 and home-signers, and the difference between NSL2 and NSL1.\n\n\\(\\beta_0\\) is the mean probability of multi-verb predicates in the home-signer group. TRUEFALSE\nThe model implies three dummy variables for the Group variable. TRUEFALSE\n\\(NSL1\\) is 0 when the group is NSL2 and 1 when it is NSL1. TRUEFALSE\nThe mean probability of multi-verb predicates for NSL2 is \\(inv\\_logit(\\beta_0 + \\beta_2)\\). TRUEFALSE\n\n\n\n\n\n\n\n\n\nImportantExplanation\n\n\n\n\n\na. With default treatment contrasts, the intercept is the mean of the reference level, while the other coefficients are the difference of the other level and the reference.\nb. It is false because \\(\\beta_0\\) is the log-odd probability of multi-verb predicates in the home-signer group.\nc. It is false because the number of dummy variables is the number of levels minus one.\nd. The dummy \\(NSL1\\) is 0 if group is either home-signers or NSL2.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#fit-the-bernoulli-model-with-brms",
    "href": "ch-regression-bernoulli.html#fit-the-bernoulli-model-with-brms",
    "title": "30  Binary outcomes",
    "section": "30.5 Fit the Bernoulli model with brms",
    "text": "30.5 Fit the Bernoulli model with brms\nWe can now fit the model with brm(). The model formula has no surprises: Num_Predicates ~ Group. This looks like the formula of the model in Chapter 28: VOT_ms ~ phonation. We are modelling Num_Predicates as a function of Group, like we modelled VOT_ms as a function of phonation. In this model, you have to set the family to bernoulli to fit a Bernoulli model. We also set the number of cores, seed and file as usual.\n\nmvp_bm &lt;- brm(\n  Num_Predicates ~ Group,\n  family = bernoulli,\n  data = verb_org,\n  cores = 4,\n  seed = 1329,\n  file = \"cache/ch-regression-bernoulli_mvp_bm\"\n)\n\nLet’s inspect the model summary (we will get 80% CrIs).\n\nsummary(mvp_bm, prob = 0.8)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Num_Predicates ~ Group \n   Data: verb_org (Number of observations: 630) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.58      0.15    -0.77    -0.38 1.00     2441     2733\nGroupNSL1    -0.88      0.22    -1.17    -0.60 1.00     2503     2532\nGroupNSL2     0.56      0.21     0.30     0.83 1.00     2580     2560\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe summary informs us that we used a Bernoulli family and that the link for the mean \\(\\mu\\), i.e. \\(p\\) in this model, is the logit function. The formula, data and draws parts don’t need explanation. Since the model has only one parameter \\(p\\), to which we apply the regression equation, we only have a Regression Coefficients table, without a Further Distributional Parameters table. Let’s focus on the regression coefficients.\n\nfixef(mvp_bm, probs = c(0.1, 0.9)) |&gt; round(2)\n\n          Estimate Est.Error   Q10   Q90\nIntercept    -0.58      0.15 -0.77 -0.38\nGroupNSL1    -0.88      0.22 -1.17 -0.60\nGroupNSL2     0.56      0.21  0.30  0.83\n\n\nAccording to the Intercept (\\(\\beta_0\\)), there is an 80% probability that the log-odds of a multi-verb predicate are between -0.77 and -0.38 in home-signers. GroupNSL1 is \\(\\beta_1\\): the 80% CrI suggests a difference of -1.17 to -0.6 between the log-odds of NSL1 and home-signers. Finally, GroupNSL2 (\\(\\beta_2\\)) indicates a difference between +0.3 and +0.83 between NSL2 and home-signers, at 80% confidence. In other words, the log-odds of multi-verb predicates is lower in NSL1 and higher in NSL2 compared to home-signers.\nIt’s easier to understand the results if we convert the log-odds to probabilities. First, we need to calculate the predicted log-odds for each group and then we can use plogis(), the inverse logit, to transform log-odds to probabilities.\n\n# Get draws\nmvp_bm_draws &lt;- as_draws_df(mvp_bm)\n\n# Calculate predicted log-odds by group\nmvp_bm_draws &lt;- mvp_bm_draws |&gt; \n  mutate(\n    homesign = b_Intercept,\n    NSL1 = b_Intercept + b_GroupNSL1,\n    NSL2 = b_Intercept + b_GroupNSL2,\n  )\n\n# Pivot to longer format\nmvp_bm_draws_long &lt;- mvp_bm_draws |&gt; \n  select(homesign:NSL2) |&gt; \n  pivot_longer(everything(), names_to = \"Group\", values_to = \"log\") |&gt; \n  mutate(\n    p = plogis(log)\n  )\n\nWe can now summarise the draws to get CrIs and plot the posteriors.\n\n\nCode\nmvp_bm_draws_long |&gt; \n  group_by(Group) |&gt; \n  summarise(\n    cri_l80 = quantile2(p, 0.1) |&gt; round(2),\n    cri_u80 = quantile2(p, 0.9) |&gt; round(2)\n  )\n\n\n\n  \n\n\n\nThe predicted probability of multi-verb predicates in home-signers is 32-41%, for NSL1 it is 16-22% and for NSL2 it is 45-54%, at 80% confidence. Based on these results, we can see more clearly that the hypothesis of the study is not fully borne out by the data: we do not observe an increase in probability of multi-verb predicates from home-signers to NSL1 to NSL2. Instead, there is a decrease from home-signers to NSL1 and an increase from NSL1 to NSL2. Moreover, the predicted probability of multi-verb predicates in NSL2, 45-54%, indicates that NSL2 possibly use either single and multi-verb predicates with equal probability. Figure 30.7 plots the predicted posterior probabilities of multi-verb predicates.\n\n\nCode\nmvp_bm_draws_long |&gt; \n  mutate(Group = factor(Group, levels = c(\"homesign\", \"NSL1\", \"NSL2\"))) |&gt; \n  ggplot(aes(p, Group)) +\n  stat_halfeye() +\n  labs(x = \"Probability of multi-verb predicate\")\n\n\n\n\n\n\n\n\nFigure 30.7: Predicted probabilities of multi-verb predicates by group as per a Bernoulli regression model.\n\n\n\n\n\nVery often, we also want to know the posterior probability of the difference between a level and another level that is not the reference: here for example we might want to know the difference between NSL1 and NSL2. It is easy: we just take the difference of the inverse-logit-transformed predicted draws columns NSL1 and NSL2 in mvp_bm_draws. This approach also works with the log-odds draws directly, to get the difference in log-odds.\n\nmvp_bm_draws &lt;- mvp_bm_draws |&gt; \n  mutate(\n    NSL2_NSL1_log = NSL2 - NSL1,\n    NSL2_NSL1_p = plogis(NSL2) - plogis(NSL1)\n  )\n\nquantile2(mvp_bm_draws$NSL2_NSL1_log, c(0.1, 0.9)) |&gt; round(2)\n\n q10  q90 \n1.17 1.73 \n\nquantile2(mvp_bm_draws$NSL2_NSL1_p, c(0.1, 0.9)) |&gt; round(2)\n\n q10  q90 \n0.25 0.36 \n\n\nThe log-odds in NSL2 are 1.17-1.73 units higher than in NSL1, at 80% probability. The probability of a multi-verb predicate is 25 to 36 percentage points higher in NSL2 than NSL1, at 80% confidence. Note how we say percentage points and not percent. There isn’t a 25-36% increase, but the probability increases by 25-36 units, which for probabilities expressed as percentages are called percentage points. Mixing these things up is a common mistake, so be careful.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#reporting",
    "href": "ch-regression-bernoulli.html#reporting",
    "title": "30  Binary outcomes",
    "section": "30.6 Reporting",
    "text": "30.6 Reporting\n\n\n\n\n\n\nWarningSpotlight: Bernoulli, binomial and logistic regression\n\n\n\n\n\nA lot of researchers know Bernoulli models under the name “binomial” or “logistic” regression. Please, note that these are exactly equivalent: they refer to a model with a Bernoulli distribution for the outcome variable. It is just that different research traditions call them differently.\nSo if somebody asks you to run a logistic regression, or if you read a paper that reports one, what they just mean is to run a regression with a binary outcome variable and a Bernoulli distribution!\n\n\n\n\n\n\n\nBrentari, Diane, Susan Goldin-Meadow, Laura Horton, Ann Senghas, and Marie Coppola. 2024. “The Organization of Verb Meaning in Lengua de Señas Nicaragüense (LSN): Sequential or Simultaneous Structures?” Glossa: A Journal of General Linguistics 9 (1). https://doi.org/10.16995/glossa.10342.\n\n\nHoltz, Yan. 2019. “The Issue with Error Bars.” https://www.data-to-viz.com/caveat/error_bar.html.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#footnotes",
    "href": "ch-regression-bernoulli.html#footnotes",
    "title": "30  Binary outcomes",
    "section": "",
    "text": "This StackExchange answer is also useful: https://stats.stackexchange.com/a/367889/128897.↩︎",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat-cat.html",
    "href": "ch-regression-cat-cat.html",
    "title": "31  Regression models: multiple predictors",
    "section": "",
    "text": "31.1 Two categorical predictors\nSo far, we fitted regressions with a single predictor, like the following Gaussian model of reaction times from ?sec-regression-index:\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_1 \\cdot W_{\\text{T}[i]} + \\beta_2 \\cdot W_{\\text{F}[i]}\\\\\n\\end{align}\n\\]\nThe categorical predictor \\(W\\) (IsWord in the data) has two levels (TRUE and FALSE), so there are two indexing variables: \\(W_{\\text{T}}\\) and \\(W_{\\text{F}}\\). Each indexing variable gets its coefficient: \\(\\beta_1\\) and \\(\\beta_2\\). In most context, however, you will want to investigate the effects of more than one predictor.\nRegression models can be fit with multiple predictors. Traditionally, regression models with a single predictor were called “simple regression” and models with more than one “multiple regression”, but it doesn’t make sense to have a specific name: they are all regression models. In this chapter, we will discuss regression models with two categorical predictors.\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\nf0_bm &lt;- brm(\n  f0mn ~ gender + attitude,\n  family = gaussian,\n  data = polite,\n  cores = 4,\n  seed = 7123,\n  file = \"cache/ch-regression-cat-cat_f0_bm\"\n)\nsummary(f0_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ gender + attitude \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     255.14      4.48   246.47   263.81 1.00     4323     3184\ngenderM      -116.07      5.28  -126.30  -105.67 1.00     4618     2936\nattitudepol   -14.71      5.34   -25.05    -4.40 1.00     4708     2884\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.03      1.93    35.48    42.98 1.00     4604     2932\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nconditional_effects(f0_bm, \"gender\")\nconditional_effects(f0_bm, \"attitude\")\nconditional_effects(f0_bm, \"gender:attitude\")",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Regression models: multiple predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat-cat.html#is-the-effect-of-attitude-the-same-in-both-genders",
    "href": "ch-regression-cat-cat.html#is-the-effect-of-attitude-the-same-in-both-genders",
    "title": "31  Regression models: multiple predictors",
    "section": "31.2 Is the effect of attitude the same in both genders?",
    "text": "31.2 Is the effect of attitude the same in both genders?\n\npolite |&gt; \n  ggplot(aes(gender, f0mn, colour = attitude)) +\n  geom_jitter(alpha = 0.7, position = position_jitterdodge(jitter.width = 0.1, seed = 2836))\n\n\n\n\n\n\n\nFigure 31.1",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Regression models: multiple predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-interaction.html",
    "href": "ch-regression-interaction.html",
    "title": "32  Regression models: interactions",
    "section": "",
    "text": "polite &lt;- read_csv(\"data/winter2012/polite.csv\")\n\n\nf0_bm_int &lt;- brm(\n  f0mn ~ gender + attitude + gender:attitude,\n  family = gaussian,\n  data = polite,\n  cores = 4,\n  seed = 7123,\n  file = \"cache/ch-regression-interaction_f0_bm_int\"\n)\n\n\nsummary(f0_bm_int)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ gender + attitude + gender:attitude \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             256.56      5.20   246.38   267.27 1.00     2632     2950\ngenderM              -119.38      7.66  -134.08  -104.50 1.00     2532     2665\nattitudepol           -17.48      7.28   -31.76    -3.18 1.00     2573     2806\ngenderM:attitudepol     6.65     10.67   -14.20    27.47 1.00     2191     2693\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.10      1.94    35.52    43.14 1.00     3834     2779\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(f0_bm_int, \"gender:attitude\")\n\n\n\n\n\n\n\n\n\nf0_bm_int_draws &lt;- as_draws_df(f0_bm_int)\n\n\nf0_bm_int_draws &lt;- f0_bm_int_draws |&gt; \n  mutate(\n    f_inf = b_Intercept,\n    f_pol = b_Intercept + b_attitudepol,\n    m_inf = b_Intercept + b_genderM,\n    m_pol = b_Intercept + b_genderM + b_attitudepol + `b_genderM:attitudepol`\n  )\n\n\nlibrary(posterior)\n\nThis is posterior version 1.6.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nf0_bm_int_draws |&gt; \n  mutate(\n    m_pol_inf = m_pol - m_inf\n  ) |&gt; \n  summarise(\n    mean_diff = mean(m_pol_inf), sd_diff = sd(m_pol_inf),\n    lo_diff = quantile2(m_pol_inf, probs = 0.025), hi_diff = quantile2(m_pol_inf, probs = 0.975)\n  ) |&gt;\n  round()",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Regression models: interactions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bezeau, Scott, and Roger Graves. 2001. “Statistical Power and\nEffect Sizes of Clinical Neuropsychology Research.” Journal\nof Clinical and Experimental Neuropsychology 23 (3): 399–406. https://doi.org/10.1076/jcen.23.3.399.1181.\n\n\nBochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas,\nIrys-Amélie Champagne, Kaidi Chen, Melanie Röthlisberger, Erin M.\nBuchanan, and Timo B. Roettger. 2023. “Reproducible Research\nPractices and Transparency Across Linguistics.” Glossa\nPsycholinguistics 2 (1). https://doi.org/10.5070/g6011239.\n\n\nBrentari, Diane, Susan Goldin-Meadow, Laura Horton, Ann Senghas, and\nMarie Coppola. 2024. “The Organization of Verb Meaning in Lengua\nde Señas Nicaragüense (LSN): Sequential or Simultaneous\nStructures?” Glossa: A Journal of General Linguistics 9\n(1). https://doi.org/10.16995/glossa.10342.\n\n\nBrugger, Peter. 2001. “From Haunted Brain to Haunted Science: A\nCognitive Neuroscience View of Paranormal and Pseudoscientific\nThought.” Hauntings and Poltergeists: Multidisciplinary\nPerspectives, 195213.\n\n\nBrysbaert, Marc. 2020. “Power Considerations in Bilingualism\nResearch: Time to Step up Our Game.” Bilingualism: Language\nand Cognition 24 (5): 813818. https://doi.org/10.1017/s1366728920000437.\n\n\nBrysbaert, Marc, and Michaël Stevens. 2018. “Power Analysis and\nEffect Size in Mixed Effects Models: A Tutorial.” Journal of\nCognition 1 (1). https://doi.org/10.5334/joc.10.\n\n\nBürkner, Paul-Christian. 2017. “Brms: An r Package for Bayesian\nMultilevel Models Using Stan.” Journal of Statistical\nSoftware 80 (1): 128. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the r\nPackage Brms.” The R Journal 10 (1): 395411. https://doi.org/10.32614/RJ-2018-017.\n\n\n———. 2024. “Estimating Multivariate Models with Brms.” https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal\nRegression Models in Psychology: A Tutorial.” Advances in\nMethods and Practices in Psychological Science 2 (1): 77101. https://doi.org/10.1177/2515245918823199.\n\n\nCameron-Faulkner, Thea, Nivedita Malik, Circle Steele, Stefano Coretta,\nLudovica Serratrice, and Elena Lieven. 2020. “A Cross-Cultural\nAnalysis of Early Prelinguistic Gesture Development and Its Relationship\nto Language Development.” Child Development 92 (1):\n273290. https://doi.org/10.1111/cdev.13406.\n\n\nCassidy, Scott A., Ralitza Dimova, Benjamin Giguère, Jeffrey R. Spence,\nand David J. Stanley. 2019. “Failing Grade: 89 Per-Cent of\nIntroduction to Psychology Textbooks That Define/Explain Statistical\nSignificance Do so Incorrectly.” Advances in Methods and\nPractices in Psychological Science. https://doi.org/10.1177/2515245919858072.\n\n\nCharles, Sarah J., James E. Bartlett, Kyle J. Messick, Thomas J.\nColeman, and Alex Uzdavines. 2019. “Researcher Degrees of Freedom\nin the Psychology of Religion.” The International Journal for\nthe Psychology of Religion 29 (4): 230245.\n\n\nCohen, Jacob. 1962. “The Statistical Power of Abnormal-Social\nPsychological Research: A Review.” The Journal of Abnormal\nand Social Psychology 65 (3): 145–53. https://doi.org/10.1037/h0045186.\n\n\nCoretta, Stefano. 2019a. “An Exploratory Study of Voicing-Related\nDifferences in Vowel Duration as Compensatory Temporal Adjustment in\nItalian and Polish.” Glossa: A Journal of General\nLinguistics 4 (1): 1–25. https://doi.org/10.5334/gjgl.869.\n\n\n———. 2019b. “Vowel Duration, Voicing Duration, and Vowel Height:\nAcoustic and Articulatory Data from Italian [Research\nCompendium].” https://doi.org/10.17605/OSF.IO/XDGFZ.\n\n\n———. 2020. “Open Science in Phonetics and Phonology.” https://doi.org/10.31219/osf.io/4dz5t.\n\n\nCoretta, Stefano, and Paul-Christian Bürkner. 2025. “Bayesian Beta\nRegressions with Brms in r: A Tutorial for Phoneticians.” https://doi.org/10.31219/osf.io/f9rqg_v1.\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke,\nByron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023.\n“Multidimensional Signals and Analytic Flexibility: Estimating\nDegrees of Freedom in Human-Speech Analyses.” Advances in\nMethods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nCoretta, Stefano, Josiane Riverin-Coutlée, Enkeleida Kapia, and Stephen\nNichols. 2022. “Northern Tosk Albanian.” Journal of the\nInternational Phonetic Association, 123. https://doi.org/10.1017/s0025100322000044.\n\n\nCumming, Geoff. 2013. “The New Statistics: Why and How.”\nPsychological Science 25 (1): 729. https://doi.org/10.1177/0956797613504966.\n\n\nDarwin Holmes, Andrew Gary. 2020. “Researcher Positionality: A\nConsideration of Its Influence and Place in Qualitative\nResearcha New Researcher Guide.” Shanlax\nInternational Journal of Education 8 (4): 110. https://doi.org/10.34293/education.v8i4.3232.\n\n\nDeBruine, Lisa M., and Dale J. Barr. 2021. “Understanding\nMixed-Effects Models Through Data Simulation.” Advances in\nMethods and Practices in Psychological Science 4 (1). https://doi.org/10.1177/2515245920965119.\n\n\nDevezer, Berna, Danielle J. Navarro, Joachim Vandekerckhove, and Erkan\nOzge Buzbas. 2021. “The Case for Formal Methodology in Scientific\nReform.” Royal Society Open Science 8 (3): rsos.200805,\n200805. https://doi.org/10.1098/rsos.200805.\n\n\nDienes, Zoltan. 2008. Understanding Psychology as a Science: An\nIntroduction to Scientific and Statistical Inference. Macmillan\nInternational Higher Education.\n\n\nDobreva, Simona. 2024. “Bayesian Vs Frequentist Approach: Same\nData, Opposite Results.” https://365datascience.com/trending/bayesian-vs-frequentist-approach/.\n\n\nDryer, Matthew S. 2008. “Descriptive Theories, Explanatory\nTheories, and Basic Linguistic Theory.” In Catching Language:\nThe Standing Challenge of Grammar Writing, edited by Felix K.\nAmeka, Alan Charles Dench, and Nicholas Evans. Vol. 167. Trends in\nLinguistics Studies and Monographs. Mouton De Gruyter.\n\n\nEgurtzegi, Ander, and Christopher Carignan. 2020. “An Acoustic\nDescription of Mixean Basque.” The Journal of the Acoustical\nSociety of America 147 (4): 27912802. https://doi.org/10.1121/10.0000996.\n\n\nEllis, J. Timothy, and Yair Levy. 2008. “Framework of\nProblem-Based Research: A Guide for Novice Researchers on the\nDevelopment of a Research-Worthy Problem.” Informing Science:\nThe International Journal of an Emerging Transdiscipline 11: 1733.\nhttps://doi.org/10.28945/438.\n\n\nFanelli, Daniele. 2010. “Do Pressures to Publish Increase\nScientists’ Bias? An Empirical Support from US States Data.”\nEdited by Enrico Scalas. PLoS ONE 5 (4): e10271. https://doi.org/10.1371/journal.pone.0010271.\n\n\n———. 2012. “Negative Results Are Disappearing from Most\nDisciplines and Countries.” Scientometrics 90 (3):\n891–904. https://doi.org/10.1007/s11192-011-0494-7.\n\n\nFischhoff, Baruch. 1975. “Hindsight Is Not Equal to Foresight: The\nEffect of Outcome Knowledge on Judgment Under Uncertainty.”\nJournal of Experimental Psychology: Human Perception and\nPerformance 1 (3): 288.\n\n\nFlake, Jessica Kay, and Eiko I. Fried. 2020. “Measurement\nSchmeasurement: Questionable Measurement Practices and How to Avoid\nThem.” Advances in Methods and Practices in Psychological\nScience 3 (4): 456465. https://doi.org/10.1177/2515245920952393.\n\n\nGaeta, Laura, and Christopher R. Brydges. 2020. “An Examination of\nEffect Sizes and Statistical Power in Speech, Language, and Hearing\nResearch.” Journal of Speech, Language, and Hearing\nResearch 63 (5): 15721580. https://doi.org/10.1044/2020_jslhr-19-00299.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in\nHereditary Stature.” The Journal of the Anthropological\nInstitute of Great Britain and Ireland 15: 246. https://doi.org/10.2307/2841583.\n\n\n———. 1980. “Kinship and Correlation.” The North\nAmerican Review 150 (401): 419431.\n\n\nGelman, Andrew. 2005. “Analysis of Variance: Why It Is More\nImportant Than Ever.” The Annals of Statistics 33 (1).\nhttps://doi.org/10.1214/009053604000001048.\n\n\nGelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and\nObjective in Statistics.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 180 (4): 9671033. https://doi.org/10.1111/rssa.12276.\n\n\nGelman, Andrew, Daniel Lakeland, Brian Haig, Christian Hennig, Art Owen,\nRobert Cousins, Stan Young, et al. 2019. “Many Perspectives on\nDeborah Mayo’s “Statistical Inference as Severe Testing:\nHow to Get Beyond the Statistics Wars”.” https://doi.org/10.48550/arXiv.1905.08876.\n\n\nGelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in\nScience: Data-Dependent Analysis. A “Garden of Forking\nPaths”explains Why Many Statistically\nSignificant Comparisons Don’t Hold Up.” American\nScientist 102 (6): 460466.\n\n\nGelman, Andrew, Deborah Ann Nolan, and Deborah Ann Nolan. 2011.\nTeaching statistics: a bag of tricks. Repr. Oxford: Oxford\nUniv. Press.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between\n“Significant” and “Not\nSignificant” Is Not Itself Statistically\nSignificant.” The American Statistician 60 (4): 328331.\nhttps://doi.org/10.1198/000313006X152649.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The\nJournal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\n———. 2018. “Statistical Rituals: The Replication Delusion and How\nWe Got There.” Advances in Methods and Practices in\nPsychological Science 1 (2): 198218. https://doi.org/10.1177/2515245918771329.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The\nNull Ritual. What You Always Wanted to Know about\nSignificance Testing but Were Afraid to Ask.” In,\n391408.\n\n\nHeiss, Andrew. 2021. “A Guide to Modeling Proportions with\nBayesian Beta and Zero-Inflated Beta Regression Models.” https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide.\n\n\nHoltz, Yan. 2019. “The Issue with Error Bars.” https://www.data-to-viz.com/caveat/error_bar.html.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings\nAre False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1080/09332480.2019.1579573.\n\n\nJafar, Anisa J. N. 2018. “What Is Positionality and Should It Be\nExpressed in Quantitative Studies?” Emergency Medicine\nJournal. https://doi.org/10.1136/emermed-2017-207158.\n\n\nJohn, Leslie K., George Loewenstein, and Drazen Prelec. 2012.\n“Measuring the Prevalence of Questionable Research Practices with\nIncentives for Truth Telling.” Psychological Science 23\n(5): 524532. https://doi.org/10.1177/0956797611430953.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results\nAre Known.” Personality and Social Psychology Review 2\n(3): 196217. https://doi.org/10.1207/s15327957pspr0203_4.\n\n\nKirby, James, and Morgan Sonderegger. 2018. “Mixed-Effects Design\nAnalysis for Experimental Phonetics.” Journal of\nPhonetics 70: 7085. https://doi.org/10.1016/j.wocn.2018.05.005.\n\n\nKobrock, Kristina, and Timo B. Roettger. 2023. “Assessing the\nReplication Landscape in Experimental Linguistics.” Glossa\nPsycholinguistics 2 (1). https://doi.org/10.5070/g6011135.\n\n\nKoole, Sander L, and Daniël Lakens. 2012. “Rewarding Replications:\nA Sure and Simple Way to Improve Psychological Science.”\nPerspectives on Psychological Science 7 (6): 608614.\n\n\nKruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New\nStatistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power\nAnalysis from a Bayesian Perspective.” Psychonomic Bulletin\n& Review 25 (1): 178206. https://doi.org/10.3758/s13423-016-1221-4.\n\n\nKurtz, Solomon. 2023. Statistical Rethinking with Brms, Ggplot2, and\nthe Tidyverse: Second Edition. Version 0.4.0. https://bookdown.org/content/4857/.\n\n\nLorson, Alexandra, Chris Cummins, and Hannah Rohde. 2021.\n“Strategic Use of (Un)certainty Expressions.” Frontiers\nin Communication 6 (March): 635156. https://doi.org/10.3389/fcomm.2021.635156.\n\n\nMakel, Matthew C., Jonathan A. Plucker, and Boyd Hegarty. 2012.\n“Replications in Psychology Research: How Often Do They Really\nOccur?” Perspectives on Psychological Science 7 (6):\n537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMayo, Deborah G. 2018. Statistical Inference as Severe Testing: How\nto Get Beyond the Statistics Wars. 1st ed. Cambridge University\nPress. https://doi.org/10.1017/9781107286184.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. Second edition. Chapman & Hall/CRC\nTexts in Statistical Science Series. Boca Raton: CRC Press.\n\n\nMorin, Olivier. 2015. “A Plea for\n“Shmeasurement” in the Social\nSciences.” Biological Theory 10 (3): 237245. https://doi.org/10.1007/s13752-015-0217-z.\n\n\nNicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025.\nIntroduction to Bayesian Data Analysis for Cognitive Science.\nhttps://bruno.nicenboim.me/bayescogsci/.\n\n\nNickerson, Raymond S. 1998. “Confirmation Bias: A Ubiquitous\nPhenomenon in Many Guises.” Review of General Psychology\n2 (2): 175220. https://doi.org/10.1037/1089-2680.2.2.175.\n\n\nNissen, Silas Boye, Tali Magidson, Kevin Gross, and Carl T. Bergstrom.\n2016. “Publication Bias and the Canonization of False\nFacts.” Elife 5: e21451. https://doi.org/10.7554/eLife.21451.\n\n\nNosek, Brian A, and Daniël Lakens. 2014. “A Method to Increase the\nCredibility of Published Results.” Social Psychology 45\n(3): 137141.\n\n\nOkasha, Samir. 2016. Philosophy of Science: Very Short\nIntroduction. Oxford: Oxford University Press. https://doi.org/10.1093/actrade/9780192802835.001.0001.\n\n\nPedersen, Eric J., David L. Miller, Gavin L. Simpson, and Noam Ross.\n2019. “Hierarchical Generalized Additive Models in Ecology: An\nIntroduction with Mgcv.” PeerJ 7: e6876. https://doi.org/10.7717/peerj.6876.\n\n\nPerezgonzalez, Jose D. 2015. “Fisher, Neyman-Pearson or NHST? A\nTutorial for Teaching Data Testing.” Frontiers in\nPsychology 6 (223). https://doi.org/10.3389/fpsyg.2015.00223.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical\nComputing [Version 4.5.0].\n\n\nRoettger, Timo B. 2019. “Researcher Degrees of Freedom in Phonetic\nSciences.” Laboratory Phonology: Journal of the Association\nfor Laboratory Phonology 10 (1): 127.\n\n\nRoettger, Timo B., Bodo Winter, and Harald Baayen. 2019. “Emergent\nData Analysis in Phonetic Sciences: Towards Pluralism and\nReproducibility.” Journal of Phonetics 73: 17. https://doi.org/10.1016/j.wocn.2018.12.001.\n\n\nRosenberg, Alexander, and Lee Mclntyre. 2020. Philosophy of science:\na contemporary introduction. Fourth edition. Routledge contemporary\nintroductions to philosophy. New York London: Routledge.\n\n\nScheel, Anne M. 2022. “Why Most Psychological Research Findings\nAre Not Even Wrong.” Infant and Child Development 31\n(1): e2295. https://doi.org/10.1002/icd.2295.\n\n\nScheel, Anne M., Leonid Tiokhin, Peder M. Isager, and Daniël Lakens.\n2020. “Why Hypothesis Testers Should Spend Less Time Testing\nHypotheses.” Perspectives on Psychological Science 16\n(4): 744–55. https://doi.org/10.1177/1745691620966795.\n\n\nSedlmeier, Peter, and Gerd Gigerenzer. 1992. “Do Studies of\nStatistical Power Have an Effect on the Power of Studies?” In,\n389–406. Washington: American Psychological Association. https://doi.org/10.1037/10109-032.\n\n\nSilberzahn, Raphael, Eric L. Uhlmann, Daniel P. Martin, Pasquale\nAnselmi, Frederik Aust, Eli Awtrey, Štěpán Bahník, Feng Bai, Colin\nBannard, and Evelina Bonnier. 2018. “Many Analysts, One Data Set:\nMaking Transparent How Variations in Analytic Choices Affect\nResults.” Advances in Methods and Practices in Psychological\nScience 1 (3): 337356. https://doi.org/10.1177/2515245917747646.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011.\n“False-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as\nSignificant.” Psychological Science 22 (11): 13591366.\n\n\nSimpson, Gavin L. 2018. “Fitting GAMs with Brms: Part 1.”\nhttps://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/.\n\n\nSlow Science Academy. 2010. “The Slow Science Manifesto.”\nhttp://slow-science.org.\n\n\nSong, Yoonsang, Youngah Do, Arthur L. Thompson, Eileen R. Waegemaekers,\nand Jongbong Lee. 2020. “Second Language Users Exhibit Shallow\nMorphological Processing.” Studies in Second Language\nAcquisition 42 (5): 11211136. https://doi.org/10.1017/s0272263120000170.\n\n\nSóskuthy, Márton. 2021. “Evaluating Generalised Additive Mixed\nModelling Strategies for Dynamic Speech Analysis.” Journal of\nPhonetics 84: 101017. https://doi.org/10.1016/j.wocn.2020.101017.\n\n\n———. n.d. “Generalised Additive Mixed Models for Dynamic Analysis\nin Linguistics: A Practical Introduction.” https://doi.org/10.48550/arXiv.1703.05339.\n\n\nSterling, Theodore D. 1959. “Publication Decisions and Their\nPossible Effects on Inferences Drawn from Tests of\nSignificanceor Vice Versa.” Journal of the\nAmerican Statistical Association 54 (285): 3034.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika 6 (1): 1. https://doi.org/10.2307/2331554.\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley,\nFilip Nenadić, and Michelle Sims. 2019. “The Massive Auditory\nLexical Decision (MALD) Database.” Behavior Research\nMethods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.\n\n\nTukey, John W. 1969. “Analyzing Data: Sanctification or Detective\nWork?” American Psychologist 24 (2): 83–91. https://doi.org/10.1037/h0027108.\n\n\n———. 1980. “We Need Both Exploratory and Confirmatory.”\nThe American Statistician 34 (1): 23–25. https://doi.org/10.2307/2682991.\n\n\nTversky, Amos, and Daniel Kahneman. 1974. “Judgment Under\nUncertainty: Heuristics and Biases: Biases in Judgments Reveal Some\nHeuristics of Thinking Under Uncertainty.” Science 185\n(4157): 11241131. https://doi.org/10.1126/science.185.4157.1124.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. “How to Embrace\nVariation and Accept Uncertainty in Linguistic and Psycholinguistic Data\nAnalysis.” Linguistics 59 (5): 13111342. https://doi.org/10.1515/ling-2019-0051.\n\n\nVeenman, Myrthe, Angelika M. Stefan, and Julia M. Haaf. 2023.\n“Bayesian Hierarchical Modeling: An Introduction and\nReassessment.” Behavior Research Methods 56 (5):\n4600–4631. https://doi.org/10.3758/s13428-023-02204-3.\n\n\nVerissimo, Joao. 2021. “Analysis of Rating Scales: A Pervasive\nProblem in Bilingualism Research and a Solution with Bayesian Ordinal\nModels.” Bilingualism: Language and Cognition 24 (5):\n842848. https://doi.org/10.1017/S1366728921000316.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der\nMaas, and Rogier A. Kievit. 2012. “An Agenda for Purely\nConfirmatory Research.” Perspectives on Psychological\nScience 7 (6): 632638. https://doi.org/10.1177/1745691612463078.\n\n\nWicherts, Jelte M., Denny Borsboom, Judith Kats, and Dylan Molenaar.\n2006. “The Poor Availability of Psychological Research Data for\nReanalysis.” American Psychologist 61 (7): 726.\n\n\nWicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn,\nMarjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen.\n2016. “Degrees of Freedom in Planning, Running, Analyzing, and\nReporting Psychological Studies: A Checklist to Avoid p-Hacking.”\nFrontiers in Psychology 7. https://doi.org/10.3389/fpsyg.2016.01832.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science (2e). Second edition. https://r4ds.hadley.nz.\n\n\nWieling, Martijn. 2018. “Analyzing Dynamic Phonetic Data Using\nGeneralized Additive Mixed Modeling: A Tutorial Focusing on Articulatory\nDifferences Between L1 and L2 Speakers of English.” Journal\nof Phonetics 70: 86116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using\nr. Routledge.\n\n\n———. n.d. “Linear Models and Linear Mixed Effects Models in r with\nLinguistic Applications.”\n\n\nWinter, Bodo, and Paul-Christian Bürkner. 2021. “Poisson\nRegression for Linguists: A Tutorial Introduction to Modelling Count\nData with Brms.” Language and Linguistics Compass 15\n(11): e12439. https://doi.org/10.1111/lnc3.12439.\n\n\nWinter, Bodo, and Sven Grawunder. 2012. “The Phonetic Profile of\nKorean Formal and Informal Speech Registers.” Journal of\nPhonetics 40 (6): 808–15. https://doi.org/10.1016/j.wocn.2012.08.006.\n\n\nYarkoni, Tal. 2022. “The Generalizability Crisis.”\nBehavioral and Brain Sciences 45. https://doi.org/10.1017/s0140525x20001685.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "app-reg-cheat-sheet.html",
    "href": "app-reg-cheat-sheet.html",
    "title": "Appendix A — Regression models cheat sheet",
    "section": "",
    "text": "A.1 Step 0: Number of outcome variables\nRegression models, aka linear regression models or linear models, are a group of statistical models based on the simple idea that we can predict an outcome variable \\(Y\\) based on a function \\(f(X)\\). The “simplest” regression model is the formula of a line:1\n\\[\ny = \\alpha + \\beta x\n\\]\nwhere \\(\\alpha\\) is the intercept of the line and \\(\\beta\\) the slope. The principles behind this formula can be extended to represent virtually any other type of regression model, independent of the nature of the outcome variable(s) (\\(y\\)), the predictor(s), the types of relationship between outcome and predictor, and so on.\nThis means that if you master the principles of regression models, then you can virtually fit any kind of data using regression models. You can bid farewell to classical ANOVAs, \\(t\\)-tests, \\(\\chi^2\\)-tests, and what not. In fact, these can all be thought of as specific cases of regression models. It just so happens that they got themselves a specific name. But the underlying mechanics is the same. Same goes with “logistic regression”, “generalised regression models”, “mixed-effects regression” and so on. These are all regression models, so they all follow the same principles. And again, the fact that they got specific name is a historical “accident”.\nUnderstanding that these named models are in fact all regression models gives you super powers you can use on data (Sauron would be so jealous):\nEhm… perhaps this is not going to win a poetry context, but… the message is that with a single tool, i.e. regression models, you can go a long way!\nEach of the following sections asks you about the nature of your data and/or experimental design. By answering each, you will find out which “pieces” you need to add to your model structure.\nWe will get back to this step at the end of this post, since it makes things a bit more complex.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Regression models cheat sheet</span>"
    ]
  },
  {
    "objectID": "app-reg-cheat-sheet.html#sec-outcome",
    "href": "app-reg-cheat-sheet.html#sec-outcome",
    "title": "Appendix A — Regression models cheat sheet",
    "section": "A.2 Step 1: Choose a distribution for your outcome variable",
    "text": "A.2 Step 1: Choose a distribution for your outcome variable\nThe first step towards building a regression model is to choose the family of distributions you believe the outcome variable belongs to. You can start by answering the following question.\n\n\n\n\n\n\nTipQuestion 1\n\n\n\nIs the outcome variable continuous or discrete?\n\n\nDepending on the answer, check out Section A.2.1 or Section A.2.2.\n\nA.2.1 Continuous outcome variable\n\nThe variable can take on any positive and negative real number, including 0: Gaussian (aka normal) distribution.\n\nThere are very few truly Gaussian variables, although in some cases one can speak of “approximate” or “assumed” normality.\nThis family is fitted by default in lm(), lme4::lmer() and brms::brm(). You can explicitly select the family with family = gaussian.\n\nThe variable can take on any positive real number only: Log-normal distribution.\n\nDuration of segments, words, pauses, etc, are known to be log-normally distributed.\nReaction times can be modelled with a log-normal distribution.\nMeasurements taken in Hz (like f0, formants, centre of gravity, …) could be considered to be log-normal.\nThere are other families that could potentially be used depending on the nature of the variable: exponential-Gaussian (reaction times), gamma, …\nFit a log-normal model with brms::brm(..., family = lognormal).\n\nThe variable can take on any real number between 0 and 1, but not 0 nor 1: Beta distribution.\n\nProportions fall into this category (for example proportion of voicing within closure), although 0 and 1 are not allowed in the beta distribution.\nFit a beta model with brms::brm(..., family = Beta).\nCheck this tutorial: Coretta and Bürkner (2025).\n\nThe variable can take on any real number between 0 and 1, including 0 or 0 and 1: Zero-inflated or Zero/one-inflated beta (ZOIB) distribution.\n\nIf the proportion data includes many 0s and 1s, then this is the ideal distribution to use. ZOIB distributions are somewhat more difficult to fit than a simple beta distribution, so a common practice is to transform the data so that it doesn’t include 0s nor 1s (this can be achieved using different techniques, some better than others).\nFit a ZOIB model with brms::brm(..., family = zero_one_inflated_beta.\nCheck this tutorial: Heiss (2021).\n\n\n\n\nA.2.2 Discrete outcome variable\n\nThe variable is dichotomous, i.e. it can take one of two levels: Bernoulli distribution.\n\nCategorical outcome variables like yes/no, correct/incorrect, voiced/voiceless, follow this distribution.\nThis family is fitted by default when you run glm(..., family = binomial), aka “logistic regression” or “binomial regression” or with brms::brm(..., family = bernoulli).2\n\nThe variable is counts: Poisson distribution.\n\nCounts of words, segments, gestures, f0 peaks, …\nCheck out this tutorial: Winter and Bürkner (2021).\nFit a Poisson model with brms::brm(..., family = poisson).\nSometimes a negative binomial distribution is preferable, if the count data is dispersed. Fit this model with brms::brm(..., family = negbinomial).\n\nThe variable is a scale: ordinal linear model.\n\nLikert scales and ratings, language attitude questionnaires.\nFit ordinal regression models (aka ordinal logistic regression) with brms::brm(..., family = cumulative).\nSee these tutorials: Verissimo (2021), Bürkner and Vuorre (2019).\n\nThe variable has more than two levels, but it is not ordered: categorical (multinomial) model.\n\nFit categorical (multinomial) models with brms::brm(..., family = categorical).\nAs far as I know, there isn’t a tutorial for this family, but Lorson, Cummins, and Rohde (2021) uses categorical models. The research compendium (with data and code) of the paper can be found here: https://osf.io/e5av9/.\nJust a quick note: if you have an outcome variable with 3 levels (A, B and C) and you fit a categorical (multinomial) model, then \\(P(A) = \\frac{e^0}{e^0 + e^{B} + e^{C}}\\), where the superscripts \\(B\\) and \\(C\\) are the estimated log-odds difference of the B and C level vs the A level (these are the two intercepts in the model). This makes sense, because, assuming each level is equally probable, \\(\\frac{e^{0}}{1 + e^{0} + e^{0}} = 0.333\\) for all levels.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Regression models cheat sheet</span>"
    ]
  },
  {
    "objectID": "app-reg-cheat-sheet.html#step-2-are-there-hierarchical-groupings-andor-repeated-measures",
    "href": "app-reg-cheat-sheet.html#step-2-are-there-hierarchical-groupings-andor-repeated-measures",
    "title": "Appendix A — Regression models cheat sheet",
    "section": "A.3 Step 2: Are there hierarchical groupings and/or repeated measures?",
    "text": "A.3 Step 2: Are there hierarchical groupings and/or repeated measures?\nThe second step is to ensure that, if the data is structured hierarchically or repeated measures were taken, this is taken into account in the model. Here is where so-called varying terms (aka random effects, group-level effects/terms) come in (Gelman 2005). Models that include random effects/group-level terms are called: random-effects models, mixed-effects models, hierarchical models, nested models, multilevel models. These terms are for all intents and purposes equivalent (it just happens that different traditions use different terms).\nAs an example, let’s assume you asked a number of participants to read a list of words and each word was repeated 5 times by each participant. You then took f0 measurements from the stressed vowel of each word, of each repetition. Now, the data has a “hierarchical” structure to it:\n\nFirst, observations are grouped by participant (some observations belong to one participant and others to another and so on).\nSecond, observations are grouped by word (some observations belong to one word and others to another and so on).\nThird, within the observations of each word, some belong to the same participant (or, from a different perspective, within the observations of each participant, some belong to the same word).\n\nThe presence of “levels” within the data (whether they come from natural groupings like participant or word, or from repeated measures) breaks one of the assumptions of regression models: that each observation must be independent. This is why you must include varying terms in the regression model, to account for this structure (and now you see why they are called hierarchical and multilevel models). If you don’t include any varying term, your model will expect that each observation is independent and hence it will underestimate variance and return unreliable results.\nIn the toy-example of f0 measurements, you will want to include varying terms for participant and word. These will take care to let the model know of the structure of the data mentioned above. If you have other predictors in the model, you should also add them as (varying) slopes in the varying terms. For example: (question | participant) + (question | word) (where question = statement vs question).\nHere are some tutorials: Winter (n.d.), DeBruine and Barr (2021), Kirby and Sonderegger (2018), Bürkner (2018), Veenman, Stefan, and Haaf (2023), Pedersen et al. (2019).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Regression models cheat sheet</span>"
    ]
  },
  {
    "objectID": "app-reg-cheat-sheet.html#step-3-are-there-non-linear-effects",
    "href": "app-reg-cheat-sheet.html#step-3-are-there-non-linear-effects",
    "title": "Appendix A — Regression models cheat sheet",
    "section": "A.4 Step 3: Are there non-linear effects?",
    "text": "A.4 Step 3: Are there non-linear effects?\nA typical use-case of non-linear terms is when you are dealing with time-series data or spatial data (i.e. geographic coordinates). Generalised Additive Models (GAMs) allow you to fit non-linear effects using so called “smooth” (or “smoother”) terms. You can fit a regression model with smooth terms with brms::brm(y ~ s(x)) or with mgcv:gam(y ~ s(x)), among others. See Simpson (2018), Sóskuthy (n.d.), Sóskuthy (2021), Wieling (2018), Pedersen et al. (2019).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Regression models cheat sheet</span>"
    ]
  },
  {
    "objectID": "app-reg-cheat-sheet.html#step-0-bis-number-of-outcome-variables",
    "href": "app-reg-cheat-sheet.html#step-0-bis-number-of-outcome-variables",
    "title": "Appendix A — Regression models cheat sheet",
    "section": "A.5 Step 0-bis: Number of outcome variables",
    "text": "A.5 Step 0-bis: Number of outcome variables\nIf you want to model just one outcome variable, you are already covered if you went through steps 1-3. If instead your design has two or more outcome variables (for example F1 and F2, or duration of the stressed and unstressed vowel of a word) which you want to model together, then you want to fit a multivariate model (i.e. a model with multiple outcome variables). The same steps we went through before can be applied to multiple outcome variables. In some cases, you will want to use the same model structure for all the outcome variables, while in others you might want to use a different model structure for each.\nTo learn more about multivariate models, I really recommend Bürkner (2024).\n\n\n\n\nBürkner, Paul-Christian. 2018. “Advanced Bayesian Multilevel Modeling with the r Package Brms.” The R Journal 10 (1): 395411. https://doi.org/10.32614/RJ-2018-017.\n\n\n———. 2024. “Estimating Multivariate Models with Brms.” https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal Regression Models in Psychology: A Tutorial.” Advances in Methods and Practices in Psychological Science 2 (1): 77101. https://doi.org/10.1177/2515245918823199.\n\n\nCoretta, Stefano, and Paul-Christian Bürkner. 2025. “Bayesian Beta Regressions with Brms in r: A Tutorial for Phoneticians.” https://doi.org/10.31219/osf.io/f9rqg_v1.\n\n\nDeBruine, Lisa M., and Dale J. Barr. 2021. “Understanding Mixed-Effects Models Through Data Simulation.” Advances in Methods and Practices in Psychological Science 4 (1). https://doi.org/10.1177/2515245920965119.\n\n\nGelman, Andrew. 2005. “Analysis of Variance: Why It Is More Important Than Ever.” The Annals of Statistics 33 (1). https://doi.org/10.1214/009053604000001048.\n\n\nHeiss, Andrew. 2021. “A Guide to Modeling Proportions with Bayesian Beta and Zero-Inflated Beta Regression Models.” https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide.\n\n\nKirby, James, and Morgan Sonderegger. 2018. “Mixed-Effects Design Analysis for Experimental Phonetics.” Journal of Phonetics 70: 7085. https://doi.org/10.1016/j.wocn.2018.05.005.\n\n\nLorson, Alexandra, Chris Cummins, and Hannah Rohde. 2021. “Strategic Use of (Un)certainty Expressions.” Frontiers in Communication 6 (March): 635156. https://doi.org/10.3389/fcomm.2021.635156.\n\n\nPedersen, Eric J., David L. Miller, Gavin L. Simpson, and Noam Ross. 2019. “Hierarchical Generalized Additive Models in Ecology: An Introduction with Mgcv.” PeerJ 7: e6876. https://doi.org/10.7717/peerj.6876.\n\n\nSimpson, Gavin L. 2018. “Fitting GAMs with Brms: Part 1.” https://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/.\n\n\nSóskuthy, Márton. 2021. “Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis.” Journal of Phonetics 84: 101017. https://doi.org/10.1016/j.wocn.2020.101017.\n\n\n———. n.d. “Generalised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction.” https://doi.org/10.48550/arXiv.1703.05339.\n\n\nVeenman, Myrthe, Angelika M. Stefan, and Julia M. Haaf. 2023. “Bayesian Hierarchical Modeling: An Introduction and Reassessment.” Behavior Research Methods 56 (5): 4600–4631. https://doi.org/10.3758/s13428-023-02204-3.\n\n\nVerissimo, Joao. 2021. “Analysis of Rating Scales: A Pervasive Problem in Bilingualism Research and a Solution with Bayesian Ordinal Models.” Bilingualism: Language and Cognition 24 (5): 842848. https://doi.org/10.1017/S1366728921000316.\n\n\nWieling, Martijn. 2018. “Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: A Tutorial Focusing on Articulatory Differences Between L1 and L2 Speakers of English.” Journal of Phonetics 70: 86116. https://doi.org/10.1016/j.wocn.2018.03.002.\n\n\nWinter, Bodo. n.d. “Linear Models and Linear Mixed Effects Models in r with Linguistic Applications.”\n\n\nWinter, Bodo, and Paul-Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11): e12439. https://doi.org/10.1111/lnc3.12439.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Regression models cheat sheet</span>"
    ]
  },
  {
    "objectID": "app-reg-cheat-sheet.html#footnotes",
    "href": "app-reg-cheat-sheet.html#footnotes",
    "title": "Appendix A — Regression models cheat sheet",
    "section": "",
    "text": "Technically, the “simplest” regression model is \\(y = f(x)\\), but oh well…↩︎\nNote that, despite using family = binomial in glm(), under the hood a Bernoulli distribution is used.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Regression models cheat sheet</span>"
    ]
  }
]