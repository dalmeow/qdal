[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Data Analysis for Linguists in R",
    "section": "",
    "text": "Welcome!\nHello! Welcome to the Quantitative Data Analysis for Linguists in R textbook! This is the textbook for the Quantitative Methods in Linguistics and English Language course at the University of Edinburgh, but the textbook is open to all. Please, read the Preface to familiarise yourself with the pedagogical background and structure of the book.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Audience\nThis textbook is specifically designed for the students taking the Quantitative Methods in Linguistics and English Language (QML) course at the University of Edinburgh (UoE), but you don’t have to be enrolled in the course to work through it. This book is an introduction to quantitative methods and statistics in R for absolute beginners in the field of linguistics. No prior familiarity with quantitative methods, statistics or knowledge of R is expected, just a sense of adventure! Of course, the book can be helpful to researchers who have experience with statistics but want to learn about a more modern approach to statistical analysis like Bayesian statistics. Independent of your background, you will be exposed to several aspects of quantitative data analysis and R skills that can be applied and extended to many cases of data analysis in linguistics and related fields.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#justification-and-pedagogical-background",
    "href": "preface.html#justification-and-pedagogical-background",
    "title": "Preface",
    "section": "Justification and pedagogical background",
    "text": "Justification and pedagogical background\nThis book is a response to the need for a structured textbook that can fit into a one semester course and yet cover enough materials for an absolute beginner to be able to complete at least basic quantitative analyses. While there are many good books out there, they tend to focus on one aspect or the other, rather than covering all the necessary topics without assuming some prior knowledge. Examples of excellent textbooks are R for Data Science (R4DS, Wickham, Çetinkaya-Rundel, and Grolemund 2023) which covers the basics of R and data processing and visualisation; Statistics for linguists: an introduction using R (Winter 2020), Statistical rethinking: a Bayesian course with examples in R and Stan (McElreath 2020), and Introduction to Bayesian Data Analysis for Cognitive Science (Nicenboim, Schad, and Vasishth 2025), among many others, that focus on statistics with R. In fact, this textbook has taken a lot of inspiration from those books, and I am forever grateful to the authors for their fantastic work.\nHowever, the QML course in the Linguistics and English Language department at UoE is the only quantitative methods course in the department and the majority of students start as absolute beginners. The course must cover basic principles of research methods, some aspects of the philosophy of “science”, statistical concepts, and practical skills in R to run appropriate quantitative analyses. It is a lot to cover and with 9 weeks of teaching available, only the surface can be scratched, but scratched enough that by the end of the course you will feel comfortable taking a further step outside your comfort zone. So this textbook is not in any way meant to be exhaustive and it lives within the constraints of the specifics of the course it is intended to serve. However, where relevant, pointers to other resources will be given so that each reader can choose to focus on some aspects over others.\nAnother important point about this book is that, like the textbooks mentioned above, it moves away from the “traditional” (perhaps old fashion) way of doing statistics and instead it adopts a fresh take on quantitative data analysis which some have called the “New Statistics” (Cumming 2013; Kruschke and Liddell 2018). All of you view will be familiar with research papers in linguistics (whether you read them for a class or as part of your job as a researcher) and you will surely have encountered the (in)famous p-value and statements like “statistically significant”. These concepts belong to a particular way of doing statistics, called frequentist statistics, which has become ritualised into a set of cookbook recipes that we started to blindly follow (the “Null Ritual,” Gigerenzer 2004, 2018; Gigerenzer, Krauss, and Vitouch 2004). While (good) frequentist statistics is not bad in itself, the so-called Null Ritual has done a lot of damage, as you will learn in later chapters of this book. Because of these and other reasons, this book adopts a Bayesian approach to statistics, where instead of chasing after “significant p-values” we focus on a robust estimation of effects and patterns in the data. This is a bit of an oversimplification, but it should give you enough sense for where the textbook comes from. From a practical perspective, Bayesian statistics just works, even in those cases where the traditional way of doing statistics fails for one reason or the other. By learning a few building blocks of Bayesian statistics, you will be able to extend your skills to develop expertise in more advanced techniques, all within a coherent framework. You will of course learn about p-values and how (not) to interpret them, since a lot of current research is still carried out under the Null Ritualistic approach.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#book-structure",
    "href": "preface.html#book-structure",
    "title": "Preface",
    "section": "Book structure",
    "text": "Book structure\nThe book is structured according to the schedule of the QML course. Chapters are divided into “weeks” and the course will cover those topics weekly. If you are reading this book without being enrolled in the course, you are free to go through the chapters at your own pace! Not however that the chapters are written so that there is a certain progression of topics, and later chapters build on previous ones, so I recommend to start from the first chapter and if need be maybe read through the first chapters quickly if they cover things you are already familiar with, and start reading more intently when you hit a chapter that covers something new.\nEach chapter has “badges” that indicate the major topic area of the chapter. While some chapters focus on a specific area, others focus on more than one. These are the badges:\n\n is for chapters on research methods more generally, including research practices and philosophy.\n is for chapters that introduce statistical concepts without going necessarily into the details of how to do that in R.\n is for chapters that teach you how to use R to complete a particular task, like reading or plotting data, using statistical models or transform data.\n\nThe book also uses different types of “call-out boxes” to present specific content. Here are examples.\n\n\n\n\n\n\nDefinition or hint\n\n\n\nA green box contains definitions or hints to solve exercises.\n\n\n\n\n\n\n\n\nExercise or activity\n\n\n\nOrange boxes are for exercises or more general activities.\n\n\n\n\n\n\n\n\nQuiz, examples and summaries\n\n\n\nBlue boxes contain quizzes, examples or summaries. The title in the box will specify what type of content.\n\n\n\n\n\n\n\n\nR Note, Spotlight or solutions\n\n\n\nRed boxes called “R Note” explain something about R or contain R tips that don’t quite fit in the main text. Spotlight” boxes focus on statistical concepts, historical context or philosophy. Red boxes called “Solutions” are solutions to the exercises.\n\n\nThe textbook will teach how to use R. R is a programming language, meaning that you will have to write code which is executed and the results are returned to (as output in the R Console, as plots, tables, so on). R code in-text will look like this: \"this is R code\", while longer code chunks will look like this:\n# Sum two numbers!\n\na &lt;- 1 + 2\nprint(a)\nSometimes, when the R code is not that important, for example for certain plots, you will see a little grey triangle next to Code and if you click on it the code will be shown, like in the following example.\n\n\nCode\nlibrary(tidyverse)\nlibrary(glue)\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nrt_mean &lt;- mean(mald$RT)\nrt_sd &lt;- sd(mald$RT)\nrt_mean_text &lt;- glue(\"mean: {round(rt_mean)} ms\")\nrt_sd_text &lt;- glue(\"SD: {round(rt_sd)} ms\")\nx_int &lt;- 2000\n\nggplot(data = tibble(x = 0:300), aes(x)) +\n  geom_density(data = mald, aes(RT), colour = \"grey\", fill = \"grey\", alpha = 0.2) +\n  stat_function(fun = dnorm, n = 101, args = list(rt_mean, rt_sd), colour = \"#9970ab\", linewidth = 1.5) +\n  scale_x_continuous(n.breaks = 5) +\n  geom_vline(xintercept = rt_mean, colour = \"#1b7837\", linewidth = 1) +\n  geom_rug(data = mald, aes(RT), alpha = 0.1) +\n  annotate(\n    \"label\", x = rt_mean + 1, y = 0.0015,\n    label = rt_mean_text,\n    fill = \"#1b7837\", colour = \"white\"\n  ) +\n  annotate(\n    \"label\", x = x_int, y = 0.0015,\n    label = rt_sd_text,\n    fill = \"#8c510a\", colour = \"white\"\n  ) +\n  annotate(\n    \"label\", x = x_int, y = 0.001,\n    label = \"theoretical sample\\ndistribution\",\n    fill = \"#9970ab\", colour = \"white\"\n  ) +\n  annotate(\n    \"label\", x = x_int, y = 0.0003,\n    label = \"empirical sample\\ndistribution\",\n    fill = \"grey\", colour = \"white\"\n  ) +\n  labs(\n    title = \"Theoretical sample distribution of reaction times\",\n    subtitle = glue(\"Gaussian distribution: mean = {round(rt_mean)} ms, SD = {round(rt_sd)}\"),\n    x = \"RT (ms)\", y = \"Relative probability (density)\"\n  )\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\nCumming, Geoff. 2013. “The New Statistics: Why and How.” Psychological Science 25 (1): 729. https://doi.org/10.1177/0956797613504966.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The Journal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\n———. 2018. “Statistical Rituals: The Replication Delusion and How We Got There.” Advances in Methods and Practices in Psychological Science 1 (2): 198218. https://doi.org/10.1177/2515245918771329.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual. What You Always Wanted to Know about Significance Testing but Were Afraid to Ask.” In, 391408.\n\n\nKruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin & Review 25 (1): 178206. https://doi.org/10.3758/s13423-016-1221-4.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press.\n\n\nNicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025. Introduction to Bayesian Data Analysis for Cognitive Science. https://bruno.nicenboim.me/bayescogsci/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science (2e). Second edition. https://r4ds.hadley.nz.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch-research-methods.html",
    "href": "ch-research-methods.html",
    "title": "1  Research methods",
    "section": "",
    "text": "1.1 Empirical research\nResearch Methods are about the theory, methods and practice of conducting research. I like to call the discipline that deals with research methods Hodotics, but it hasn’t caught on yet. One way of categorising different aspects of research methods is represented in Figure 1.1. You can think of research methods as the combination of:\nLet’s zoom in on the research process, as represented in Figure 1.2.\nEmpirical research is one approach to research. This type of research focusses on learning about the Universe through data and observation.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-methods.html#empirical-research",
    "href": "ch-research-methods.html#empirical-research",
    "title": "1  Research methods",
    "section": "",
    "text": "Empirical research\n\n\n\nThe word empirical is related to experience, and in the context of research it basically means “based on experience (i.e. data and observation)”.\nLear more about the etymology of empirical here.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-methods.html#axes-of-research",
    "href": "ch-research-methods.html#axes-of-research",
    "title": "1  Research methods",
    "section": "1.2 Axes of research",
    "text": "1.2 Axes of research\n\n\n\n\n\n\nFigure 1.3: Research axes.\n\n\n\nThere are two main “axes” of empirical research types: exploratory vs corroboratory and descriptive vs explanatory.\n\n\n\n\n\n\nExploratory vs corroboratory research\n\n\n\n\nExploratory research is about exploring the data looking for patterns, associations, features and so on. This type of research is also known as “hypothesis generating” because exploration can lead to the formulation of new hypotheses.\nCorroboratory research (aka as confirmatory research) is about checking expectations against data. It is also known as “hypothesis testing” because it is about testing hypotheses using data.\n\n\n\nWhile there is still a lot of prejudice against exploratory research (typical sentiments are “it doesn’t have theory”) it is an important way of doing research, as recognised by important scholars. For example, Tukey (1980) stressed the importance of both approaches. The other axis of research is the descriptive/explanatory axis.\n\n\n\n\n\n\nDescriptive vs explanatory research\n\n\n\n\nDescriptive research is about describing facts through observation and collection of data. In other words, descriptive research is about the what.\nExplanatory research is about explaining facts, i.e. understanding why they are the way they are. In other words, explanatory research is about the why.\n\n\n\nSimilarly to the exploratory/corroboratory axis, there is still prejudice against descriptive research (again, typical sentiments are that “it doesn’t have theory”). Within linguistics, several scholars have shown that both descriptive and explanatory research are fundamental and that both need conceptual and methodological theories to function. Indeed, Dryer (2008) talks about descriptive theories and explanatory theories, granting both the same status.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-methods.html#research-objectives",
    "href": "ch-research-methods.html#research-objectives",
    "title": "1  Research methods",
    "section": "1.3 Research objectives",
    "text": "1.3 Research objectives\nOrthogonal to the two research axes from the previous section, we can classify research instances based on their objectives. There are in principle three types of research objectives: establishing facts, improving the fit of a framework to the facts, and comparing the fit of difference frameworks to the facts. Each has its merits and to improve our understanding of the Universe we need all three, although there is nothing wrong for any one study to focus just on one or two!\n\n\n\n\n\n\nEstablish facts\n\n\n\nResearch can establish facts and fill a gap in the knowledge of one or more phenomena.\nThe aim of establishing facts is to accumulate evidence of particular events, features, associations.\nExamples:\n\nWhat are the uses of the Sanskrit verb gam ‘to go’?\nWhat is the duration of vowels in Mawayana (Arawakan)?\nDo people interact with AI as with other people?\n\n\n\n\n\n\n\n\n\nImprove fit of framework to facts\n\n\n\nResearch can improve the fit of a specific framework to established facts. Usually this is done to fine-tune a framework in light of new evidence but it also just works when you want to test new expectations/hypotheses. When the facts do not match the expectations, researchers modify the framework to accommodate the results.\nIn some cases, a framework can be totally abandoned in light of the facts, or a new one could be developed.\nExamples:\n\nStrong exemplar-based models preclude the possibility of abstract representations, but certain categorisation tasks seem to involve abstract representations so these must be included in exemplar-based models.\n\n\n\n\n\n\n\n\n\nCompare fit of different frameworks to facts\n\n\n\nThis objective allows researcher two compare two or more frameworks in light of empirical results. The main prerequisite for this approach is that each framework must have different expectations in relation to the phenomenon at hand.\nWhen different frameworks entail different and exclusive hypotheses, one can test the hypotheses with data: the results might help excluding certain hypotheses and keep others. The frameworks that generate the excluded hypotheses have to be abandoned (unless they can be modified to fit the new results, see above, while still be different enough from other frameworks).\nExamples:\n\nThere are two possible models for the bilingual lexicon: Word association and concept mediation. Which one better describes and explains the data?\nA strict feed-forward architecture of grammar does not allow phonetic details to be sensitive to morphological structure, while some exemplar-based models allow that.\n\n\n\nEach of the three objectives are important in research, but note that in order to really advance our understanding of things the third objective is fundamental: it is only by directly comparing different frameworks that we can accumulate knowledge and weed out inaccurate explanations. Every time you read about a study, ask yourself which of these objectives the study is setting to address.\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nSelect the appropriate research types for the following study: Previous research showed that in several Euroasiatic languages, vowels followed by voiced consonants tend to be longer than vowels followed by voiceless consonants. We investigate this tendency in Quechua.\n\n Descriptive, exploratory. Descriptive, corroboratory. Explanatory, corroboratory.\n\nWhich of the following studies aims to improve the fit of a framework to the data? (Thanks to András Bárány for suggesting the second example)\n\n We set out to test whether gestural timing is affected by foot-structure (as per the foot-sensitivity hypothesis) or not (as per the segmental hypothesis). In particular, we expect V-to-V timing to be stable within but not across feet independent of intervening segments if the foot-sensitivity hypothesis holds, while the timing should be affected by intervening segments both within and across feet if the segmental hypothesis holds. According to one hypothesis, there is one operation, Agree, which assigns Case features to an argument (accusative to objects, in the example) and at the same time gets the argument's person, number, and gender features. This means that in an English sentence like John sees her, her gets accusative case from a functional head (v) and v in turn gets the object's features — these are not spelled out in English; there is never any object agreement. Other languages are different in this respect: for example, in Hungarian, all direct objects have accusative case and the verb can show object agreement but it doesn't always do so. So there are a few theoretical options: either in all of these languages Case and agreement happen but case is not always realised (in English, case is sometimes realised but agreement never is; in Hungarian, object case is always realised, but object agreement only sometimes is), or Agree is actually not both Case and feature-agreement at the same time.\n\n\n\n\n\n\n\n\nDryer, Matthew S. 2008. “Descriptive Theories, Explanatory Theories, and Basic Linguistic Theory.” In Catching Language: The Standing Challenge of Grammar Writing, edited by Felix K. Ameka, Alan Charles Dench, and Nicholas Evans. Vol. 167. Trends in Linguistics Studies and Monographs. Mouton De Gruyter.\n\n\nTukey, John W. 1980. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.2307/2682991.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research methods</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html",
    "href": "ch-research-context.html",
    "title": "2  Research context",
    "section": "",
    "text": "2.1 Research questions\nFigure 1.2 shows the main steps that compose the research process. The first component is the research context. Ellis and Levy (2008) discuss the research context and propose a convenient break-down of the concept. Figure 2.1 is a schematic representation of different aspects of the research context, from the most general to the most specific. An example of each is also provided.\nThe following sections treat research questions and research hypotheses in more detail.\nResearch questions are questions whose answers directly address the research problem. They take the form of actual questions. For example:\nResearch questions are always necessary, independent of the type and objective of the research. While there is an undue pressure on researcher to come up with “novel” research questions all the time, it is perfectly fine to ask the same question multiple times.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html#research-questions",
    "href": "ch-research-context.html#research-questions",
    "title": "2  Research context",
    "section": "",
    "text": "What is the average speech rate of adolescents vs that of older adults?\nWhat happens to infants syntactic processing when they move from a monolingual to a multilingual environment?\nIs the morphological complexity of languages spoken by larger populations different from that of languages spoken by smaller populations?",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html#research-hypotheses",
    "href": "ch-research-context.html#research-hypotheses",
    "title": "2  Research context",
    "section": "2.2 Research hypotheses",
    "text": "2.2 Research hypotheses\nResearch questions can be further developed into research hypotheses. Research hypotheses are statements (not questions) about the research problem. Hypotheses are never true nor confirmed. We can only corroborate hypothesis, and it’s a long term process. The same hypothesis has to be tested again and again, by multiple researchers in multiple contexts. Research is not a one-off matter: knowledge can only be acquired slowly and with a lot of effort. This idea has been beautifully synthesised into the “Slow Science” movement (Slow Science Academy 2010): “[Researchers] need time to think. [Researchers] need time to read, and time to fail. [Researchers] do not always know what it might be at right now.”\nIt is however perfectly fine to run a study with only research questions, without a research hypothesis. As long as you clearly state whether you are talking about research questions or research hypotheses and you don’t mix them up, you are fine.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-research-context.html#precision-and-testability",
    "href": "ch-research-context.html#precision-and-testability",
    "title": "2  Research context",
    "section": "2.3 Precision and testability",
    "text": "2.3 Precision and testability\nSolid research questions and hypotheses must have two main properties: they must be precise and testable. Precision is about the semantics of the words and phrases that make up the question or hypothesis. For example, in the question “Is the morphological complexity of languages spoken by larger populations different from that of languages spoken by smaller populations?” we need to clearly define the following: morphological complexity, larger population, smaller population. What do we mean by “morphological complexity”? How do we classify a population as large or small? For our research question to be a good research question, it is important that we think very hard about what we mean by those words. This is because depending on the specific meaning, we might obtain different outcomes and to be sure that the outcomes answer out specific research question we need to ensure that the question itself and the words within it are well defined.\nSecondly, research questions and hypotheses must be testable. Testability is about formulating research questions and hypotheses in an enough precise manner that naturally leads to a well defined, specific study design. For example, the testability of the hypothesis from Figure 2.1 would be compromised if we didn’t define “processing cost” precisely. For example, processing cost could be related to the cognitive load of processing the sentences, or to the number of “computational steps” needed to process the sentence, or to ease of the computational steps independent of their number. All of these aspects are strictly entangled with the researcher’s assumptions and favourite linguistic framework or model of sentence processing. Very often, “fast research” leads to hypotheses that look precise and testable on the surface, but they fail to hit the mark upon greater scrutiny. Lack of precision and testability undermines the robustness of research, as pointed out for example by Yarkoni (2022), Scheel (2022), Scheel et al. (2020), and Devezer et al. (2021).\n\n\n\n\n\n\nPrecision and testability\n\n\n\nResearch questions and hypotheses should be precise (all the components should be clearly defined) and testable (they clearly translate into a well-defined, specific study design).\n\n\nIt is difficult to come by precise and testable hypotheses in linguistics just because our current knowledge and understanding of Language and languages is limited. At best, we can normally come up with vague hypotheses that state whether a difference between two conditions is expected or not and, if we are lucky, the direction of the difference (i.e. “A is greater than B” or vice versa). This state of affairs makes testing hypotheses using statistical methods less straightforward, because of the non-straightforward mapping of (vague) hypotheses to statistical models.\n\n\n\n\n\n\nSpotlight: Falsificationism and falsifiability\n\n\n\n\n\nFalsification is a procedure proposed by philosopher Karl Popper in relation to the “problem of induction”. Induction is based on observations. Imagine you observe several swans over a long time period in the United Kingdom and they are all white. You induce that “all swans are white” and expect that to be true because you have never observed a swan that was not white. However, black swans do exist (they are native of Australia and New Zealand). You can see that it doesn’t matter how many white swans you observe in the UK, you cannot be certain of the truthfulness of the statement “all swans are white”. On the other hand, you only need see one single black swan to know that “all swans are white” is false. In other words, a statement can only ever be shown to be false, never to be true.\nSo, induction does not necessarily lead us to true statements, but falsification (observing even one case that makes the statement false) surely tells us which statements are false. A falsifiable statement or hypothesis should prevent us from wrongly accepting a false statement (but we can never know if it is true). John Spacey defines statement falsificability in his blog post Seven examples of falsifiability:\n\nA statement is falsifiable if it can be contradicted by an observation. If such observation is impossible to make with current technology, falsifiability is not achieved.\n\nSome examples of falsifiable hypotheses:\n\n“Life only exists on Earth.” (it would be falsified by the observation of life somewhere else).\n“If there is a 1st person exclusive dual, then there is also a 1st person inclusive dual.” [Universal 1871] (it would be falsified by the observation of languages with a 1st person exclusive dual but without the inclusive alternative).\n“Infants start uttering full sentences only after their 12th month of life.” (it would be falsified by the observation of infants uttering full sentences before their 12th month of life).\n\nThe following are some examples of non-falsifiable hypotheses:\n\n“Life might exist outside of the Solar system.” (if we observe life outside the Solar system or we don’t, the statement is still true, because of the might exist).\n“Languages with a 1st person inclusive dual can have a 1st person exclusive dual.” (whether we observe a language with both 1st inclusive and exclusive dual or not, the statement is still true, because of the can have.)\n\nFalsification has become a tenet of a lost of modern quantitative research and has become what could be regarded as falsificationism, but falsification is not the only approach to quantitative research, as you have learned in this chapter: precision and testability are two other equally valid criteria to follow when formulating hypotheses.\nIf you are interested in the philosophy behind research and statistics (commonly known as “philosophy of science”) I recommend the following books (of increasing length and depth): Okasha (2016), Dienes (2008), Rosenberg and Mclntyre (2020).\n\n\n\n\n\n\n\nDevezer, Berna, Danielle J. Navarro, Joachim Vandekerckhove, and Erkan Ozge Buzbas. 2021. “The Case for Formal Methodology in Scientific Reform.” Royal Society Open Science 8 (3): rsos.200805, 200805. https://doi.org/10.1098/rsos.200805.\n\n\nDienes, Zoltan. 2008. Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Macmillan International Higher Education.\n\n\nEllis, J. Timothy, and Yair Levy. 2008. “Framework of Problem-Based Research: A Guide for Novice Researchers on the Development of a Research-Worthy Problem.” Informing Science: The International Journal of an Emerging Transdiscipline 11: 1733. https://doi.org/10.28945/438.\n\n\nOkasha, Samir. 2016. Philosophy of Science: Very Short Introduction. Oxford: Oxford University Press. https://doi.org/10.1093/actrade/9780192802835.001.0001.\n\n\nRosenberg, Alexander, and Lee Mclntyre. 2020. Philosophy of science: a contemporary introduction. Fourth edition. Routledge contemporary introductions to philosophy. New York London: Routledge.\n\n\nScheel, Anne M. 2022. “Why Most Psychological Research Findings Are Not Even Wrong.” Infant and Child Development 31 (1): e2295. https://doi.org/10.1002/icd.2295.\n\n\nScheel, Anne M., Leonid Tiokhin, Peder M. Isager, and Daniël Lakens. 2020. “Why Hypothesis Testers Should Spend Less Time Testing Hypotheses.” Perspectives on Psychological Science 16 (4): 744–55. https://doi.org/10.1177/1745691620966795.\n\n\nSlow Science Academy. 2010. “The Slow Science Manifesto.” http://slow-science.org.\n\n\nYarkoni, Tal. 2022. “The Generalizability Crisis.” Behavioral and Brain Sciences 45. https://doi.org/10.1017/s0140525x20001685.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research context</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html",
    "href": "ch-quantitative.html",
    "title": "3  Quantitative data analysis",
    "section": "",
    "text": "3.1 Quantitative data analysis\nData analysis is anything that relates to analysing data, whether you collected it yourself or you used pre-existing data.\nThere are two main approaches to data analysis:\nNote that while it is common to talk about “quantitative vs qualitative data” in fact in most cases data can be conceived as both quantitative and qualitative. It is really how we approach the data that can be quantitative and/or qualitative. Moreover, these two approaches to data analysis are not necessarily opposite to each other and there are some aspects of each in each other. This will become clearer at the end of the course this textbook is written for.\nThis textbook focuses on quantitative data analysis. The rest of this chapter introduces fundamental concepts of quantitative methods.\nQuantitative analyses are usually comprised of three parts (these are not strictly distinct and the boundaries are sometimes blurred):\nSummary measures are numbers that represent certain properties of the data: common summary measures are the mean and the standard deviation. You will have frequently seen these in published paper, either in text or as a table. You will learn about summary measures in Chapter 10.\nPlots, or graphs, are another common way to summarise data but they are based on visual representation rather than single numbers. As the saying goes, “a picture is worth a thousand words”. The aim of plots is to make explicit certain patterns in the data. Choosing and designing plots that are effective and captivating is more of an art and you will learn the basics and heuristics of good (and bad plots) in Chapter 14.\nStatistical models are mathematical representations of patterns and relationship in data. Statistical modelling is a powerful tool to learn from the data or to assess research hypotheses. This textbook introduces you to a specific type of statistical models: regression models. These are highly flexible model that can be used with a variety of data types. You will start learning about statistical models in Chapter 21.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html#quantitative-data-analysis",
    "href": "ch-quantitative.html#quantitative-data-analysis",
    "title": "3  Quantitative data analysis",
    "section": "",
    "text": "Summarise data with summary measures.\nVisualise data with plots.\nModel data with statistical models.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html#the-computational-workflow",
    "href": "ch-quantitative.html#the-computational-workflow",
    "title": "3  Quantitative data analysis",
    "section": "3.2 The computational workflow",
    "text": "3.2 The computational workflow\n\n\n\n\n\n\nFigure 3.1: An overview of the computational workflow of quantitative data analysis from Wickham, Çetinkaya-Rundel, and Grolemund (2023). CC BY-NC-ND 3.0\n\n\n\nAnother way to look at quantitative data analysis is through its computational workflow. Figure 3.1 shows a typical workflow (Wickham, Çetinkaya-Rundel, and Grolemund 2023): you import data, you tidy data up (i.e. you reshape the data so that it is easy to work with), you transform it (i.e., you filter observations, change existing columns or create new ones, obtain summary measures and join data together), visualise it, apply statistical models and the communicate what you learned. Very often, transforming, visualising and modelling data is done iteratively, that is why these steps are shown in a loop in Figure 3.1, and together they form the “understanding” part of the process. Through the transform-visualise-model cycle, you understand things about the data. All of the steps in Figure 3.1 are surrounded by program: this is “computational programming”, in other words using the computer to execute those steps.\nYou will learn the basics of how to import (aka read) data in Chapter 9, transform it in Chapter 11 and Chapter 12, visualise it in Chapter 14 and Chapter 15, and model it from Chapter 19 onwards. However, you will find bits from any of these steps in many other chapters, so that you won’t have to learn everything at once.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-quantitative.html#numbers-have-no-meaning",
    "href": "ch-quantitative.html#numbers-have-no-meaning",
    "title": "3  Quantitative data analysis",
    "section": "3.3 Numbers have no meaning",
    "text": "3.3 Numbers have no meaning\nFinally, I should mention a more philosophical aspect of quantitative data analysis. As said above, both qualitative and quantitative approaches are valid and necessary to improve our understanding of things. Crucially, even a very complex quantitative analysis will always contain some qualitative aspects to it.\n\n\n\n\n\n\nThe numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.\n—Nate Silver, The Signal and the Noise\n\n\n\nThere’s a lot of wisdom in that quote. Numbers do not mean anything by themselves. We need to interpret numbers, “imbue them with meaning”, based on many aspects of research and beyond, including our own identity and positionality (Jafar 2018; Darwin Holmes 2020). Gelman and Hennig (2017) highlight how we should move away from concepts of “objectivity” and “subjectivity” as applied to statistics, and instead propose a broader collection of “virtues”. They say: “Instead of debating over whether a given statistical method is subjective or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize attributes such as transparency and acknowledgement of multiple perspectives as complementary” (Gelman and Hennig 2017, 973). The philosophical backdrop of this textbook (and its author) very much embody this sentiment.\n\n\n\n\nDarwin Holmes, Andrew Gary. 2020. “Researcher Positionality: A Consideration of Its Influence and Place in Qualitative Researcha New Researcher Guide.” Shanlax International Journal of Education 8 (4): 110. https://doi.org/10.34293/education.v8i4.3232.\n\n\nGelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and Objective in Statistics.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 180 (4): 9671033. https://doi.org/10.1111/rssa.12276.\n\n\nJafar, Anisa J. N. 2018. “What Is Positionality and Should It Be Expressed in Quantitative Studies?” Emergency Medicine Journal. https://doi.org/10.1136/emermed-2017-207158.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science (2e). Second edition. https://r4ds.hadley.nz.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantitative data analysis</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html",
    "href": "ch-r-rstudio.html",
    "title": "4  R basics",
    "section": "",
    "text": "4.1 Why R?\nR can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, map (GIS, Geographic Information System) data and even images.\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free for anyone to use!",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#r-vs-rstudio",
    "href": "ch-r-rstudio.html#r-vs-rstudio",
    "title": "4  R basics",
    "section": "4.2 R vs RStudio",
    "text": "4.2 R vs RStudio\nBeginners usually have trouble understanding the difference between R and RStudio. Let’s use a car analogy. What makes the car go is the engine and you can control the engine through the dashboard. You can think of R as an engine and RStudio as the dashboard.\n\n\n\n\n\n\n\nR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the related task is executed.\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\n\nRStudio is an Integrated Development Environment or IDE.\nIt helps you using R more efficiently.\nIt has a graphical user interface or GUI.\n\n\n\nThe next section will give you a tour of RStudio.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#rstudio-1",
    "href": "ch-r-rstudio.html#rstudio-1",
    "title": "4  R basics",
    "section": "4.3 RStudio",
    "text": "4.3 RStudio\nOpen RStudio on your computer and familiarise yourself with the different parts. When you open RStudio, you can see the window is divided into 3 panels:\n\nBlue (left): the Console.\nGreen (top-right): the Environment tab.\nPurple (bottom-right): the Files tab.\n\n\nThe Console is where R commands can be executed. Think of this as the interface to R. Now, try to run (execute) some R code in the console:\n\n\n\n\n\n\nExercise 1\n\n\n\n\nWrite the following code in the Console:\nsum(3, 1, 4)\nPress ENTER/RETURN on your keyboard. This will run (i.e. execute) the code. The code sums the numbers 3, 1, and 4.\nThe output of the code is shown below it, in the Console. (Never mind the [1] for now).\n\n\n\nThe Environment tab lists the objects created with R, while in the Files tab you can navigate folders on your computer to get to files and open them in the file Editor.\n\n4.3.1 RStudio and Quarto projects\nRStudio is an IDE (see above) which allows you to work efficiently with R, all in one place. Note that files and data live in folders on your computer, outside of RStudio: do not think of RStudio as an app where you can save files in. All the files that you see in the Files tab are files on your computer and you can access them from the Finder or File Explorer as you would with any other file.\nIn principle, you can open RStudio and then navigate to any folder or file on your computer. However, there is a more efficient way of working with RStudio: RStudio and Quarto Projects.\n\n\n\n\n\n\nProjects\n\n\n\nAn RStudio Project is a folder on your computer that has an .Rproj file.\nA Quarto Project is an RStudio Project with a _quarto.yml file.\n\n\nYou can create as many Quarto Projects as you wish, and I recommend to create one per project (your dissertation, a research project, a course, etc…). We will create a Quarto Project for this course (meaning, you will create a folder for the course which will be the Quarto Project). You will have to use this project/folder throughout the semester.\nTo create a new Quarto Project, click on the button that looks like a transparent light blue box with a plus, in the top-left corner of RStudio. A window like the one below will pop up.\n\nClick on New Directory then Quarto Project.\n\nNow, this will create a new folder (aka directory) on your computer and will make that a Quarto Project (meaning, it will add a file with the .Rproj extension and a file called _quarto.yml to the folder; the name of the .Rproj file will be the name of the project/folder).\nGive a name to your new project, something like the name of the course and year (e.g. qml-2025).\nThen you need to specify where to create this new folder/Project. Click on Browse… and navigate to the folder you want to create the new folder/Project in. This could be your Documents folder, or the Desktop (we had issues with OneDrive in the past, so we recommend you save the project outside of OneDrive).\nWhen done, click on Create Project. RStudio will automatically open your new project.\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through this textbook, always make sure you are in the Quarto Project you just created for the course.\nYou know you are in an RStudio/Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light blue cube icon.\nIf you see Project (none) in the top-right corner, that means your are not in a Quarto Project.\nAn easy way to ensure that RStudio is opened from within a specific project, to open RStudio go to the project folder in File Explorer or Finder and double click on the .Rproj file. This will automatically open RStudio and set the project to that folder.\n\n\nThere are several ways of opening a Quarto Project:\n\nYou can go to the Quarto Project folder in Finder or File Explorer and double click on the .Rproj file.\nYou can click on File &gt; Open Project in the RStudio menu.\nYou can click on the project name in the top-right corner of RStudio, which will bring up a list of projects. Click on the desired project to open it.\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nClose RStudio.\nNow re-open RStudio by double-clicking on the .Rproj file of the project you created.\n\n\n\n\n\n4.3.2 A few important settings\nBefore moving on, there are a few important settings that you need to change.\n\n\nOpen the RStudio preferences (Tools &gt; Global options...).\nUn-tick Restore .RData into workspace at startup.\n\nThis mean that every time you start RStudio you are working with a clean Environment. Not restoring the workspace ensures that the code you write is fully reproducible. When this setting is enabled, your environment is saved in a hidden file called .Rdata and loaded every time you start RStudio. This very frequently leads to errors where the wrong variables/data is read or used by the code without you noticing, so always make sure the setting is disabled.\n\nSelect Never in Save workspace to .RData on exit.\n\nSince we are not restoring the workspace at startup, we don’t need to save it. Remember that as long as you save the code, you will not lose any of your work! You will learn how to save code from next week.\n\nClick OK to confirm the changes.\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nTrue or false?\n\nRStudio executes the code. TRUEFALSE\nR is a programming language. TRUEFALSE\nAn IDE is necessary to run R. TRUEFALSE\nRStudio projects are folders with an .Rproj file. TRUEFALSE\nQuarto projects can’t be RStudio projects TRUEFALSE\nThe project name is shown in the top-right corner of RStudio. TRUEFALSE\nI have disabled Restore .RData and Save workspace in the settings. TRUEFALSE",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#r-basics",
    "href": "ch-r-rstudio.html#r-basics",
    "title": "4  R basics",
    "section": "4.4 R basics",
    "text": "4.4 R basics\nIn this part of the tutorial you will learn the very basics of R. If you have prior experience with programming, you should find all this familiar. If not, not to worry! Make sure you understand the concept highlighted in the green boxes and practice the related skills.\nIn the following sections, you should just run code directly in the R Console in RStudio, i.e. you will type code in the Console and press ENTER to run it.\nIn later chapters, you will learn how to save your code in a script file or in Quarto documents, so that you can keep track of which code you have run and make your work reproducible.\n\n4.4.1 R as a calculator\nWrite this code 1 + 2 in the Console, then press ENTER/RETURN to run the code. Fantastic! You should see that the answer of the addition has been printed in the Console, like this:\n[1] 3\n(Never mind the [1] for now).\nNow, try some more operations (write each of the following in the Console and press ENTER). Feel free to add your own operations to the mix!\n\n67 - 13\n2 * 4\n268 / 43\n\nYou can also chain multiple operations.\n\n6 + 4 - 1 + 2\n4 * 2 + 3 * 2\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nAre the following pairs of operations equivalent?\n\n3 * 2 / 4 = 3 * (2 / 4) TRUEFALSE\n10 * 2 + 5 * 0.2 = (10 * 2 + 5) * 0.2 TRUEFALSE\n\n\n\n\n\n\n\n\n\nSpotlight: Arithmetics\n\n\n\n\n\nIf you need a maths refresher, I recommend checking the following pages:\n\nhttps://www.mathsisfun.com/definitions/order-of-operations.html\nhttps://www.mathsisfun.com/algebra/introduction.html\n\n\n\n\n\n\n4.4.2 Variables\n\nForget-me-not.\n\nMost times, we want to store a certain value so that we can use it again later.\nWe can achieve this by creating variables.\n\n\n\n\n\n\nVariable\n\n\n\nA variable holds one or more values and it’s stored in the computer memory for later use.\n\n\nYou can create a variable by using the assignment operator &lt;-.\nLet’s assign the value 156 to the variable my_num.\n\nmy_num &lt;- 156\n\nNow, check the list of variables in the Environment tab of the top-right panel of RStudio. You should see the my_num variable and its value there.\nNow, you can just call the variable back when you need it! Write the following in the Console and press ENTER.\n\nmy_num\n\n[1] 156\n\n\nA variable like my_num is also called a numeric vector: i.e. a vector that contains a number (hence numeric).\n\n\n\n\n\n\nVector\n\n\n\nA vector is an R object that contains one or more values of the same type.\n\n\nA vector is a type of variable and a numeric vector is a type of vector. However, it’s fine in most cases to use the word variable to mean vector (just note that a variable can also be something else than a vector; you will learn about other R objects in later chapters).\nLet’s now try some operations using variables.\n\nincome &lt;- 1200\nexpenses &lt;- 500\nincome - expenses\n\n[1] 700\n\n\nSee? You can use operations with variables too! And you can also go all the way with variables.\n\nsavings &lt;- income - expenses\n\nAnd check the value…\n\nsavings\n\n[1] 700\n\n\nVectors can hold more than one item or value. Just use the combine c() function to create a vector containing multiple values. The following are all numeric vectors.\n\none_i &lt;- 6\n# Vector with 2 values\ntwo_i &lt;- c(6, 8)\n# Vector with 3 values\nthree_i &lt;- c(6, 8, 42)\n\nCheck the list of variables in the Environment tab. You will see now that before the values of two_i and three_i you get the vector type num for numeric. (If the vector has only one value, you don’t see the type in the Enviroment list but it is still of a specific type).\n\n\n\n\n\n\nNumeric vector\n\n\n\nA numeric vector is a vector that holds one or more numeric values.\n\n\nNote that the following are the same:\n\none_i &lt;- 6\none_i\n\n[1] 6\n\none_ii &lt;- c(6)\none_ii\n\n[1] 6\n\n\nAnother important aspect of variables is that they are… variable! Meaning that once you assign a value to one variable, you can overwrite the value by assigning a new one to the same variable.\n\nmy_num &lt;- 88\nmy_num &lt;- 63\nmy_num\n\n[1] 63\n\n\n\n\n\n\n\n\nQuiz 3\n\n\n\nTrue or false?\n\nA vector is a type of variable. TRUEFALSE\nNot all variables are vectors. TRUEFALSE\nA numeric vector can only hold numeric values. TRUEFALSE\n\n\n\n\n\n4.4.3 Functions\n\nR cannot function without… functions.\n\n\n\n\n\n\n\nFunction\n\n\n\nA function usually runs an operation on one or more specified arguments.\n\n\nA function in R has the form function() where:\n\nfunction is the name of the function, like sum.\n() are round parentheses, inside of which you write arguments, separated by commas.\n\nLet’s see an example:\n\nsum(3, 5)\n\n[1] 8\n\n\nThe sum() function sums the number listed as arguments. Above, the arguments are 3 and 5.\nAnd of course arguments can be vectors!\n\nmy_nums &lt;- c(3, 5, 7)\n\nsum(my_nums)\n\n[1] 15\n\nmean(my_nums)\n\n[1] 5\n\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nTrue or false?\n\nFunctions can take other functions as arguments. TRUEFALSE\nAll function arguments must be specified. TRUEFALSE\nAll functions need at least one argument. TRUEFALSE\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n4c\nThe Sys.Date() function and other functions like it don’t take any arguments.\n\n\n\n\n\n\n\n\n\nR Note: R vs Python\n\n\n\n\n\nIf you are familiar with Python, you will soon realise that R and Python, although they share many concepts and types of objects, they can differ substantially. This is because R is a functional programming language (based on functions) while Python is an Object Oriented programming language (based on methods applied on objects).\nGenerally speaking, functions look like print(x) while methods look like x.print()\n\n\n\n\n\n4.4.4 String and logical vectors\n\nNot just numbers.\n\nWe have seen that variables can hold numeric vectors. But vectors are not restricted to being numeric. They can also store strings. A string is basically a set of characters (a word, a sentence, a full text). In R, strings have to be quoted using double quotes \" \".\nChange the following strings to your name and surname. Remember to use the double quotes.\n\nname &lt;- \"Stefano\"\nsurname &lt;- \"Coretta\"\n\nname\n\n[1] \"Stefano\"\n\nsurname\n\n[1] \"Coretta\"\n\n\nStrings can be used as arguments in functions, like numbers can.\n\ncat(\"My name is\", name, surname)\n\nMy name is Stefano Coretta\n\n\nRemember that you can reuse the same variable name to override the variable value.\n\nname &lt;- \"Raj\"\n\ncat(\"My name is\", name, surname)\n\nMy name is Raj Coretta\n\n\nYou can combine multiple strings into a character vector, using the combine function c() (the function works with any type of vectors, not only characters!).\n\n\n\n\n\n\nCharacter vector\n\n\n\nA character vector is a vector that holds one or more strings.\n\n\n\nfruit &lt;- c(\"apple\", \"oranges\", \"bananas\")\nfruit\n\n[1] \"apple\"   \"oranges\" \"bananas\"\n\n\nCheck the Environment tab. Character vectors have chr before the list of values.\nAnother type of vector is one that contains either TRUE or FALSE. Vectors of this type are called logical vectors and they are listed as logi in the Environment tab.\n\n\n\n\n\n\nLogical vector\n\n\n\nA logical vector is a vector that holds one or more TRUE or FALSE values.\n\n\n\ngroceries &lt;- c(\"apple\", \"flour\", \"margarine\", \"sugar\")\nin_pantry &lt;- c(TRUE, TRUE, FALSE, TRUE)\n\ndata.frame(groceries, in_pantry)\n\n\n  \n\n\n\nTRUE and FALSE values must be written in all capitals and without double quotes (they are not strings!).\n(We will talk about data frames, another type of object in R, in the following chapters.)\n\n\n\n\n\n\nQuiz 5\n\n\n\n\nWhich of the following is not a character vector.\n\n c(1, 2, \"43\") \"s\" c(apple) (assuming apple &lt;- 45) c(letters)\n\nWhich of the following is not a logical vector.\n\n c(T, T, F) TRUE \"FALSE\" c(FALSE)\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the class() function to check the type (“class”) of a vector.\n\nclass(FALSE)\n\n[1] \"logical\"\n\nclass(c(1, 45))\n\n[1] \"numeric\"\n\nclass(c(\"a\", \"b\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n5a\n\nc(1, 2, \"43\") is a character vector because the last number \"43\" is a string (it’s between double quotes!). A vector cannot have a mix of types of elements: they have to be all numbers or all strings or else, but not some numbers and some strings. Numbers are special in that if you include a number in a character vector without quoting it, it is automatically converted into a string. Try the following:\n\n\nchar &lt;- c(\"a\", \"b\", \"c\")\nchar &lt;- c(char, 1)\nchar\nclass(char)\n\n\nc(letters) is a character vector because letters contains the letters of the alphabet as strings (this vector comes with base R).\nc(apple) is not a character vector because the variable apple holds a number, 45!\n\n5b\n\n\"FALSE\" is not a logical vector because FALSE has been quoted (anything that is quoted is a string!).\n\n\n\n\n\n\n\n\n\n\n\n\nR Note: For-loops and if-else statements\n\n\n\n\n\nThis course does not cover programming in R in the strict sense, but if you are curious here’s a short primer on for-loops and if-else statements in R.\nFor-loops\n\nfruits &lt;- c(\"apples\", \"mangos\", \"durians\")\n\nfor (fruit in fruits) {\n  cat(\"I like\", fruit, \"\\n\")\n}\n\nI like apples \nI like mangos \nI like durians \n\n\nIf-else\n\nfor (fruit in fruits) {\n  if (grepl(\"n\", fruit)) {\n    cat(fruit, \"has an 'n'\", \"\\n\")\n  } else {\n    cat(fruit, \"does not have an 'n'\", \"\\n\")\n  }\n}\n\napples does not have an 'n' \nmangos has an 'n' \ndurians has an 'n' \n\n\nFor more, check the For loops section of the R4DS book and the R if else statement post from DataMentor.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-r-rstudio.html#summary",
    "href": "ch-r-rstudio.html#summary",
    "title": "4  R basics",
    "section": "4.5 Summary",
    "text": "4.5 Summary\nYou made it! You completed this chapter.\nHere’s a summary of what you learned.\n\n\n\n\n\n\n\nR is a programming language while RStudio is an IDE.\nRStudio projects are folders with an .Rproj file (you can see the name of the project you are currently in in the top-right corner of RStudio).\nYou can perform mathematical operations with +, -, *, /.\nYou can store values in variables.\nA typical object to be stored in a variable is a vector: there are different type of vectors, like numeric, character and logical.\nFunctions are used to perform an operation on its arguments: sum() sums it’s arguments, mean() calculates the mean and cat() prints the arguments.\n\n\n\n\n\n\n\n\n\n\nR Note: Programming in R\n\n\n\n\n\nIf you are interested in learning about programming in R, I recommend you go through Chapters 26-28 of the R4DS book and the Advanced R book.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "ch-packages.html",
    "href": "ch-packages.html",
    "title": "5  R packages",
    "section": "",
    "text": "5.1 Install packages\nWhen you install R, a library of packages is also installed. Packages provide R with extra functionalities, usually by making extra functions available for use. You can think of packages as “plug-ins” that you install once and then you can “activate” them when you need them. The library installed with R contains a set of packages that are collectively known as the base R packages, but you can install more any time!\nNote that the R library is a folder on your computer. Packages are not installed inside RStudio. Remember that RStudio is just an interface.\nYou can check all of the currently installed packages in the bottom-right panel of RStudio, in the Packages tab. There you can also install new packages.\nYou can install extra packages in the R library in two ways:\nGo ahead and try to install a package using the second method. Install the cowsay and the fortunes packages (see picture above for how to write the packages). After installing you will see that the package fortunes is listed in the Packages tab.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-packages.html#install-packages",
    "href": "ch-packages.html#install-packages",
    "title": "5  R packages",
    "section": "",
    "text": "You can use the install.packages() function. This function takes the name of the package you want to install as a string, for example install.packages(\"cowsay\").\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you install a package with the function install.packages(), do so in the Console! You will start using R scripts soon, so please do not include this function in your scripts (this is because you install packages only once, see below).\n\n\n\nOr you can go the Packages tab in the bottom-right panel of RStudio and click on Install. A small window will pop up. See the screenshot below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall packages\n\n\n\nTo install packages, go to the Packages tab of the bottom-right panel of RStudio and click on Install.\nIn the “Install packages” window, list the package names and then click Install.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to install a package ONLY ONCE! Once installed, it’s there for ever, saved in the R library. You will be able to use all of your installed packages in any RStudio/Quarto project you create.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-packages.html#attaching-packages",
    "href": "ch-packages.html#attaching-packages",
    "title": "5  R packages",
    "section": "5.2 Attaching packages",
    "text": "5.2 Attaching packages\nNow, to use a package you need to attach the package to the current R session with the library() function. Attaching a package makes the functions that come with the package available to us.\n\n\n\n\n\n\nImportant\n\n\n\nYou need to attach the packages you want to use once per R session.\nNote that every time you open RStudio, a new R session is started.\n\n\nLet’s attach the cowsay and fortunes packages. Run the following code in the Console.\n\nlibrary(cowsay)\nlibrary(fortunes)\n\nNote that library(cowsay) takes the name of the package without quotes, although if you put the name in quotes it also works. You need one library() function per package (there are other ways, but we will stick with this one).\n\n\n\n\n\n\nAttaching packages\n\n\n\nPackages are attached with the library(pkg.name) function, where pkg.name is the name of the package.\n\n\nNow you can use the functions provided by the attached packages. Try out the say() function from the cowsay package.\n\nsay(\"hot diggity\", \"frog\")\n\n(I know, the usefulness of the package might be questionable, but it is fun!)\n\n\n\n\n\n\nImportant\n\n\n\nRemember, you need to install a package only once but you need to attach it with library() every time you start R.\nThink of install.packages() as mounting a light bulb (installing the package) and library() as the light switch (attaching the package).",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-packages.html#package-documentation",
    "href": "ch-packages.html#package-documentation",
    "title": "5  R packages",
    "section": "5.3 Package documentation",
    "text": "5.3 Package documentation\nTo learn what a function does, you can check its documentation by typing in the Console the function name preceded by a ? question mark. Type ?say in the Console and hit ENTER to see the function documentation. You should see something like this:\n\n\n\n\n\nThe Description section is usually a brief explanation of what the function does.\nIn the Usage section, the usage of the function is shown by showing which arguments the function has and which default values (if any) each argument has. When the argument does not have a default value, NULL is listed as the value.\nThe Arguments section gives a thorough explanation of each function argument. (Ignore … for now).\nHow many arguments does say() have? How many arguments have a default value?\nDefault argument values allow you to use the function without specifying those arguments. Just write say() in your script on a new line and run it. Does the output make sense based on the Usage section of the documentation?\nThe rest of the function documentation usually has further details, which are followed by Examples. It is always a good idea to look at the example and test them in the Console when learning new functions.\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhich of the following statements is wrong?\n\n You attach libraries with library().\n install.packages() does not load packages.\n The R library is a folder.\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThis was a question about terminology. In R, you attach packages from the library using (confusingly) the library() function.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "ch-inference.html",
    "href": "ch-inference.html",
    "title": "6  Inference",
    "section": "",
    "text": "6.1 Uncertainty and variability\nImbuing numbers with meaning is a good characterisation of the inference process. Here is how it works. We have a question about something. Let’s imagine that this something is the population of British Sign Language signers. We want to know whether the cultural background of the BSL signers is linked to different pragmatic uses of the sign for BROTHER. But we can’t survey the entire population of BSL signers. So instead of surveying all BSL users, we take a sample from the BSL population. The sample is our data (the product of our study or observation). Now, how do we go from data/observation to answering our question about the use of BROTHER? We can use the inference process!\nThe figure below is a schematic representation of the inference process.\nThe inference process has two main stages: producing data and inference. For the first step, producing data, we start off with a population. Note that population can be a set of anything, not just a specific group of people. For example, the words in a dictionary can be a “population”; or the antipassive constructions of Austronesian languages, and so on. From that population, we select a sample and that sample produces our data. We analyse the data to get results. Finally, we use inference to understand something about the population based on the results from the sampled data. Inference can take many forms and the type of inference we are interested in here is statistical inference: i.e. using statistics to do inference.\nHowever, despite inference being based on data, it does not guarantee that the answers to our questions are right or even that they are true. In fact, any observation we make comes with a certain degree of uncertainty and variability.\nPliny the Elder was a Roman philosopher who died in the Vesuvius eruption in 79 CE. He certainly did not expect to die then. Leaving dark irony aside, as researchers we have to deal with uncertainty and variability.\nSo uncertainty is a feature of each mesurement, while variability occurs between different measurements. Together, uncertainty and variability render the inference process more complex and can interfere with its outcomes.\nThe following picture is a reconstruction of what Galileo Galilei saw when he pointed one of his first telescopes towards Saturn, based on his 1610 sketch: a blurry circle flanked by two smaller blurry circles.\nOnly six years later, telescopes were much better and Galileo could correctly identify that the flaking circles were not spheres orbiting around Saturn, but rings. The moral of the story is that at any point in history we are like Galileo in at least some of our research: we might be close to understanding something but not quite yet. So what do we do with such uncertainty and variability? We can use statistics to quantify them!\nBut what is statistics exactly?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#uncertainty-and-variability",
    "href": "ch-inference.html#uncertainty-and-variability",
    "title": "6  Inference",
    "section": "",
    "text": "Uncertainty and variability\n\n\n\n\nUncertainty is a characteristic of each observation of a phenomenon, due to measurement error or because we cannot directly measure what we want to measure.\nVariability is found among different observations of the same phenomenon, due to natural fluctuations and measurement error.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nStatistics is a tool that helps us quantifying uncertainty and controlling for variability.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#what-is-statistics-and-isnt",
    "href": "ch-inference.html#what-is-statistics-and-isnt",
    "title": "6  Inference",
    "section": "6.2 What is statistics (and isn’t)?",
    "text": "6.2 What is statistics (and isn’t)?\nStatistics is a tool. But what does it do? There are at least four ways of looking at statistics as a tool.\n\nStatistics is the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data. (From UCI Department of Statistics)\nStatistics is the technology of extracting information, illumination and understanding from data, often in the face of uncertainty. (From the British Academy)\nStatistics is a mathematical and conceptual discipline that focuses on the relation between data and hypotheses. (From the Standford Encyclopedia of Philosophy)\nStatistics is the art of applying the science of scientific methods. (From ORI Results, Nature)\n\nTo quote a historically important statistician:\n\n\n\n\n\n\nStatistic is both a science and an art.\nIt is a science in that its methods are basically systematic and have general application and an art in that their successful application depends, to a considerable degree, on the skill and special experience of the statistician, and on his knowledge of the field of application.\n—L. H. C. Tippett\n\n\n\n\n\n\n\n\n\nSpotlight: Etymology\n\n\n\n\n\nThe word statistics is related to state and it is no coincidence. The discipline of statistics was born as a sub-field of politics and economics. Check out the full etymology of statistics here: https://en.wiktionary.org/wiki/statistics#Etymology_1.\n\n\n\nStatistics is a many things, but it is also not a lot of things.\n\nStatistics is not maths, but it is informed by maths.\nStatistics is not about hard truths not how to seek the truth.\nStatistics is not a purely objective endeavour. In fact there are a lot of subjective aspects to statistics (see below).\nStatistics is not a substitute of common sense and expert knowledge.\nStatistics is not just about \\(p\\)-values and significance testing.\n\nAs Gollum would put it, all that glisters is not gold.\n\n\n\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nTrue or false?\n\nStatistics is a necessary if one wants to know the truth. TRUEFALSE\nStatistics is only relevant to objective science. TRUEFALSE\nStatistics is based on mathematics but it is also informed by philosophy. TRUEFALSE\nWe can completely remove uncertainty with statistics. TRUEFALSE",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#many-analysts-one-data-set-subjectivity-exposed",
    "href": "ch-inference.html#many-analysts-one-data-set-subjectivity-exposed",
    "title": "6  Inference",
    "section": "6.3 Many Analysts, One Data Set: subjectivity exposed",
    "text": "6.3 Many Analysts, One Data Set: subjectivity exposed\nIn Silberzahn et al. (2018), a group of researchers asked 29 independent analysis teams to answer the following question based on provided data: Is there a link between player skin tone and number of red cards in soccer? Crucially, 69% of the teams reported an effect of player skin tone, and 31% did not. In total, the 29 teams came up with 21 unique types of statistical analysis. These results clearly show how subjective statistics is and how even a straightforward question can lead to a multitude of answers. To put it in Silberzah et al’s words: “The observed results from analyzing a complex data set can be highly contingent on justifiable, but subjective, analytic decisions. This is why you should always be somewhat sceptical of the results of any single study: you never know what results might have been found if another research team did the study. This is a reason for why replicating research is very important. You will learn about replication and related concepts in Chapter 16.\nCoretta et al. (2023) tried something similar, but in the context of the speech sciences: they asked 30 independent analysis teams (84 signed up, 46 submitted an analysis, 30 submitted usable analyses) to answer the question: Do speakers acoustically modify utterances to signal atypical word combinations? Outstandingly, the 30 teams submitted 109 individual analyses—a bit more than 3 analyses per team!—and 52 unique measurement specifications in 47 unique model specifications. Coretta et al. (2023) say: “Nine teams out of the thirty (30%) reported to have found at least one statistically reliable effect (based on the inferential criteria they specified). Of the 170 critical model coefficients, 37 were claimed to show a statistically reliable effect (21.8%).” Figure 6.1 illustrates the analytic flexibility typical of acoustic analyses. (A) shows the pipeline of decision a researcher would have to do: which linguistic unit, which temporal window, which acoustic parameters and how to measure those. You can appreciate that there are potentially many combinations. (B) illustrates the fundamental frequency (f0) contour of the sentences “I can’t bear ANOTHER meeting on Zoom” and “I can’t bear another meeting on ZOOM”. In both sentences, the green shaded area marks the word “another”. Finally, in (C) you see the different parameters that can be extracted from the f0 contour of the word “another”. In sum, there are many choices a researcher is faced with and, while most of these choices might be justifiable, they are still subjective, as shows by the large variability of actual analyses carried out by the analyses teams in Coretta et al. (2023).\n\n\n\n\n\n\nFigure 6.1: Illustration of the analytic flexibility associated with acoustic analyses (from Coretta et al. 2023)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-inference.html#the-new-statistics",
    "href": "ch-inference.html#the-new-statistics",
    "title": "6  Inference",
    "section": "6.4 The “New Statistics”",
    "text": "6.4 The “New Statistics”\nThe Silberzahn et al. (2018) and Coretta et al. (2023) studies are just the tip of the iceberg. We are currently facing a “research crisis”. As mentioned above, we will dig deeper into this subject in Chapter 16. In brief, the research crisis is a mix of problems related to how research is conducted and published. In response to the research crisis, Cumming (2013) introduced a new approach to statistics, which he calls the “New Statistics”. The New Statistics mainly addresses three problems: (1) published research is a biased selection of all (existing and possible) research; (2) data analysis and reporting are often selective and biased, (3) in many research fields, studies are rarely replicated, so false conclusions persist. To help solve those problems, the New Statistics proposes these solutions (among others): (1) promoting research integrity, by which researchers explicitly discuss the subjectivity and shortcomings of quantitative research, (2) shifting away from statistical significance of differences between groups to quantitative estimation of those differences, (3) building a cumulative quantitative discipline, in which phenomena are studied again and again in the same contexts and with the same conditions to ensure they are robust enough.\nKruschke and Liddell (2018) revisit the New Statistics and make a further proposal: to adopt the historically older but only recently popularised approach of Bayesian statistics. They call this the Bayesian New Statistics. The classical approach to statistics is the frequentist method, based on work by Fisher, Neyman and Pearson. Put simply, frequentist statistics is based on rejecting the “null hypothesis” (i.e. the hypothesis that there is no difference between groups) using p-values. Bayesian statistics provides researchers with more appropriate and more robust ways to answer research questions, by reallocating belief or credibility across possibilities. You will learn more about the frequentist and the Bayesian approaches in Chapter 18.\nThis textbook adopts the Bayesian New Statistics approach. Note that we will not really touch upon Bayesian statistics in the strict sense until Chapter 18, just before statistical modelling will be introduced. So you should not worry too much about it for now: just try to appreciate that not only statistics is not intended to objectively separate truths from falsities, but also there are several ways to practice statistics. After all, statistics is a human activity, and like all other human activities it is embedded in the world constructed by humans and their idiosyncrasies.\n\n\n\n\n\n\nQuiz 3\n\n\n\nThree researchers meet at a coffee shop. Each of them tells the other two about their recent findings. Below, you can find what each said. Based on how they talk about the results, which one among them aligns with the New Statistics approach and recognises the shortcomings of research?\n\n Researcher A. My team investigated the effect of emotional dysregulation on speaking rate and they found a significant effect. We have ultimately shown that people with emotional dysregulation speak faster. Researcher B. I wanted to know if it is true that languages with morphologically rich grammars are more difficult to learn than isolating languages. We tested several measures of learning difficulty in two groups of infants, one learning a morphologically rich language and one learning an isolating language. We found that two measures were significantly higher for the morphologically rich language group than the isolating language group. Hence we have found solid evidence that morphologically rich grammars are more difficult to learn. Researcher C. We compared reaction times (RTs) of chimpanzees looking at videos of humans vs chimpanzees signing. If low-level motor perception is mostly affected by the conspecificity (human vs conspecific), we should see differences in RTs of 20-70 ms. If motor resonance is mostly affected (activation of the observer’s own motor system when seeing an action they could perform themselves), we should find differences in RTs of 80-200 ms. We found that in the conspecific condition, RTs were 74-98 ms shorter at 95% probability. This range mostly overlaps with the motor resonance hypothesis (80-200) but it also lies outside of it, somewhat close to the higher end of the low-level perception range (20-70). In sum, we could not establish which hypothesis could better explain the data.\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\nInference is the process of learning something about a population through a sample.\nUncertainty in each observation and variability across observations affect the inference process.\nStatistics is a tool to quantify uncertainty and variability.\nThe (Bayesian) New Statistics is an approach to statistics that highlights the subjective nature of statistics and stressed the importance of estimation over statistical significance.\n\n\n\n\n\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke, Byron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023. “Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses.” Advances in Methods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nCumming, Geoff. 2013. “The New Statistics: Why and How.” Psychological Science 25 (1): 729. https://doi.org/10.1177/0956797613504966.\n\n\nGelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and Objective in Statistics.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 180 (4): 9671033. https://doi.org/10.1111/rssa.12276.\n\n\nKruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin & Review 25 (1): 178206. https://doi.org/10.3758/s13423-016-1221-4.\n\n\nSilberzahn, Raphael, Eric L. Uhlmann, Daniel P. Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahník, Feng Bai, Colin Bannard, and Evelina Bonnier. 2018. “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.” Advances in Methods and Practices in Psychological Science 1 (3): 337356. https://doi.org/10.1177/2515245917747646.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. “How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis.” Linguistics 59 (5): 13111342. https://doi.org/10.1515/ling-2019-0051.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html",
    "href": "ch-scripts.html",
    "title": "7  R scripts",
    "section": "",
    "text": "7.1 Create an R script\nIn Chapter 4 and Chapter 5, you’ve been writing R code in the Console and run it there. But this is not a very efficient way of using R code. Every time, you need to write the code and execute it in the right order and it quickly becomes very difficult to keep track of everything when things start getting more involved. A solution is to use R scripts.\nFrom now on, you should write all code in an R script, until you will learn about Quarto documents in Chapter 13.\nFirst, create a folder called code in your Quarto project folder. You can do so in two different ways:\nThe code/ folder will be the folder where you will save all of your R scripts and other documents.\nNow, to create a new R script, look at the top-left corner of RStudio: the first button to the left looks like a white sheet with a green plus sign. This is the New file button. Click on that and you will see a few options to create a new file.\nClick on R Script. A new empty R script will be created and will open in the File Editor window of RStudio.\nNote that creating an R script does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\nSave the file inside the code/ folder with exactly the following name: week-02.R.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#create-an-r-script",
    "href": "ch-scripts.html#create-an-r-script",
    "title": "7  R scripts",
    "section": "",
    "text": "You can click on the New Folder button in the Files panel (bottom-right) in RStudio, set the name and click Ok. The folder will be created within the current folder shown in the Files list.\nSince Quarto Projects are just folders on your computer, you can create a new folder as you would with any other folder from your computer File Explorer/Finder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\nSo you can always access them from the Finder or File Explorer! However, do not open a file by double clicking on it from the Finder/File Explorer.\nRather, open the Quarto project by double clicking on the .Rproj file and then open files from RStudio to ensure you are working within the RStudio project and the working directory is set correctly.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#write-code",
    "href": "ch-scripts.html#write-code",
    "title": "7  R scripts",
    "section": "7.2 Write code",
    "text": "7.2 Write code\nNow, let’s start filling up that script! Generally, you start the script with calls to library() to load all the packages you need for the script. Please, get in the habit of doing this from now, so that you can keep your scripts tidy and pretty! You will learn soon about the tidyverse packages in the following chapter so for now just attach the cowsay and fortune packages.\n\n\n\n\n\n\nImportant\n\n\n\nStart your R scripts with calls to library() and attach all of the packages that are needed to run that script.\n\n\nGo ahead, write the following code in the top of the .R script. (The code chunk has a convenient copy button in the top-right corner which appears when you place the cursor inside the chunk. If you click the button the code will be copied and you can then paste it in the script).\n\nlibrary(cowsay)\nlibrary(fortunes)\n\nsay(\"fortune\", \"monkey\")\nsay(\"What a lovely day for a wedding\", \"spider\")\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease, don’t include install.packages() in your R scripts!\nRemember, you only have to install a package once, and you can just type it in the Console.\nBut DO include library() calls at the top of your scripts.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#running-scripts",
    "href": "ch-scripts.html#running-scripts",
    "title": "7  R scripts",
    "section": "7.3 Running scripts",
    "text": "7.3 Running scripts\nFinally, the time has come to run the script.\nThere are several ways of doing this. The most straightforward is to click on the Run button. You can find this in the top-right corner of the script window. Pressing Run will run the line of code your text cursor is currently on. So you should place the cursor back on line one and press Run. The code will be executed and you will see it in the Console. If the code returns any output, this will be shown in the Console too. After the line of code is executed, the text cursor moves to the next line. You can click on Run again and so on to run each line one by one. You can also just select all the code (like you would when selecting text in a text editor) and click Run: in this case, all of the code is run, line by line, in the order they appear in the script.\n\nAn alternative way is to place the text cursor on the line of code you want to run and then press CMD+ENTER/CTRL+ENTER. As with clicking Run, this will run the line of code and move the text cursor to the next line of code. It also works with a selection, like the Run button. Now that you know how to use R scripts and run code in them, I will assume that you will keep writing new code in your script and run it from there.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#comments",
    "href": "ch-scripts.html#comments",
    "title": "7  R scripts",
    "section": "7.4 Comments",
    "text": "7.4 Comments\nSometimes we might want to add a few lines of text in our script, for example to take notes. You can add so-called comments in R scripts, simply by starting a line with #. You can also add trailing comments, by adding a # at the end of a line of R code. For example:\n\n# This is a comment. Let's add 6 + 3.\n6 + 3\n\n[1] 9\n\n3 + 6 # This is a trailing comment. 6 + 3 = 3 + 6\n\n[1] 9\n\n\n\n\n\n\n\n\nCode comments\n\n\n\nText that starts with a hash symbol # in an R script is a comment. Comments are not executed.\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nIs the following a valid line of R code? TRUEFALSE\nsum(x + 2) # x = 4\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nIt is a valid line of R code with a trailing comment. If you tried to run it in the Console and got an error it is because the variable x does not exist (unless you had created one earlier). If you add the line x &lt;- 4 before sum(x + 2) # x = 4, the latter will work just fine.\nSo you see there is a difference between valid code and working code.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-scripts.html#ensuring-the-script-runs",
    "href": "ch-scripts.html#ensuring-the-script-runs",
    "title": "7  R scripts",
    "section": "7.5 Ensuring the script runs",
    "text": "7.5 Ensuring the script runs\nThat’s all there is to know about using R scripts. You write code and some comments and you can run code in the script and see the output in the Console. However, there is an important aspect that was not explicitly mentioned above: a script is supposed to work from top (first line) to bottom (last line), so the order of the code in the script matters. A good habit to get into is to restart the R session every now and then and re-run the entire script. To restart the R session you can either go to the Session menu &gt; Restart R or you can press SHIFT+CMD/CTRL+0 (the last key is “zero”). Try this now. Restart the R session and run your script again.\nBut why it is important to restart the session to verify that the script runs? A typical case of scripts that don’t run is when you call a variable in a function before having declared the variable (with &lt;-) or when you call a function without having attached the package the function is from. However, an R session remembers everything you run: if you try to run code with a non-declared variable (like sum(a, 1), but a is not declared) you will get an error; if you now write the code that declares the variable (a &lt;- 3) but you put it after the line of code that uses the variable, the code will run because now the variable is declares and available in the session. If you keep the code this way and restart the session, the code will no longer work. This is because each line is executed in order and by the time R gets to the sum(a, 1) , the line a &lt;- 3 hasn’t been executed yet so a is not available. This example might seem trivial (and it is) but with more complex scripts it is actually quite easy to do things like this (calling a variable on line 10 of the script while it is declared on line 1263).\n\n\n\n\n\n\nExercise 1\n\n\n\nCreate a new script and call it week–02-ex7.1.R (save it in code/). Copy the following code and try to run it. The script will not run because there are several errors: some are code errors (i.e. the code is wrong), others are because the code is not written in the right order. Fix the errors until the script runs correctly. Remember to restart the session!\nlibery(fortunes)\n\na -&gt; 3\nsum(a, b)\nb &lt;- 10\n\n#\nLet's print a fortune\nfotrune(c)\nc &lt;- a + b",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R scripts</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html",
    "href": "ch-statistical-vars.html",
    "title": "8  Statistical variables",
    "section": "",
    "text": "8.1 Estimandum, estimands and statistical variables\nStatistical variables are a fundamental aspect of quantitative data analysis. There isn’t an agreed upon definition of statistical variable, but generally speaking, anything that you have measured or counted is a statistical variable. For example, let’s say you want to measure language proficiency in L2 learners: “language proficiency” is your estimandum, i.e. the concept or entity you wish to measure; you decide to measure language proficiency using the score of a proficiency test, this is the estimand, i.e. the specific measurement of the estimandum “language proficiency”. When the estimand can take on different values, the estimand is a statistical variable: every participant will have a different proficiency score.\nLanguage research involves a large variety of statistical variables. Here just a few examples:\nTry and think of more!\nSo a statistical variable is a measured characteristic. More specifically, a statistical variable is also a mathematical construct: the outcome of the specific mathematical process that generates the values that can be observed and measured. In the case of language proficiency, the statistical variable “proficiency test score” is generated by a process that includes a lot of factors (which can themselves be construed as statistical variables), like actual proficiency, stress levels when taking the test, baseline memory capacity, years of learning and so on. The generative process, i.e. the process that generates the values of a statistical variable, is ultimately what the researcher is interested in.\nWhen you observe or measure something, i.e. when you collect a sample, you are taking note of the values of the statistical variable generated by the generative process. We call them statistical variables because each time you sample the variable, you get different values. In other words, the generative process allows for variation in the output values. The opposite of a variable is a called a statistical constant. Generative processes can contain both variables and constants. Statistical variables and constants are two types of estimands. In practice, you don’t have to worry about whether something is a variable or a constant and in most research contexts you will be working with statistical variables.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html#estimandum-estimands-and-statistical-variables",
    "href": "ch-statistical-vars.html#estimandum-estimands-and-statistical-variables",
    "title": "8  Statistical variables",
    "section": "",
    "text": "Estimandum, estimands and variable\n\n\n\nAn estimandum is any characteristic, phenomenon, entity, concept that is the target of the measurement/counting process.\nAn estimand is the specific quantity of an estimandum that can be measured.\nA (statistical) variable is any estimand or characteristics, number, or quantity that has been measured or counted and can vary.\n\n\n\n\nToken number of telic verbs and atelic verbs in a corpus of written Sanskrit.\nVoice Onset Time of stops in Mapudungun.\nFriendliness ratings of synthetic speech.\nAccuracy of responses in a lexical decision task.\nDigit memory span.\nPhrasal headedness (head-initial vs head-final).\n\n\n\n\n\n\n\n\n\nGenerative process\n\n\n\nThe generative process of an estimand is the mathematical process that generates the values of the estimand that are observed or measured.\n\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nTrue or false?\n\nThe estimandum refers to the specific measurable quantity of a concept or entity. TRUEFALSE\nThe generative process comprises only statistical variables. TRUEFALSE\nA statistical variable is defined as any measurable or countable entity that can vary in value. TRUEFALSE",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html#types-of-variables",
    "href": "ch-statistical-vars.html#types-of-variables",
    "title": "8  Statistical variables",
    "section": "8.2 Types of variables",
    "text": "8.2 Types of variables\nYou will find that some statistics textbooks overcomplicate things when it come to types of statistical variables. From an applied statistics perspective, you only need to be able to identify numeric vs categorical variables and continuous vs discrete variables.\n\n8.2.1 Numeric vs categorical variables\n\n\n\n\n\nThe distinction is quite self-explanatory:\n\nNumeric variables are variables that are numbers.\nCategorical variables are variables that correspond to categories, groups or levels on a scale.\n\n\n\n\n\n\n\nExamples\n\n\n\nNumeric variables\n\nNumber of multi-verb predicates in a book.\nDuration of stressed vowels.\nRating score between 0-100.\n\nCategorical variables\n\nGender (non-binary, female, male, …).\nFirst vs second language users.\nEjective vs non-ejective consonant.\n\n\n\nLearning how to recognise variables is a fundamental skill in quantitative data analysis, since the type of variables determines the type of analyses you can carry out.\n\n\n8.2.2 Continuous vs discrete variables\n\n\n\n\n\nOrthogonal to the numeric/categorical distinction, there is the continuous vs discrete distinction. This one can be at times less straightforward.\n\nA continuous variable is a variable that can take on any value between any two numbers. For example, speech segment duration can be 0.2 s, 0.25 s, 0.2534 s and so on. Segment duration is continuous.\nA discrete variable is a variable that can only take on a set of values, and no value in between. For example, number of gestures is discrete because you can measure 1, 2, 3, 10 gestures but not 3 gestures and three quarters.\n\nNumeric variables can be either continuous or discrete, while categorical variables can only be discrete. There are also sub-types of numeric continuous, numeric discrete and categorical (discrete) variables. The following call-out introduces these sub-types, with examples.\n\n\n\n\n\n\nTypes of variables\n\n\n\nNumeric continuous variable: between any two values there is an infinite number of values.\n\nThe variable can take on any positive and negative number, including 0. For example, temperature in degrees Celsius.\nThe variable can take on any positive number only. For example, segment duration, fundamental frequency (f0), reaction times.\nProportions and percentages: The variable can take on any number between 0 and 1. For example, proportion of accurate responses, probability of scalar inference, proportion of voicing during stop closure, acceptability rating on a 0-100 scale.\n\nNumeric discrete variable: between any two consecutive values there are no other values.\n\nCounts: The variable can take only on any positive integer number. For example, number of telic and atelic verbs in a corpus, number of words known by a child, number of turns in a conversation.\n\nCategorical (discrete) variable. There are three main subtypes.\n\nBinary or dichotomous: The variable can take only one of two values. For example, accuracy (incorrect, correct), voicing (voiceless, voiced), headedness (initial vs final).\nThe variable can take any of three of more values (sometimes called a multinomial variable). For example, gender (non-binary, female, male), place of articulation (labial, coronal, dorsal, glottal, …).\nOrdinal: The variable can take any of three of more values and the values have a natural order. For example, Likert scales of attitude (positive, indifferent, negative), proficiency (functional, good, very good, native-like), lenition (stop, fricative, approximant, deletion).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-statistical-vars.html#sec-operationalise",
    "href": "ch-statistical-vars.html#sec-operationalise",
    "title": "8  Statistical variables",
    "section": "8.3 Operationalisation",
    "text": "8.3 Operationalisation\nIt should be clear now that the estimand is not quite the same thing as the estimandum. The estimand is the researcher’s way to capture the estimandum so that it can be analysed. The relationship between the estimandum and the estimand variable is called operationalisation: an estimandum is operationalised into an estimand. The action of operationalisation consists in choosing how to measure something: as a numeric or as a categorical variable. In some cases, the choice is obvious, but in most cases something could be operationalised either way and different considerations have to be taken into account when choosing, like the particular framework adopted and the study design.\nLet’s think about “age” for a moment: age can be operationalised as years or months (numeric discrete) or as age bins, like young vs old (categorical). Different studies might require one or the other operationalisation of the estimandum “age”. Another example is “acceptability” in morphosyntactic studies: acceptability can be operationalised as a binary categorical variable (grammatical vs agrammatical, and we normally talk of “grammaticality”), as a categorical scale (acceptable, somewhat acceptable, somewhat not acceptable, unacceptable), or a numeric continuous scale (0 to 100). It is important, when planning a study, to carefully think about estimanda (the plural of estimandum) and estimands and how their relationship could be less clear than one might think.\n\n\n\n\n\n\nExercise 1\n\n\n\nThink of all the ways to operationalise the following variables:\n\nVoice Onset Time.\nFriendliness of speech.\nLexical frequency.\n\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nWhich of the following sets contains only discrete variables.\n\n Number of occurences in corpus, sentence duration (ms), articulation rate (syllables per second) Reaction times (ms), accuracy (correct/incorrect), 7-point likert scale Accuracy (correct/incorrect), 7-point likert scale, number of occurrences in corpus Fundamental frequency (hz), response accuracy (percentage), reaction times (ms)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html",
    "href": "ch-read-data.html",
    "title": "9  Read data in R",
    "section": "",
    "text": "9.1 Tabular data\nData comes in a lot of different formats, shape and sizes. However, the most common way to store data used in quantitative analysis is so-called tabular data. R is especially designed to work with such data. Tabular (aka rectangular) data is simply data in the form of a table, with columns and rows.\nTabular data can be saved in different file formats. Different file formats have different file extensions. The comma separated values format (file extension .csv) is the best format to save data in because it is basically a plain text file, it’s quick to parse, and can be opened and edited with any software (plus, it’s not a proprietary format like .docx or .xlsx—these formats are specific to particular commercial software).\nThis is what a .csv file looks like when you open it in a text editor (showing only the first few lines). The file contains tabular data (data that is structured as columns and rows, like a spreadsheet).\nThis is what the file would look like when layed out as a table.\nTo separate the values of each column, a .csv file uses a comma , (hence the name “comma separated values”) to separate the values in every row. The first line of the file indicates the names of the columns of the table:\nThere are 11 columns. The rest of the rows is the data, i.e. the values of each column separated by commas.\nThis might look a bit confusing, but you will see later that, after importing this type of file, you can view it as a nice spreadsheet (as you would in Excel), like in the figure above.\nAnother common type of tabular data file is spreadsheets, like spreadsheets created by Microsoft Excel or Apple Numbers. These are all proprietary formats that require you to have the software that were created with if you want to modify them. Portability and openness are important aspects of conducting research, so that using open and non-proprietary file types makes your research more accessible and doesn’t privilege those who have access to specific software (remember, R is free!). Despite of this, a lot of data is shared as Excel files.\nThere are also variations of the comma separated values type, like tab separated values files (.tsv, which uses tab characters instead of commas) and fixed-width files (usually .txt, where columns are separated by as many white spaces as needed so that the columns align).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#tabular-data",
    "href": "ch-read-data.html#tabular-data",
    "title": "9  Read data in R",
    "section": "",
    "text": "Important\n\n\n\nWhen working through the book, always make sure you are in a Quarto Project by checking the top-right corner of RStudio. If you see the name of the project you are fine, if you see Project (none) then you are not in the Quarto Project. Close RStudio and open the Quarto project.\n\n\n\n\n\n\n\n\n\nTabular data\n\n\n\nTabular data is data that has a form of a table: i.e. values structured in columns and rows.\n\n\n\n\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\n\n\n\n\n\n\n\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\n\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\n\n\n\n\n9.1.1 Non-tabular data\nOf course, R can import also data that is not tabular, like map data and complex hierarchical data, including XML, HTML and json data. We will not cover these types of data, but you can check out the resources in the Extra box.\n\n\n\n\n\n\nR Note: Non-tabular data\n\n\n\n\n\n\nSee Chapters 21-24 of R for Data Science.\nLook up the sf package for mapping.\n\n\n\n\n\n\n9.1.2 .rds files\nR has a special way of saving data: .rds files. .rds files allow you to save an R object to a file on your computer, so that you can read that file back in when you need it. A common use for .rds files is to save tabular data that you have processed so that it can be readily used in many different scripts or even by other people, but .rds files can contain any type of R objects, also lists (so not only tabular data). In the following sections you will learn how to import (aka read) three types of data: .csv, Excel and .rds files.\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhich of the following is not tabular data.\n\n a. A file with 3 columns and 100 rows. b. An HTML file. c. An Excel spreadsheet.\n\nNon-tabular data can be saved to .rds files. TRUEFALSE",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#get-the-data",
    "href": "ch-read-data.html#get-the-data",
    "title": "9  Read data in R",
    "section": "9.2 Get the data",
    "text": "9.2 Get the data\nThe data used in this textbook come from a variety of published and unpublished linguistic studies. You can download the data files from the QML Data website according to the following instructions.\n\n\n\n\n\n\nHow to get the data\n\n\n\n\nDownload the zip archive with all the data by clicking on the following link (if this doesn’t work, right-click and choose “Save linked file” or similar): data.zip. The data is in a zip archive.\nUnzip the zip file to extract the contents. (If you don’t know how to do this, search for it online for your operating system! Zip archives are a very common way of distributing data and it is important to know how to use them).\nCreate a folder called data/ (the slash is there just to remind you that it’s a folder, but you don’t have to include it in the name) in the Quarto project you are using for the course. You know how to do this from Chapter 7.\nMove the contents of the data.zip archive into the data/ folder.\n\nOpen a Finder or File Explorer window.\nNavigate to the folder where you have extracted the zip file (it will very likely be the Downloads/ folder).\nCopy the contents of the zip file.\nIn Finder or File Explorer, navigate to the Quarto project folder, then the data/ folder, and paste the contents in there. (You can also drag and drop if you prefer.)\n\n\n\n\nThe rest of this chapter will assume that you have created a folder called data/ in the Quarto project folder and that the files you downloaded are in that folder. The data folder should like something like this:\ndata/\n└── cameron2020/\n    └── gestures.csv\n└── coretta2018/\n    └── formants.csv\n    └── token-measures.csv\n└── ...\nI recommend that you start being very organised with your files in other projects from now on, whether it’s for a course or your dissertation or anything else. I also suggest to avoid overly nested structures (folders in folders in folders in folders…), unless strictly necessary.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#organising-your-files",
    "href": "ch-read-data.html#organising-your-files",
    "title": "9  Read data in R",
    "section": "9.3 Organising your files",
    "text": "9.3 Organising your files\nThe Open Science Framework has the following recommendations that can be applied to any type of research project.\n\nUse one folder per project. The project folder will also be your RStudio/Quarto project folder. Ideally, the project folder should have all the files related to the project (one exception is PDFs of papers that form the literature background of the project: for those I recommend using bibliography managing software, like the free Zotero or JabRef).\nSeparate code from data. A general recommendation is to have a folder code/ or scripts/ with all the code files of the project and a folder data/ that has all the data. This makes keeping files in order easier, since everything has its natural place.\nSeparate raw data from derived data. Raw data is data that you have gathered that, if lost, is lost for ever. Derived data is any data that is derived from raw data and that can be derived again (for example by running a script) if it’s deleted or corrupted.\nMake raw data read-only. You should assume that anything can happen to raw data, so you should treat it as “read-only”.\n\nTo summarise, these recommendations suggest to have a folder for your research project/course/else, and inside the folder two more folders: one for data and one for code. The data/ folder could further contain raw/ for raw data (data that should not be lost or changed, for example collected data or annotations) and derived/ for data that derives from the raw data, for example through automated data processing.\nIt might be useful to also have a separate folder called figs/ or img/ to save figures and plots. Of course which folders you will have it’s ultimately up to you and needs will vary depending on the nature and practical aspects of each study.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#read-.csv-files",
    "href": "ch-read-data.html#read-.csv-files",
    "title": "9  Read data in R",
    "section": "9.4 Read .csv files",
    "text": "9.4 Read .csv files\nIn this section, you will learn how to read .csv files. Reading .csv files is very easy. You can use the read_csv() function from a collection of R packages known as the tidyverse. Specifically, the read_csv() function is from the readr package, one of the tidyverse packages. If you are learning R for the first time, then you won’t already have the tidyverse packages installed (you can check in the Packages tab in the bottom-right panel). Installing the tidyverse packages is easy: you just need to install the tidyverse package and that will take care of installing the most important packages in the collection (called the “core” tidyverse packages). Note that installation of the core tidyverse packages can take some time (but remember that you do this only once). If you need to install the tidyverse packages, do it now.\n\n\n\n\n\n\nDid you open the Quarto project?\n\n\n\nBefore moving on, make sure that you have opened the RStudio Quarto project correctly (see warning at the beginning of the chapter).\n\n\nNow that you have ensured the tidyverse packages are available, let’s read in data from Song et al. (2020). The study consists of a lexical decision task in which participants were first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word. The prime word belonged to one of three possible groups, each of which refers to the morphological relation of the prime and the target word. We will get back to this data in later chapters, so for now it is sufficient if you just read the paper’s abstract to get a general idea of the research context.\nThe read_csv() function from the readr package only requires you to specify the file path as a string (remember, strings are quoted between \" \", for example \"year_data.txt\"). The data to be read are in the data/ folder, in song2020/shallow.csv. On my computer, the file path of song2020/shallow.csv is /Users/ste/qdal/data/song2020/shallow.csv, but on your computer the file path will be different, of course. However, you will learn a trick below, i.e. relative paths, that allows you to specify file paths in a shortened form.\n\nNote that while the read_csv() function does read the data in R, you must assign the output of the read_csv() function (i.e. the data we are reading) to a variable, using the assignment arrow &lt;-, just like we were assigning values to R variables in previous chapters. And since the read_csv() is a function from the tidyverse, you first need to attach the tidyverse packages with library(tidyverse) (remember, you need to attach packages only once per session). This will attach the core tidyverse packages, including readr. Of course, you can also attach the individual packages directly: library(readr). If you use library(tidyverse) there is no need to attach individual tidyverse packages.\nOpen your week-02.R script. Add the following lines in the script (don’t change the file path! explanation below) and run the code (you might want to put the library() line at the top of the script, with the other packages). The read_csv() line will print information about the data and read the data into shallow.\n\nlibrary(tidyverse)\n\nshallow &lt;- read_csv(\"./data/song2020/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you look at the Environment tab, you will see shallow listed under Data. You can preview the data by clicking on the name of the data in the Environment tab. A View tab will be opened in the top-left panel of RStudio and you will see a nicely formatted table, as you would in a programme like Excel. We will dive into this data later, so just have a peak for now.\n\n\n\n\n\n\nData frames and tibbles\n\n\n\nIn R, a data table is called a data frame.\nTibbles are special data frame created with the read functions from the tidyverse. If you are curious about the difference, check this page.\nIn this textbook, “data frame” and “tibble” will be used interchangeably (since we are using the read functions from the tidyverse, all resulting data frames will be tibbles).\n\n\nBut wait, what is that \"./data/song2020/shallow.csv\"? That’s a relative path. Let’s understand the concept of relative paths now.\n\n9.4.1 Relative paths\nFile paths can be specified in two formats. One format is called absolute file path. An absolute file path include all folders from the top-most folder, which is normally your computer’s hard drive. For example, /Users/ste/qdal/data/song2020/shallow.csv from above is an absolute path. You know it’s an absolute path because it starts with the forward slash /. This means that there isn’t anything above Users/: it’s the top-most folder. A downside of absolute paths is that they are not portable: if I move the qdal/ folder to ste/Documents then I need to change every occurrence in my scripts to /Users/ste/Documents/qdal/data/song2020/shallow.csv. Moreover, when you share your research code (and you should!), using absolute paths means that each person that wants to run the code has to update the absolute path to reflect their own.\nA solution is to use relative paths. Relative paths work by including the path only from within a specific folder. Whichever folders contain that specific folder do not matter. The specific folder is called the working directory. When you are using Quarto projects, the working directory is the project folder, i.e. the folder with the .Rproj and _quarto.yml files.\n\n\n\n\n\n\nWorking directory\n\n\n\nThe working directory is the folder which relative paths are relative to.\nWhen using Quarto projects, the working directory is the project folder.\n\n\nRelative paths are specified by starting the path with ./. For example, if your project is called awesome_proj and it’s in Downloads/stuff/, then if you write read_csv(\"./data/results.csv\") R knows you mean to read the file in Downloads/stuff/awesome_proj/data/results.csv! This works because when working with Quarto projects, all relative paths are relative to the working directory which is automatically set to the project folder.\n\n\n\n\n\n\nRelative path\n\n\n\nA relative path is a file path that is relative to a folder (the working directory). The folder the path starts at is represented by ./.\n\n\nThe code read_csv(\"./data/song2020/shallow.csv\") above will work because you are using a Quarto project and inside the project folder there is a folder called data/ and in it there’s the song2020/shallow.csv file. When you run the code, R will “expand” the relative path to the absolute path and correctly find the file to read. I strongly recommend you to use Quarto projects and relative paths to make your work portable. As hinted at above, the benefit of Quarto projects and relative paths is that, if you move your project or rename it, or if you share the project with somebody, all the paths will just work because they are relative.\n\n\n\n\n\n\nExercise 1: Get the working directory\n\n\n\nYou can get the current working directory with the getwd() command.\nRun it now in the Console! Is the returned path the project folder path?\nIf not, it might be that you are not working from a Quarto project. Check the top-right corner of RStudio: is the project name in there or do you see Project (none)?\nIf it’s the latter, you are not in a Quarto project, but you are running R from somewhere else (meaning, the working directory is somewhere else). If so, close RStudio and open the project.\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\n\nGiven the following absolute path /Users/raj/projects/thesis/data/raw/data.csv and the working directory /Users/raj/projects/, which of the following paths is the correct one to read the data.csv file?\n\n a. /thesis/data/raw/data.csv b. ./projects/thesis/data/raw/data.csv c. ./data/raw/data.csv d. ./thesis/data/raw/data.csv",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#read-excel-sheets",
    "href": "ch-read-data.html#read-excel-sheets",
    "title": "9  Read data in R",
    "section": "9.5 Read Excel sheets",
    "text": "9.5 Read Excel sheets\nTo read an Excel file we need first to attach the readxl package. It should already be installed, because it comes with the tidyverse. If not, install it. Then add the following line to the script.\n\nlibrary(readxl)\n\nNow we can use the read_excel() function. Let’s read the file.\n\nrelatives &lt;- read_excel(\"./data/los2023/relatives.xlsx\")\n\nNow you can view the tibble relatives in the RStudio Viewer. Note that if the Excel file has more than one sheet, you can specify the sheet number when reading the file (the default is sheet = 1).\n\nrelatives_2 &lt;- read_excel(\"./data/los2023/relatives.xlsx\", sheet = 2)\n\nThe second sheet in los2023/relatives.xlx contains the description of the columns in the first sheet.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-read-data.html#import-.rds-files",
    "href": "ch-read-data.html#import-.rds-files",
    "title": "9  Read data in R",
    "section": "9.6 Import .rds files",
    "text": "9.6 Import .rds files\nAnother useful type of data files is a file type specifically designed for R: .rds files. Each .rds file can only contain a single R object, like a tibble. You can read .rds files with the readRDS() function.\n\nglot_status &lt;- readRDS(\"./data/coretta2022/glot_status.rds\")\n\nAs always, you need to assign the output of the function to a variable, here glot_status.\n\n\n\n\n\n\n.rds files\n\n\n\n.rds files are a type of R file which can store any R object and save it on disk.\nR objects can be saved to an .rds file with the saveRDS() function and they can be read with the readRDS() function.\n\n\nView the glot_status tibble now. It is also very easy to save a tibble to an .rds file with the saveRDS() function. For example:\n\nsaveRDS(shallow, \"./data/song2020/shallow.rds\")\n\nThe first argument is the name of the tibble object and the second argument is the file path to save the object to.\n\n\n\n\n\n\nExercise 2\n\n\n\nRead the following files in R, making sure you use the right read_*() function. You can write your code in the week-02.R script.\n\ndata/koppensteiner2016/takete_maluma.txt (a tab separated file).\ndata/pankratz2021/si.csv.\nGo to https://datashare.ed.ac.uk/handle/10283/4006, download the file conflict_data_.xlsx, and save it in data/. Read both sheets (“conflict_data2” and “demographics”). Any issues? (I suggest looking at the spreadsheet in Excel).\n\n\n\n\n\n\n\nSong, Yoonsang, Youngah Do, Arthur L. Thompson, Eileen R. Waegemaekers, and Jongbong Lee. 2020. “Second Language Users Exhibit Shallow Morphological Processing.” Studies in Second Language Acquisition 42 (5): 11211136. https://doi.org/10.1017/s0272263120000170.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Read data in R</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html",
    "href": "ch-sum-measures.html",
    "title": "10  Summary measures",
    "section": "",
    "text": "10.1 Overview\nAs you learned in Chapter 3, quantitative data analysis can be conceived as three activities: summarising, visualising and modelling data. In this chapter, you will learn about summarising data. When we say “summarising data” we usually mean summarising data variables, by themselves or in group. We can summarise statistical variables using summary measures. There are two types of summary measures.\nAlways report a measure of central tendency together with its measure of dispersion! A central tendency measure captures only one aspect of the “distribution” of the values and variables with the same central tendency value could have very different dispersion, and hence be very different in nature. For example, look at the density plot in Figure 10.1 (you will learn more about them in Chapter 17). These plots are good at showing the distribution of values of numeric variables. The higher the density the curve, the more the values under that part of the curve are represented in the sample. Variable a and b have the same mean (central tendency): the mean is 0. But a has a standard deviation (measure of dispersion, more on this below) of 1 while b’s standard deviation is 3. You can appreciate how different a and b are, despite having exactly the same mean. This should show how important it is to not only report (and think about) central tendencies, like the mean, but also the dispersion of the data around the central tendency.\nFigure 10.1\nThe following call-outs list common measures of central tendency and dispersions and how they are calculated. You will probably be familiar with most of them and you don’t have to memorise the formulae. The sections after this one will dive into when to use each measure (and how to get them in R), which is much more important.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#overview",
    "href": "ch-sum-measures.html#overview",
    "title": "10  Summary measures",
    "section": "",
    "text": "Measures of central tendency indicate the typical or central value of a variable.\nMeasures of dispersion indicate the spread or dispersion of the variable values around the central tendency value.\n\n\n\n\n\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMean\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + ... + x_n}{n}\\]\nMedian\n\\[\\text{if } n \\text{ is odd, } x_\\frac{n+1}{2}\\]\n\\[\\text{if } n \\text{ is even,  } \\frac{x_\\frac{n}{2} + x_{\\frac{n}{2}+1}}{2}\\]\nMode\nThe mode is simply the most common value.\n\n\n\n\n\n\n\n\nMeasures of dispersion\n\n\n\nMinimum and maximum values\nRange\n\\[ max(x) - min(x)\\]\nThe range is the difference between the largest and smallest value.\nStandard deviation\n\\[\\text{SD} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + ... + (x_n - \\bar{x})^2}{n-1}}\\]",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#measures-of-central-tendency-1",
    "href": "ch-sum-measures.html#measures-of-central-tendency-1",
    "title": "10  Summary measures",
    "section": "10.2 Measures of central tendency",
    "text": "10.2 Measures of central tendency\nA measure of central tendency approximately tells you where the data is most concentrated. There are three common measures of central tendency: mean, median and mode.\n\n10.2.1 Mean\nUse the mean with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\nmean(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] -0.354\n\n\n\nThe variable can take on any positive number only.\n\n\nmean(c(0.32, 2.58, 1.5, 0.12, 1.09))\n\n[1] 1.122\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDon’t take the mean of proportions and percentages!\nBetter to calculate the proportion/percentage across the entire data, rather than take the mean of individual proportions/percentages: see this blog post. If you really really have to, use the median.\n\n\n\n\n10.2.2 Median\nUse the median with numeric (continuous and discrete) variables.\n\n# odd N\nmedian(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] 0.09\n\n# even N\neven &lt;- c(4, 6, 3, 9, 7, 15)\nmedian(even)\n\n[1] 6.5\n\n# the median is the mean of the two \"central\" number\nsort(even)\n\n[1]  3  4  6  7  9 15\n\nmean(c(6, 7))\n\n[1] 6.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are two important characteristics of the mean and the median:\n\nThe mean is very sensitive to outliers.\nThe median is not.\n\nThe following list of numbers does not have obvious outliers. The mean and median are not to different.\n\n# no outliers\nmedian(c(4, 6, 3, 9, 7, 15))\n\n[1] 6.5\n\nmean(c(4, 6, 3, 9, 7, 15))\n\n[1] 7.333333\n\n\nIn the following case, there is quite a clear outlier, 40. Look how the mean is higher than the median. This is because the outlier 40 pulls the mean towards it.\n\n# one outlier\nmedian(c(4, 6, 3, 9, 7, 40))\n\n[1] 6.5\n\nmean(c(4, 6, 3, 9, 7, 40))\n\n[1] 11.5\n\n\n\n\n\n\n10.2.3 Mode\nUse the mode with categorical (discrete) variables. Unfortunately the mode() function in R is not the statistical mode, but rather it returns the R object type.\nYou can use the table() function to “table” out the number of occurrences of elements in a vector.\n\ntable(c(\"red\", \"red\", \"blue\", \"yellow\", \"blue\", \"green\", \"red\", \"yellow\"))\n\n\n  blue  green    red yellow \n     2      1      3      2 \n\n\nThe mode is the most frequent value: here it is red, with 3 occurrences.\n\n\n\n\n\n\nImportant\n\n\n\nLikert scales are ordinal (categorical) variables, so the mean and median are not appropriate! This is true even when Likert scales are represented with numbers, like “1, 2, 3, 4, 5” for a 5-point scale.\nYou should use the mode (you can use the median with Likert scales if you really really need to…).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#measures-of-dispersion-1",
    "href": "ch-sum-measures.html#measures-of-dispersion-1",
    "title": "10  Summary measures",
    "section": "10.3 Measures of dispersion",
    "text": "10.3 Measures of dispersion\nA measure of dispersion measures how much spread the data is around the measure of central tendency.\n\n10.3.1 Minimum and maximum\nYou can report minimum and maximum values for any numeric variable.\n\nx_1 &lt;- c(-1.12, 0.95, 0.41, -2.1, 0.09)\n\nmin(x_1)\n\n[1] -2.1\n\nmax(x_1)\n\n[1] 0.95\n\nrange(x_1)\n\n[1] -2.10  0.95\n\n\nNote that the range() function does not return the statistical range (see next section), but simply prints both the minimum and the maximum.\n\n\n10.3.2 Range\nUse the range with any numeric variable.\n\nx_1 &lt;- c(-1.12, 0.95, 0.41, -2.1, 0.09)\nmax(x_1) - min(x_1)\n\n[1] 3.05\n\nx_2 &lt;- c(0.32, 2.58, 1.5, 0.12, 1.09)\nmax(x_2) - min(x_2)\n\n[1] 2.46\n\nx_3 &lt;- c(4, 6, 3, 9, 7, 15)\nmax(x_3) - min(x_3)\n\n[1] 12\n\n\n\n\n10.3.3 Standard deviation\nUse the standard deviation with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\nsd(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] 1.23658\n\n\n\nThe variable can take on any positive number only.\n\n\nsd(c(0.32, 2.58, 1.5, 0.12, 1.09))\n\n[1] 0.9895555\n\n\n\n\n\n\n\n\nImportant\n\n\n\nStandard deviations are relative and depend on the measurement unit/scale!\nDon’t use the standard deviation with proportions and percentages!",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-sum-measures.html#summary-table-of-summary-measures",
    "href": "ch-sum-measures.html#summary-table-of-summary-measures",
    "title": "10  Summary measures",
    "section": "10.4 Summary table of summary measures",
    "text": "10.4 Summary table of summary measures\nTo conclude, here is a table that summarises when each measure should be used, depending on the nature of the variable. You can use this table as a cheat-sheet. Green cells indicate that the measure is appropriate for the variable, red cells indicates that they are not and should not be used, and orange cells indicate you should exercise caution when using those measures with those variables. Gray cells indicate that it’s mathematically impossible to apply that measure to that type of variable.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary measures</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html",
    "href": "ch-summarise.html",
    "title": "11  Summarise data",
    "section": "",
    "text": "11.1 Summarise with summarise()\nNow that you have learned about summary measures, we can talk about how to summarise data in R, rather than just vectors as we did in the previous chapter. When you work with data, you always want to get summary measures for most of the variables in the data. Data reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable, which was covered in the previous section. Now, you will learn how to obtain summary measures using the summarise() function from the dplyr tidyverse package. Let’s practice with the data from Song et al. (2020) you read in Chapter 9. We want to get a measure of central tendency and dispersion for the reaction times, in the RT column. In order to decide which measures to pick, think about the nature of the RT variable. Reaction times is a numeric and continuous statistical variable, and it can only have positive values. So the mean and standard deviations are appropriate measures. Let’s start with the mean of the reaction time column RT. Go to your week-02.R script: if you followed Chapter 9 (and you should have), the script should already have the code to attach the tidyverse and read the song2020/shallow.csv file into a variable called shallow.\nNow let’s calculate the mean of RT with summarise(). The summarise() function takes at least two arguments: (1) the tibble to summarise, (2) one or more summary functions applied to columns in the tibble. In this case we just want the mean RTs. To get this, you write RT_mean = mean(RT) which tells the function to calculate the mean of the RT column and save the result in a new column called RT_mean. Yes, summarise() returns a tibble (a data frame)! It might seem overkill now, but you will see below that it is useful when you are grouping the data, so that for example you can get the mean of different groups in the data. Here is the code with its output:\nsummarise(shallow, RT_mean = mean(RT))\nGreat! The mean reaction times of the entire sample is 867.3592 ms. Sometimes you might want to round the numbers. You can round numbers with the round() function. For example:\nnum &lt;- 867.3592\nround(num)\n\n[1] 867\n\nround(num, 1)\n\n[1] 867.4\n\nround(num, 2)\n\n[1] 867.36\nThe second argument of the round() function sets the number of decimals to round to (by default, it is 0, so the number is rounded to the nearest integer, that is, to the nearest whole number with no decimal values). Let’s recalculate the mean by rounding it this time.\nsummarise(shallow, RT_mean = round(mean(RT)))\nWhat if we want also the standard deviation? Easy: we use the sd() function. Round the mean and SD with the round() function when you write the code in your week-02.R script.\n# round the mean and SD\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer). Let’s go all the way and also get the minimum and maximum RT values with the min() and max() functions (again, round all the summary measures).\nFab! When writing a data report, you could write something like this.\nRemember that standard deviations are a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data. However, you won’t be able to compare standard deviations across different measures: for example, you can’t compare the standard deviation of reaction times and of vowel formants because the first is in milliseconds and the second in Hertz; these are two different numeric scales. When required, you can use the median() function to calculate the median, instead of the mean(). Go ahead and calculate the median reaction times in the data. Is it similar to the mean?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#summarise-with-summarise",
    "href": "ch-summarise.html#summarise-with-summarise",
    "title": "11  Summarise data",
    "section": "",
    "text": "Exercise 1\n\n\n\nComplete this code to also get the minimum and maximum RT and round all measures to the nearest integer.\n\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = ..., RT_max = ...\n)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe functions for minimum and maximum are just a few lines above! Have you tried it yourself before seeing the solution?\n\n\nShow me\n\n\n\nsummarise(\n  shallow,\n  RT_mean = round(mean(RT)), RT_sd = round(sd(RT)),\n  RT_min = round(min(RT)), RT_max = round(max(RT))\n)\n\n\n\n\n\n\n\n\nReaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nCalculate the median of RTs in the shallow data.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#na-not-available",
    "href": "ch-summarise.html#na-not-available",
    "title": "11  Summarise data",
    "section": "11.2 NA: Not Available",
    "text": "11.2 NA: Not Available\nMost base R functions, like mean(), sd(), median() and so on, behave unexpectedly if the vector they are used on contains NA values. NA is a special object in R, that indicates that a value is Not Available, meaning that that observation does not have a value (or that the value was not observed in that case). For example, in the following numeric vector, there are 5 objects:\n\na &lt;- c(3, 5, 3, NA, 4)\n\nFour are numbers and one is NA. If you calculate the mean of a with mean() something strange happens.\n\nmean(a)\n\n[1] NA\n\n\nThe functions returns NA. This is because by default when just one value in the vector is NA then operations on the vector will return NA.\n\nmean(a)\n\n[1] NA\n\nsum(a)\n\n[1] NA\n\nsd(a)\n\n[1] NA\n\n\nIf you want to discard the NA values when operating on a vector that contains them, you have to set the na.rm (for “NA remove”) argument to TRUE.\n\nmean(a, na.rm = TRUE)\n\n[1] 3.75\n\nsum(a, na.rm = TRUE)\n\n[1] 15\n\nsd(a, na.rm = TRUE)\n\n[1] 0.9574271\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhat does the na.rm argument of mean() do?\n\n It changes NAs to FALSE. It converts NAs to 0s. It removes NAs before taking the mean.\n\nWhich is the mean of c(4, 23, NA, 5) when na.rm has the default value?\n\n NA. 0. 10.66.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the documentation of ?mean.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#grouping-data-with-group_by",
    "href": "ch-summarise.html#grouping-data-with-group_by",
    "title": "11  Summarise data",
    "section": "11.3 Grouping data with group_by()",
    "text": "11.3 Grouping data with group_by()\nMore often, you will want to calculate summary measures for specific subsets of the data. An elegant way of doing this is with the group_by() function from dplyr. This function takes a tibble, groups the data based on the specified columns, and returns another tibble with the grouping.\n\nshallow_g &lt;- group_by(shallow, Group)\n\nIt looks as if nothing happened, but now the rows in the shallow_g tibble are grouped depending on the value of Group (L1 or L2). If you print out the tibble in the console (just write shallow_g in the Console and press enter), you will notice that the second line of the output says Groups: Group [2], like in the output below. This line tells you how the tibble is grouped: here it is grouped by Group and there are two groups.\n\n\n# A tibble: 6,500 × 11\n# Groups:   Group [2]\n   Group ID    List  Target        ACC    RT logRT Critical_Filler Word_Nonword\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;       \n 1 L1    L1_01 A     banoshment      1   423  6.05 Filler          Nonword     \n 2 L1    L1_01 A     unawareness     1   603  6.40 Critical        Word        \n 3 L1    L1_01 A     unholiness      1   739  6.61 Critical        Word        \n 4 L1    L1_01 A     bictimize       1   510  6.23 Filler          Nonword     \n 5 L1    L1_01 A     unhappiness     1   370  5.91 Critical        Word        \n 6 L1    L1_01 A     entertainer     1   689  6.54 Filler          Word        \n 7 L1    L1_01 A     unsharpness     1   821  6.71 Critical        Word        \n 8 L1    L1_01 A     fersistent      1   677  6.52 Filler          Nonword     \n 9 L1    L1_01 A     specificity     0   798  6.68 Filler          Word        \n10 L1    L1_01 A     termination     1   610  6.41 Filler          Word        \n# ℹ 6,490 more rows\n# ℹ 2 more variables: Relation_type &lt;chr&gt;, Branching &lt;chr&gt;\n\n\nThe grouping information is stored as an “attribute” in the tibble, named groups. You can check this attribute with attr(). You get a tibble with the groupings. Hopefully now you understand that, even if nothing seems to have happened, the tibble has been grouped. Since you saved the output of group_by() into a new variable shallow_g, note that shallow was not affected (try running atrr(shallow, \"groups\") and you will get a NULL). Here’s the output:\n\n\n# A tibble: 2 × 2\n  Group       .rows\n  &lt;chr&gt; &lt;list&lt;int&gt;&gt;\n1 L1        [2,900]\n2 L2        [3,600]\n\n\nThere are 2,900 rows in Group = L1 and 3,600 rows in Group = L2. Now let’s take the shallow_g data and calculate summary measures for L1 and L2 participants separately (as per the Group column).\n\n\n\n\n\n\nExercise 3\n\n\n\nGet the rounded mean, median, SD, minimum and maximum of RTs for L1 and L2 participants in shallow_g.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can do it! You’ve done this above but with shallow. Now you just need to use shallow_g plus get the mininimum and maximum.\n\n\nShow me\n\n\n\nsummarise(\n  shallow_g,\n  mean = round(mean(RT)),\n  median = round(median(RT)),\n  sd = round(sd(RT))\n)\n\n\n\n\n\n\nThis way of grouping the data first with group_by() first and then using summarise() on the grouped tibble works, but it can become tedious if you want to get summaries for different groups and/or combinations of groups. There is a more succinct way of doing this using the pipe |&gt;. Read on to learn about it.\n\n11.3.1 What the pipe!?\nThink of a pipe |&gt; as a teleporter. The pipe |&gt; teleports whatever is on its left into whatever is on its right. The pipe allows you to “stack” multiple operations into a pipeline, without the need to assign each output to a variable. This means that the code is more succinct and even more readable because the way you write code follows exactly the pipeline. So we can get summary measures for each group in Group like so:\n\nshallow |&gt; \n  group_by(Group) |&gt; \n  summarise(mean = round(mean(RT)))\n\n\n  \n\n\n\nThe code says:\n\nTake the shallow data.\nPipe it into group_by() and group it by Group.\nSummarise the grouped data with summarise().\n\nHopefully this just makes sense, but check the R Note box below if you want more details.\ngroup_by() can group according to more than one column, by listing the columns separated by commas (like group_by(Col1, Col2, Col3)). When you list more than one column, the grouping is fully crossed: you get a group for each combination of the grouping columns. Try to group the data by Group and Word_Nonword and get summary measures.\n\n\n\n\n\n\nExercise 4\n\n\n\nGroup shallow by Group and Word_Nonword and get summary measures of RTs. Use the pipe.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ngroup_by(Group, Word_Nonword)",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-summarise.html#counting-observations-with-count",
    "href": "ch-summarise.html#counting-observations-with-count",
    "title": "11  Summarise data",
    "section": "11.4 Counting observations with count()",
    "text": "11.4 Counting observations with count()\nIf you want to count observations you can use the summarise() function with n(), another dplyr function that returns the group size. For example, let’s count the number of languages by their endangerment status. The data in coretta2022/glot_status.rds contains the endangerment status for 7,845 languages from Glottolog. There are thousands of languages in the world, but most of them are losing speakers, and some are already no longer spoken. The column status contains the endangerment status of a language in the data, on a scale from not endangered (languages with large populations of speakers) through threatened, shifting and nearly extinct, to extinct (languages that have no living speakers left). Read the coretta2022/glot_status.rds data and check it out.\nTo count the number of languages by status, we group the data by status and we summarise with n().\n\nglot_status |&gt; \n  group_by(status) |&gt; \n  summarise(n = n())\n\n\n  \n\n\n\nThis approach works. However, dplyr offers a more compact way to get counts with the count() function! You can think of this function as a group_by/summarise combo. You list the columns you want to group by as arguments to count() and the output gives you a column n with the counts. It works with a single column or more than one, like group_by().\n\nglot_status |&gt; \n  count(status)\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nGet the number of languages by status and Macroarea.\n\n\n\n\n\n\n\n\nR Note: Piping\n\n\n\n\n\nWith the release of R 4.1.0, a new feature was introduced to the base language that has significantly improved the readability and expressiveness of R code: the native pipe operator, written as |&gt;. The native pipe allows the result of one expression to be passed automatically as the first argument to another function. This simple idea has a profound impact on how we write R code, particularly when we are performing a sequence of data transformations.\nBefore the native pipe, it was common to see deeply nested function calls that could be difficult to read and reason about. For example, consider the task of computing the square root of the sum of a vector:\nsqrt(sum(c(1, 2, 3, 4)))\nWhile this is relatively simple, as functions become more complex and more transformations are chained together, nested calls quickly become cumbersome. The native pipe solves this by allowing you to write each operation in a left-to-right, stepwise manner, which mirrors the logical flow of data. The key principle of the native pipe is that the left-hand side (LHS) is evaluated first, and its result is automatically passed as the first argument to the right-hand side (RHS). This means that for a simple pipe like:\nx |&gt; f()\nit is equivalent to writing:\nf(x)\nThis principle is important because it defines the natural behavior of the pipe: whatever computation you produce on the LHS will be injected as the first input to the next function. Consider the mtcars dataset, which is built into R. Suppose we want to compute the average miles per gallon (mpg) for each number of cylinders (cyl). Using the native pipe in combination with tidyverse functions, the code is straightforward and highly readable:\nlibrary(dplyr)\n\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(avg_mpg = mean(mpg))\nLet’s break this down:\n\nThe mtcars dataset is the left-hand side. It is evaluated first and becomes the input for the next function.\ngroup_by(cyl) receives the dataset as its first argument, groups the data by the cyl column, and returns a grouped dataframe.\nThe grouped dataframe is then piped into summarise(avg_mpg = mean(mpg)), which calculates the mean mpg for each cylinder group.\n\nNotice that each step receives the output from the previous step as its first argument. This eliminates the need for intermediate variables and nested function calls, creating a natural, readable sequence of transformations.\n\nComparison with the magrittr pipe (%&gt;%)\nBefore R introduced the native pipe, the magrittr package popularized piping with the %&gt;% operator. Functionally, it achieves a very similar goal: passing the result of one expression to another function. For example, the earlier group_by and summarise operation can be written with magrittr as:\nlibrary(magrittr)\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg))\nSome differences between the native pipe and magrittr:\n\nThe native pipe is built into base R, so no external package is required.\nThe native pipe always passes the LHS value to the first argument of the RHS function.\nmagrittr allows more flexibility via the . placeholder, which can inject the LHS value into any argument.\nPerformance-wise, the native pipe has minimal overhead compared to %&gt;%, which is a function call.\n\nOverall, the native pipe provides a simple, consistent, and readable way to chain operations, especially when working with tidyverse workflows. For users already familiar with %&gt;%, the transition is intuitive, with the added benefit that this feature is now a part of base R.\n\n\n\n\n\n\n\n\nSong, Yoonsang, Youngah Do, Arthur L. Thompson, Eileen R. Waegemaekers, and Jongbong Lee. 2020. “Second Language Users Exhibit Shallow Morphological Processing.” Studies in Second Language Acquisition 42 (5): 11211136. https://doi.org/10.1017/s0272263120000170.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarise data</span>"
    ]
  },
  {
    "objectID": "ch-transform.html",
    "href": "ch-transform.html",
    "title": "12  Transform data",
    "section": "",
    "text": "12.1 Filter rows\nData transformation is a fundamental aspect of data analysis. After the data you need to use is imported into R, you will often have to filter rows, create new columns, summarise data or join data frames, among many other transformation operations. In Chapter 11 you have already learned about summarising data, and in this chapter you will learn how to use filter() to filter the data and mutate() to mutate or create new columns.\nFiltering is normally used to filter rows in the data according to specific criteria: in other words, keep certain rows and drop others. Filtering data couldn’t be easier with filter(), from the dplyr package (one of the tidyverse core packages), Let’s work with the coretta2022/glot_status.rds data. Below you can find a preview of the data. Create a new R script called week-03.R and save it in code/. Read the coretta2022/glot_status.rds data.\nglot_status\nIn the following sections we will filter the rows of the data based on the status column. Before we can move on onto filtering however, we first need to learn about logical operators.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transform data</span>"
    ]
  },
  {
    "objectID": "ch-transform.html#filter-rows",
    "href": "ch-transform.html#filter-rows",
    "title": "12  Transform data",
    "section": "",
    "text": "12.1.1 Logical operators\n\n\n\n\n\n\nLogical operators\n\n\n\nLogical operators are symbols that compare two objects and return either TRUE or FALSE.\nThe most common logical operators are ==, !=, &gt;, and &lt;.\n\n\nThere are four main logical operators, each testing a specific logical statements:\n\nx == y: x equals y.\nx != y: x is not equal to y.\nx &gt; y: x is greater than y.\nx &lt; y: x is smaller than y.\n\nLogical operators return TRUE or FALSE depending on whether the statement they convey is true or false. Remember, TRUE and FALSE are logical values.\nTry the following code in the Console:\n\n# This will return FALSE\n1 == 2\n\n[1] FALSE\n\n# FALSE\n\"apples\" == \"oranges\"\n\n[1] FALSE\n\n# TRUE\n10 &gt; 5\n\n[1] TRUE\n\n# FALSE\n10 &gt; 15\n\n[1] FALSE\n\n# TRUE\n3 &lt; 4\n\n[1] TRUE\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhich of the following does not contain a logical operator?\n\n 3 &gt; 1 \"a\" = \"a\" \"b\" != \"b\" 19 &lt; 2\n\nWhich of the following returns c(FALSE, TRUE)?\n\n 3 &gt; c(1, 5) c(\"a\", \"b\") != c(\"a\") \"apple\" != \"apple\"\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n1a.\nCheck for errors in the logical operators.\n1b.\nRun them in the console to see the output.\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n1a.\nThe logical operator == has TWO equal signs. A single equal sign = is an alternative way of writing the assignment operator &lt;-, so that a = 1 and a &lt;- 1 are equivalent.\n1b.\nLogical operators are “vectorised” (you will learn more about this below), i.e they are applied sequentially to all elements in pairs. If the number of elements on one side does not match than of the other side of the operator, the elements on the side that has the smaller number of elements will be recycled.\n\n\n\nYou can use these logical operators with filter() to filter rows that match with TRUE in all the specified statements with logical operators.\n\n\n12.1.2 The filter() function\nFiltering in R with the tidyverse is straightforward. You can use the filter() function. filter() takes one or more statements that return TRUE or FALSE. A common use case is with logical operators. The following code filters the status column so that only the extinct status is included in the new data frame extinct. You’ll notice we are using the pipe |&gt; to transfer the data into the filter() function (you learned about the pipe in Chapter 11). The output of the filter function is assigned &lt;- to extinct. The flow might seem a bit counter-intuitive but you will get used to think like this when writing R code soon enough (although see the R Note box on assignment direction)!\n\nextinct &lt;- glot_status |&gt;\n  filter(status == \"extinct\")\n\nextinct\n\n\n  \n\n\n\nNeat! extinct contains only those languages whose status is extinct. What if we want to include all statuses except extinct? Easy, we use the non-equal operator !=.\n\nnot_extinct &lt;- glot_status |&gt;\n  filter(status != \"extinct\")\n\nnot_extinct contains all languages whose status is not extinct. And if we want only non-extinct languages from South America? We can include multiple statements separated by a comma!\n\nsouth_america &lt;- glot_status |&gt;\n  filter(status != \"extinct\", Macroarea == \"South America\")\n\nCombining statements like this will give you only those rows where all statements return TRUE. You can add as many statements as you need.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t assign the output of filter() to a variable (like in south_america &lt;-), the resulting tibble will be printed in the Console and you won’t be able to do more operations on it! Always remember to assign the output of filtering to a new variable and to avoid overwriting the full tibble, use a different name.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nFilter the glot_status data so that you include only not_endangered languages from all macro-areas except Eurasia.\n\n\nThis is all great, but what if we want to include more than one status or macro-area? To do that we need another operator: %in%.\n\n\n12.1.3 The %in% operator\n\n\n\n\n\n\n%in%\n\n\n\nThe %in% operator is a special logical operator that returns TRUE if the value to the left of the operator is one of the values in the vector to its right, and FALSE if not.\n\n\nTry these in the Console:\n\n# TRUE\n5 %in% c(1, 2, 5, 7)\n\n[1] TRUE\n\n# FALSE\n\"apples\" %in% c(\"oranges\", \"bananas\")\n\n[1] FALSE\n\n\nBut %in% is even more powerful because the value on the left does not have to be a single value, but it can also be a vector! We say %in% is vectorised because it can work with vectors (most functions and operators in R are vectorised).\n\n# TRUE, TRUE\nc(1, 5) %in% c(4, 1, 7, 5, 8)\n\n[1] TRUE TRUE\n\nstocked &lt;- c(\"durian\", \"bananas\", \"grapes\")\nneeded &lt;- c(\"durian\", \"apples\")\n\n# TRUE, FALSE\nneeded %in% stocked\n\n[1]  TRUE FALSE\n\n\nTry to understand what is going on in the code above before moving on.\nNow we can filter glot_status to include only the macro-areas of the Global South and only languages that are either “threatened”, “shifting”, “moribund” or “nearly_extinct”.\n\n\n\n\n\n\nExercise 2\n\n\n\nFilter glot_status to include only the macro-areas (Macroarea) of the Global South and only languages that are either “threatened”, “shifting”, “moribund” or “nearly_extinct”. I have started the code for you, you just need to write the line for filtering status.\n\nglobal_south &lt;- glot_status |&gt;\n  filter(\n    Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"),\n    ...\n  )\n\nThis should not look too alien! The first statement, Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\") looks at the Macroarea column and, for each row, it returns TRUE if the current row value is in c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"), and FALSE if not.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nglobal_south &lt;- glot_status |&gt;\n  filter(\n    Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"),\n    status %in% c(\"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\")\n  )",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transform data</span>"
    ]
  },
  {
    "objectID": "ch-transform.html#mutate-columns",
    "href": "ch-transform.html#mutate-columns",
    "title": "12  Transform data",
    "section": "12.2 Mutate columns",
    "text": "12.2 Mutate columns\nTo change existing columns or create new columns, we can use the mutate() function from the dplyr package. To learn how to use mutate(), we will re-create the status column (let’s call it Status this time) from the Code_ID column in glot_status. The Code_ID column contains the status of each language in the form aes-STATUS where STATUS is one of not_endangered, threatened, shifting, moribund, nearly_extinct and extinct. You can check the labels in a column with the unique() function. This function is not from the tidyverse, but it is a base R function, so you need to extract the column from the tibble with $ (the dollar-sign operator). unique() will list all the unique labels in the column (note that it works with numbers too).\n\nunique(glot_status$Code_ID)\n\n[1] \"aes-shifting\"       \"aes-extinct\"        \"aes-moribund\"      \n[4] \"aes-nearly_extinct\" \"aes-threatened\"     \"aes-not_endangered\"\n\n\n\n\n\n\n\n\nThe dollar sign `$`\n\n\n\nYou can use the dollar sign $ to extract a single column from a data frame as a vector.\n\n\nWe want to create a new column called Status which has only the status part of the label without the aes- part. To remove aes- from the Code_ID column we can use the str_remove() function from the stringr package. Check the documentation of ?str_remove to learn which arguments it uses.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = str_remove(Code_ID, \"aes-\")\n  )\n\nIf you check glot_status now you will find that a new column, Status, has been added. This column is a character column (chr). You see that, as with filter(), you have to assign the output of mutate() to a variable. In the code above we are re-assigning the output to the glot_status variable which we started with. This means that we are overwriting the original glot_status. However, since we have added a new column, we have in practice only added the new column to the old data. If you use the name of an existing column, you will be effectively overwriting that column, so you must be careful with mutate().\nLet’s count the number of languages for each endangerment status using the new Status column. You learned about the count() feature in Chapter 11.\n\nglot_status |&gt;\n  group_by(status) |&gt; \n  count()\n\n\n  \n\n\n\nYou might have noticed that the order of the levels of Status does not match the order from least to most endangered/extinct. Try count() now with the pre-existing status column (with a lower case “s”). You will get the sensible order from least to most endangered/extinct. Why? This is because status (the pre-existing column) is a factor column with a specified order of the different statuses. A factor column is a column that is based on a factor vector (note that tibble columns are vectors), i.e. a vector that contains a list of values, called levels, from a specified set. Factor vectors (or factors for short) allow the user to specify the order of the values. If the order is not specified, the alphabetical order is used by default. Differently from factor vector/columns, character columns (columns that are character vectors) can only use the default alphabetical order. The Status column we created above is a character column. Check the column type by clicking on the small white triangle in the blue circle next to the name of the tibble in the Environment panel (tip-right panel of RStudio). Next to the Status column name you will see chr, for character. But if you look next to status you will see Factor.\n\n\n\n\n\n\nFactor vector\n\n\n\nA factor vector (or column) is a vector that contains a list of values (called levels) from a closed set.\nThe levels of a factor are ordered alphabetically by default.\n\n\nA vector/column can be mutated into a factor column with the as.factor() function. In the following code, we change the existing column Status, in other words we overwrite it (this happens automatically, because the Status column already exists, so it is replaced).\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = as.factor(Status)\n  )\n\nlevels(glot_status$Status)\n\n[1] \"extinct\"        \"moribund\"       \"nearly_extinct\" \"not_endangered\"\n[5] \"shifting\"       \"threatened\"    \n\n\nThe levels() functions returns the levels of a factor column in the order they are stored in the factor: as mentioned above, by default the order is alphabetical. What if we want the levels of Status to be ordered in a more logical manner: not_endangered, threatened, shifting, moribund, nearly_extinct and extinct? Easy! We can use the factor() function instead of as.factor() and specify the levels and their order in the levels argument.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = factor(\n      Status,\n      levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\")\n    )\n  )\n\nlevels(glot_status$Status)\n\n[1] \"not_endangered\" \"threatened\"     \"shifting\"       \"moribund\"      \n[5] \"nearly_extinct\" \"extinct\"       \n\n\nYou see that now the order of the levels returned by levels() is the one we specified. Transforming character columns to vector columns is helpful to specify a particular order of the levels which can then be used when summarising, counting or plotting.\n\n\n\n\n\n\nExercise 3\n\n\n\nUse count() again with the new factor Status column.\n\n\nHere is a preview of data plotting in R, which you will learn in Chapter 15, with the status in the logical order from least to most endangered and extinct.\n\n\nCode\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nFigure 12.1: Number of languages by endangerment status (repeated).\n\n\n\n\n\n\n\n\n\n\n\nR Note: Assignment direction\n\n\n\n\n\nR has two assignment operators: the assignment arrow &lt;- and =. Current R coding practices favour &lt;- over =, hence this book uses exclusively the former. Both have the same assignment direction: the object to the right of the operator is assigned to the variable name to the left of the operator. This is virtually how all programming languages work.\nIt is less know, however, that the assignment arrow can be “reversed”, -&gt; so that assignment goes from left to right. The following are equivalent.\na &lt;- 2\n2 -&gt; a\nWe could re-write the mutate code above like this:\nglot_status |&gt;\n  mutate(\n    Status = factor(\n      Status,\n      levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\")\n    )\n  ) -&gt; glot_status\nThe code fully follows the flow: you take glot_status, you pipe it into mutate, you create a new column, you assign the output to glot_status.\nHowever, you don’t particularly encourage this practice because it makes spotting variable assignment in your scripts more difficult, now that the variable is assigned at the end of the pipeline.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transform data</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html",
    "href": "ch-quarto.html",
    "title": "13  Quarto",
    "section": "",
    "text": "13.1 Code… and text!\nBefore moving onto data visualisation, it is time now to step up your coding organisation skills. Keeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\nYou will learn about reproducibility and related concepts in more details in ?sec-open-research. R scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #). But for longer text or complex data analysis reports, R scripts can be a bit cumbersome. A solution to this is using Quarto files (they have the .qmd extension).\nQuarto is a file format that allows you to mix code and formatted text in the same file. This means that you can write dynamic reports using Quarto files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report file and all code output (plots, tables, etc…) is updated accordingly!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#code-and-text",
    "href": "ch-quarto.html#code-and-text",
    "title": "13  Quarto",
    "section": "",
    "text": "Dynamic reports in Quarto\n\n\n\nQuarto is a file type with extension .qmd in which you can write formatted text and code together.\nQuarto can be used to generate dynamic reports: these are files that are generated automatically from the file source, ensuring data and results in the report are always up to date.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#formatting-text",
    "href": "ch-quarto.html#formatting-text",
    "title": "13  Quarto",
    "section": "13.2 Formatting text",
    "text": "13.2 Formatting text\nR comments in R scripts cannot be formatted (for example, you can’t make text bold or italic). Text in Quarto files can be fully formatted using a simple but powerful mark-up language called Markdown. You don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, at your pace. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #:\n\n# This is a level-1 heading\n\n## This is a level-2 heading\n\n\n\n\n\n\nMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#create-a-.qmd-file",
    "href": "ch-quarto.html#create-a-.qmd-file",
    "title": "13  Quarto",
    "section": "13.3 Create a .qmd file",
    "text": "13.3 Create a .qmd file\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course Quarto Project you created before. You know you are in a Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light-blue cube icon. If you see Project (none) in the top-right corner, that means you are not in the Quarto Project.\nTo make sure you are in the Quarto project, you can open the project by going to the project folder in File Explorer (Windows) or Finder (macOS) and double click on the .Rproj file.\n\n\nTo create a new .qmd file, just click on the New file button (the white square with the green plus symbol), then Quarto Document.... (If you are asked to install/update packages, do so.)\n\n\n\n\n\nA window will open. Add a title of your choice and your name. Make sure the Use visual markdown editor is NOT ticked, then click Create (you will be free to use the visual editor later, but it is important that you first see what a Quarto document looks like under the hood first).\n\n\n\n\n\nA new .qmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating a Quarto file does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\n\n\n\n\nSave the file inside the code/ folder with the following name: week-03.qmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\n\n13.3.1 Parts of a Quarto file\nA Quarto file usually has three main parts:\n\nThe YAML header (green in the screenshot below).\nCode chunks (red).\nText (blue).\n\n\n\n\n\n\nEach Quarto file has to start with a YAML header, but you can include as many code chunks and as much text as you wish, in any order. You can also show the document outline, marked as a dashed yellow box in the figure above. To show/hide the outline, just click on the Outline button. See the R Note box below for tips.\n\n\n\n\n\n\nQuarto: YAML header\n\n\n\nThe header of a .qmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nYAML headers start and end with three dashes ---.\n\n\n\n\n\n\n\n\nQuarto: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\n{r} indicates that the code is R code. Settings can be specified inside the chunk with the #| prefix: for example #| label: setup sets the name of the chunk (the label) to setup.\n\n\n\n\n\n\n\n\nR Note: Quarto outline and chunk labels\n\n\n\n\n\nBy default, the Quarto outline only shows the Markdown section headers. I find it very useful to also see the code chunks in the outline and if they have a label, that will be shown in italics. It makes navigating long documents very easy and clear, descriptive chunk labels sure help.\nTo enable code labels in the outline, go to Global Options &gt; R Markdown &gt; Basic panel &gt; Show in document outline: Sections and all chunks.\n\n\n\n\n\n\n\n\n\n\n13.3.2 Working directory\nWhen using Quarto projects, the working directory (the directory all relative paths are relative to) is the project folder. However, when running code from a Quarto file, the code is run as if the working directory were the folder in which the file is saved. This isn’t an issue if the Quarto file is directly in the project folder, but in our case our Quarto files live in the code/ folder within the project folder (and it is good practice to do so!). We can instruct R to always run code from the project folder (i.e. the working directory is the project folder). This is when the _quarto.yml file comes into play.\nOpen the _quarto.yml file in RStudio (you can simply click on the file in the Files tab and that will open the file in the RStudio editor). Add the line execute-dir: project under the title. Note that indentation should be respected, so the line you write should align with title:, not with project:.\nproject:\n  title: \"qml-2024\"\n  execute-dir: project\nNow, all code in Quarto files, no matter where they are saved, will be run with the project folder as the working directory.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#how-to-add-and-run-code",
    "href": "ch-quarto.html#how-to-add-and-run-code",
    "title": "13  Quarto",
    "section": "13.4 How to add and run code",
    "text": "13.4 How to add and run code\nYou will use the Quarto document you created to write text and code in this chapter. Delete everything in the Quarto document below the YAML header. It’s just example text—we’re not attached to it! This is what the Quarto document should look like now (if your YAML header also contains “format:html, that’s completely fine):\n\n\n\n\n\nNow add an empty line and in the following line write a second-level heading ## Attach packages, followed by two empty lines. Like so:\n\n\n\n\n\nNow we can insert a code chunk to add the code to attach the tidyverse. To insert a new code chunk, you can click on the Insert a new code chunk button (the little green square icon with a C and a plus) , or you can press OPT+CMD+I/ALT+CTRL+I.\n\n\n\n\n\nA new R code chunk will be inserted at the text cursor position. Now go ahead and add the following lines of code inside the R code chunk.\n#| label: setup\n\nlibrary(tidyverse)\nTo run the code, you have two options:\n\nYou click the small green triangle in the top-right corner of the chunk. This runs all the code in the code chunk.\nEnsure the text cursor is inside the code chunk and press SHIFT+CMD+ENTER/SHIFT+CTRL+ENTER. This too runs all the code in the code chunk.\n\nIf you want to run line by line in the code chunk, you can place the text cursor on the line you want to run and press CMD+ENTER/CTRL+ENTER. The current line is run and the text cursor is moved to the next line. Just like in the .R scripts that we’ve been using in past weeks. Run the setup chunk now.\n\n\n\n\n\nYou will see messages printed below the code chunk, in your Quarto file (don’t worry about the Conflicts, they just tell you that some functions from the tidyverse packages have replaced the base R functions, which is OK). Now, complete the following tasks before moving on.\n\nCreate a new second-level heading (with ##) called Read data.\nCreate a new R code chunk.\nSet the label of the chunk to read-data.\nAdd code to read the following files (hint: think about where these files are located relative to the working directory, that is, the project folder). Assign the datasets to the variable names polite and glot_status respectively.\n\nwinter2012/polite.csv\ncoretta2022/glot_status.rds\n\nRun the code.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#render-quarto-files-to-html",
    "href": "ch-quarto.html#render-quarto-files-to-html",
    "title": "13  Quarto",
    "section": "13.5 Render Quarto files to HTML",
    "text": "13.5 Render Quarto files to HTML\nYou can render a .qmd file into a nicely formatted HTML file.\nTo render a Quarto file, just click on the Render button and an HTML file will be created and saved in the same location of the Quarto file.\n\n\n\n\n\nIt may be shown in the Viewer pane (like in the picture below) or in a new browser window. There are a few ways you can set this option to whichever version you prefer. Follow the instructions that work for you—they all do the same thing.\n\nTools &gt; Global Options &gt; R Markdown &gt; Show output preview in…\nPreferences &gt; R Markdown &gt; Basics &gt; Show output preview in….\nRight beside the Render button, you will see a little white gear. Click on that gear, and a drop-down menu will open. Click on Preview in Window or Preview in Viewer Pane, whichever you prefer.\n\n\n\n\n\n\nRendering Quarto files is not restricted to HTML, but also PDFs and even Word documents! This is very handy when you are writing an analysis report you need to share with others. You could even write your dissertation in Quarto!\n\n\n\n\n\n\nQuarto: Rendering\n\n\n\nQuarto files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\nNow that you have done all of this hard work, why don’t you try and render the Quarto file you’ve been working on to an HTML file? Click on the Render button and if everything works fine you should see a rendered HTML file in a second! Note that if you are enrolled in the QML course, you will be asked to render your Quarto files to PDF for the assessments, so I recommend you try this out now by completing the next section.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#render-quarto-files-to-pdf",
    "href": "ch-quarto.html#render-quarto-files-to-pdf",
    "title": "13  Quarto",
    "section": "13.6 Render Quarto files to PDF",
    "text": "13.6 Render Quarto files to PDF\nRendering a Quarto file to PDF is done through LaTeX, a typesetting system with it’s own programming language. You don’t really need to learn LaTeX to render to PDF, because the conversion is handles by Quarto, but since usually computers don’t come with LaTeX you will have to install it. Do this now, by running the following code in the Terminal in RStudio.\n\n\n\n\n\n\nImportant\n\n\n\nNote that you are supposed to run the code in the RStudio Terminal, not the RStudio R console! The RStudio terminal can be found next to the console in the bottom-left corner of RStudio.\n\n\nquarto install tinytex\nA LaTeX distribution will be installed. Now, add the following lines to the YAML preamble of your Quarto file and save the file.\nformat:\n  html: default\n  pdf: default\nYou will see that not there is a small arrow next to the Render button. If you click on it a drop-down menu will appear, with two options: “HTML” and “PDF”. This is because we have instructed Quarto that the file should be rendered in two formats in the yaml preamble with the yaml code above. Click on the PDF option now. If all is well, you should soon see your Quarto file rendered to PDF!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpotlight: LaTeX\n\n\n\n\n\nXXX",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-quarto.html#summary",
    "href": "ch-quarto.html#summary",
    "title": "13  Quarto",
    "section": "13.7 Summary",
    "text": "13.7 Summary\n\n\n\n\n\n\n\nQuarto files can be used to create dynamic and reproducible reports.\nMark-up languages are text-formatting systems that specify text formatting and structure using symbols or keywords. Markdown is the mark-up language that is used in Quarto documents.\nThe main parts of a .qmd file are the YAML header, text and code chunks.\nYou can render Quarto files into HTML, PDF, Word documents and more.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html",
    "href": "ch-viz-principles.html",
    "title": "14  Visualisation principles",
    "section": "",
    "text": "14.1 Good data visualisation\nAs you learned in Chapter 2, quantitative data analysis can be conceived as three activities: summarising, visualising and modelling data. This chapter introduces you to basic principles of good data visualisation, while in Chapter 15 you will learn the basics of plotting data in R.\nAlberto Cairo has identified four common features of good data visualisation (Spiegelhalter 2019: 64-66):\nLet’s see a few examples that illustrate each point. Since you will learn about the plotting code in the next chapter, the code is not shown by default here, but you can see it by clicking on the expandable Code button above each plot.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#good-data-visualisation",
    "href": "ch-viz-principles.html#good-data-visualisation",
    "title": "14  Visualisation principles",
    "section": "",
    "text": "Good data visualisation\n\n\n\n\nIt contains reliable information.\nThe design has been chosen so that relevant patterns become noticeable.\nIt is presented in an attractive manner, but appearance should not get in the way of honesty, clarity and depth.\nWhen appropriate, it is organized in a way that enables some exploration.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#information-is-not-reliable",
    "href": "ch-viz-principles.html#information-is-not-reliable",
    "title": "14  Visualisation principles",
    "section": "14.2 Information is (not) reliable",
    "text": "14.2 Information is (not) reliable\nLet’s use the coretta2022/glot_status data. to create the plots because you will learn about it later. The following plot is titled Number of endangered languages by macroarea and status, but the plot contains both endangered and non-endangered languages.\n\n\nCode\nglot_status %&gt;%\n  # filter(status != \"extinct\") %&gt;%\n  ggplot(aes(Macroarea, fill = status)) +\n  geom_bar() +\n  scale_fill_brewer(type = \"seq\", palette = \"BuPu\") +\n  labs(\n    title = \"Number of endangered languages by macroarea and status\",\n    caption = \"Stacked bar-chart\"\n  )\n\n\n\n\n\n\n\n\n\nWe can fix that by filtering the data so that it contains only endangered languages.\n\n\nCode\nglot_status %&gt;%\n  filter(status != \"extinct\") %&gt;%\n  ggplot(aes(Macroarea, fill = status)) +\n  geom_bar() +\n  scale_fill_brewer(type = \"seq\", palette = \"BuPu\") +\n  labs(\n    title = \"Number of endangered languages by macroarea and status\",\n    caption = \"Stacked bar-chart\"\n  )",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#patterns-are-not-noticeable",
    "href": "ch-viz-principles.html#patterns-are-not-noticeable",
    "title": "14  Visualisation principles",
    "section": "14.3 Patterns are (not) noticeable",
    "text": "14.3 Patterns are (not) noticeable\nThe coretta2021/albvot data contains data on VOT in Albanian. It has data from 6 speakers (Coretta et al. 2022). The following plot uses a bar chart to show the VOT of different stops, but what you can’t really see is that there is a lot of variability within and among stops and within and among speakers.\n\n\nCode\nalbvot %&gt;%\n  filter(consonant %in% c(\"p\", \"t\", \"k\", \"b\", \"d\", \"ɡ\")) %&gt;%\n  ggplot(aes(vot, consonant)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Albanian Voice Onset Time\"\n  )\n\n\n\n\n\n\n\n\n\nWe can do better. The following plot shows individual measurements of VOT for different stops and speakers. Now an interesting pattern emerges: speaker 5 (s05) has particularly long VOT for /t/ and /k/ relative to the other speakers.\n\n\nCode\nalbvot %&gt;%\n  filter(consonant %in% c(\"p\", \"t\", \"k\", \"b\", \"d\", \"\\u261\")) %&gt;%\n  mutate(consonant = factor(consonant, levels = rev(c(\"p\", \"t\", \"k\", \"b\", \"d\", \"\\u261\")))) %&gt;%\n  ggplot(aes(consonant, vot, colour = speaker)) +\n  geom_line(aes(group = interaction(speaker, consonant)), position = position_dodge(width = 0.5)) +\n  geom_point(size = 1.5, alpha = 0.9, position = position_dodge(width = 0.5), aes(group = speaker)) +\n  geom_hline(aes(yintercept = 0)) +\n  scale_y_continuous(breaks = seq(-200, 200, by = 50)) +\n  coord_flip() +\n  labs(\n    ttile = \"Albanian Voice Onset Time\",\n    y = \"Voice Onset Time (ms)\", x = \"Consonant\",\n    caption = \"Time 0 corresponds to the plosive release.\"\n  )\n\n\n\n\n\n\n\n\n\nBar charts are unfortunately overused in research, even in those cases when they are not appropriate.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#aesthetics-should-not-get-in-the-way",
    "href": "ch-viz-principles.html#aesthetics-should-not-get-in-the-way",
    "title": "14  Visualisation principles",
    "section": "14.4 Aesthetics (should not) get in the way",
    "text": "14.4 Aesthetics (should not) get in the way\n\n\n\n\n\nThe graph above has a lot of issues:\n\nThe bar length and thickness are not proportional. Compare Japanese with 123 million speakers vs English with 765 million speakers.\nThe graph mixes two scales: million speakers and billion speakers. This makes it look as if Chinese does not have that many more speakers.\nThe shade of orange of the bars does not seem to become proportionally darker with more speakers. Look at Arabic and Hindi: they have a very similar number of speakers but one bar is darker than the other.\nThe three dudes speaking are just fillers. Are they really necessary? Also, they are all white men…\n\nCan you find other issues? See more examples on Ugly Charts.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#does-not-enable-exploration",
    "href": "ch-viz-principles.html#does-not-enable-exploration",
    "title": "14  Visualisation principles",
    "section": "14.5 Does (not) enable exploration",
    "text": "14.5 Does (not) enable exploration\nThe plot below shows the number of gestures enacted by infants of English, Bengali and Chinese background as recorded during a controlled session (Cameron-Faulkner et al. 2020). Three different types of gestures are shown: hold out and give gestures (ho_gv), index-finger pointing (point) and reach out gestures (reach). Moreover the plot shows the number of gestures at 10 and 12 months.\n\n\nCode\ngestures %&gt;%\n  filter(months %in% c(10, 12)) %&gt;%\n  drop_na(count) %&gt;%\n  group_by(months, background, gesture) %&gt;%\n  summarise(\n    count_sum = sum(count), .groups = \"drop\"\n  ) %&gt;%\n  ggplot(aes(as.factor(months), count_sum, fill = background)) +\n  geom_bar(stat = \"identity\") +\n  facet_grid(background ~ gesture) +\n  scale_fill_brewer(type = \"qual\") +\n  labs(\n    title = \"Infant gesture counts (tally) at 10 and 12 mo\",\n    x = \"Months old\", y = \"Gesture count\"\n  )\n\n\n\n\n\n\n\n\n\nA bar chart is appropriate with count data, like in this case, but it does not allow for much exploration. Each infant was recorded at 10 and 12 months of age, but in the plot you don’t see whether individual infants changed their number of gestures. We can only notice that overall the number of gestures increases from 10 to 12 months old.\nWe can use a “connected point” plot: each infant is represented by a dot at 10 and 12 months and the dots of the same infant are connected by a line. This allows us to see whether an individual infant uses more gestures at 12 months.\n\n\nCode\ngestures %&gt;%\n  filter(months %in% c(10, 12)) %&gt;%\n  drop_na(count) %&gt;%\n  ggplot(aes(as.factor(months), count, colour = background)) +\n  geom_line(aes(group = id), alpha = 0.5) +\n  geom_point(alpha = 0.7) +\n  facet_grid(background ~ gesture) +\n  scale_color_brewer(type = \"qual\") +\n  labs(\n    title = \"Infant gesture counts at 10 and 12 mo\",\n    x = \"Months old\", y = \"Gesture count\"\n  )\n\n\n\n\n\n\n\n\n\nYou will notice that some infants don’t really use more gestures and others even use slightly less gestures. You would not be able to see any of this if you used a bar chart, like we used above.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-viz-principles.html#practical-tips",
    "href": "ch-viz-principles.html#practical-tips",
    "title": "14  Visualisation principles",
    "section": "14.6 Practical tips",
    "text": "14.6 Practical tips\nHere is a list of practical visualisation tips for you to think about.\n\n\n\n\n\n\nTip\n\n\n\n\nShow raw data (e.g. individual observations, participants, items…).\nSeparate data in different panels as needed.\nUse simple but informative labels for axes, panels, etc…\nUse colour as a visual aid, not just for aesthetics.\nReuse labels, colours, shapes throughout different plots to indicate the same thing.\n\n\n\n\n\n\n\nCameron-Faulkner, Thea, Nivedita Malik, Circle Steele, Stefano Coretta, Ludovica Serratrice, and Elena Lieven. 2020. “A Cross-Cultural Analysis of Early Prelinguistic Gesture Development and Its Relationship to Language Development.” Child Development 92 (1): 273290. https://doi.org/10.1111/cdev.13406.\n\n\nCoretta, Stefano, Josiane Riverin-Coutlée, Enkeleida Kapia, and Stephen Nichols. 2022. “Northern Tosk Albanian.” Journal of the International Phonetic Association, 123. https://doi.org/10.1017/s0025100322000044.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualisation principles</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html",
    "href": "ch-plotting.html",
    "title": "15  Plotting",
    "section": "",
    "text": "15.0.1 Base R plotting function\nIn the previous chapter, Chapter 14, you have learned about basic visualisation principles.\nWith these principles in mind, this chapter will teach you the basics of data visualisation (aka plotting) in R. In R, you can create plots using different systems: base R, ggplot2, plotly, lattice and others. This book focusses on the ggplot2 system, which is part of the tidyverse, but before we dive in, it is useful to have a look at the base R plotting system.\nLet’s create two numeric vectors, q and w and plot them. The function plot() takes two arguments: the first argument x takes a vector with the horizontal coordinates (x-axis), here q, and the argument y takes a vector of the same length as the vector of the first argument, with the vertical coordinates (y-axis).\n# N:M is a shortcut for all integer numbers between N and M\nq &lt;- 1:10\n# w is the cube of q\nw &lt;- q^3\n\n# Plot a scatter plot with x as the x-axis and y as the y-axis\nplot(x = q, y = w)\nThe function takes care of adding tick-marks with numbers on the x and y axis, name the axes with the names of the vectors and add the points based on the coordinates in the vectors. It could not be easier! Now let’s add a few more things to this basic plot. Let’s specify we want a line plot (type = \"l\") instead of points, that the line should be coloured purple (col = \"purple\"), with a width of 3 (lwd = 3) and dashed (lty = \"dashed\"). The function connects the points from the coordinates given in the vectors with a line.\nplot(q, w, type = \"l\", col = \"purple\", lwd = 3, lty = \"dashed\")\nWith plots as simple as this one, the base R plotting system is sufficient, but to create more complex plots (which is virtually always the case), base R gets incredibly complicated. Instead, we can use the tidyverse package ggplot2. ggplot2 works well with the other tidyverse packages and it follows the same principles, so it is convenient to use it for data visualisation instead of base R. The following sections will go through the basics of plotting with ggplot2.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#your-first-ggplot2-plot",
    "href": "ch-plotting.html#your-first-ggplot2-plot",
    "title": "15  Plotting",
    "section": "15.1 Your first ggplot2 plot",
    "text": "15.1 Your first ggplot2 plot\nThe tidyverse package ggplot2 provides users with a consistent set of functions to create captivating graphics, and the package works well in combination with the other tidyverse packages. We will plot data from winter2012/polite.csv (Winter and Grawunder 2012) to learn the basics. We can read the data with read_csv() from readr and plot it with ggplot() from ggplot2. Since both readr and the ggplot2 package are part of the tidyverse, it is sufficient to attach the tidyverse with library(tidyverse).\n\nlibrary(tidyverse)\n\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\npolite\n\n\n  \n\n\n\nThe polite data contains several acoustic measurements from utterances spoken by Korean students in Germany. Each row is a single utterance and each participant has spoken many utterances. These are the columns we will focus on.\n\nf0mn: the mean f0 (fundamental frequency). This is the mean f0 of each utterance (i.e. the f0 is calculated along the entire utterance and the mean is taken).\nH1H2: the difference between H2 and H1 (second and first harmonic; the paper reports that this “was based on the central vowel portion of each vowel” although it is not clear if the H1-H2 value of each vowel in the utterance was averaged to produce a mean H1-H2 difference per utterance). A higher H1-H2 difference indicates that the voice is more breathy (as opposed to modal).\ngender: the gender of the speaker (F = female, M = male).\n\nFigure 15.1 shows the plot we will end up with and you will learn how to create it bit by bit below. This plot is a scatter plot, with mean f0 on the x-axis and the H1-H2 difference on the y-axis. Each point represent a an observation in the data, i.e. a row. The points are coloured based on the gender of the participant. You might notice that when mean f0 is high, the H1-H2 difference is lower. In other words, higher mean f0 corresponds to breathier voice.\n\n\n\n\n\n\n\n\nFigure 15.1: Mean f0 and H1-H2 difference in Korean speakers, by gender (Winter and Grawunder 2012).\n\n\n\n\n\nEach ggplot2 plot has a minimum of two constituents (which correspond to two arguments of the ggplot() function): the data and aesthetics mapping.\n\n\n\n\n\n\nggplot2 basic constituents\n\n\n\n\nThe data: you have to specify the data frame with the data (i.e. columns) you want to plot.\nThe mapping: the mapping tells ggplot how to map data columns to parts of the plot like the axes or groupings within the data. For example, which variable is shown on the x axis, and which one is on the y axis? If data comes from two different groups, should each group get its own colour? These different parts of the plot are called aesthetics, or aes for short.\n\n\n\nYou can specify the data and mapping with the data and mapping arguments of the ggplot() function. Note that the mapping argument is always specified with aes(): mapping = aes(…). In the following bare plot, we are just mapping f0mn to the x-axis and H1H2 to the y-axis, from the polite data frame. From this point on I will assume you’ll be creating a new code chunk, copy-paste the code and run it, without explicit instructions.\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n)\n\n\n\n\n\n\n\n\nNot much to see here: just two axes! So where’s the data? Don’t worry, we didn’t do anything wrong. Showing the data itself requires a further step, adding geometries, which we’ll turn to next.\n\n\n\n\n\n\nQuiz 2\n\n\n\nIs the following code correct? Why? TRUEFALSE\nggplot(\n  data = polite,\n  mapping = c(x = total_duration, y = articulation_rate)\n)\n\n\n\n15.1.1 Let’s add geometries\nOur code so far makes nice axes, but we are missing the most important part: showing the data! Data is represented with geometries, or geoms for short. geoms are added to the base ggplot with functions whose names all start with geom_.\n\n\n\n\n\n\nGeometries\n\n\n\nGeometries are plot elements that show the data through geometric shapes.\nDifferent geometries are added to a ggplot using one of the geom_*() functions.\n\n\nFor this plot, you want to use geom_point(). This geom simply adds point to the plot based on the data in the polite data frame. To add geoms to a plot, you write a plus sign + at the end of the ggplot() command and include the geom on the next line.1 The geom_point() geometry creates a scatter plot, which is a plot with two continuous axes where data is represented with points. Figure 15.2 is a scatter plot of mean f0 (mnf0) and H1-H2 difference (H1H2).\n\n\n\n\n\n\nScatter plot\n\n\n\nA scatter plot is a plot with two numeric axes and points indicating the data. It is used when you want to show the relationship between two numeric variables.\nTo create a scatter plot, use the geom_point() geometry.\n\n\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 15.2: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\nLook at Figure 15.2: is there a relationship between mean f0 and H1-H2? A pattern can be observed: when mean f0 is low, H1-H2 is high (meaning more breathiness) and when f0 is high, H1-H2 is low (meaning less breathiness). Statistically, this is called a negative relationship. The opposite is a positive relationship, when an increase in \\(x\\) corresponds to an increase in \\(y\\). Spoiler: the negative relationship in the plot is a mirage: if you look more closely, you might spot two subgroups in the data: one up to about 175 hz and one from 175 hz up. We will see below that these two groups correspond to the speakers’ genders.\nFor the time being, let’s pretend we don’t know that and we want to write a description of the plot and the pattern. You could describe the plot this way:\n\nFigure 15.2 shows a scatter plot of mean f0 on the x-axis and H1-H2 difference on the y-axis. The plot suggest an overall negative relationship between mean f0 and H1-H2 difference. In other words, increasing mean f0 corresponds to decreasing breathiness.\n\n\n\n\n\n\n\nR: The Layered Grammar of Graphics\n\n\n\n\n\nUsing the + is a quirk of ggplot(). The idea behind it is that you start from a bare plot and you add (+) layers of data on top of it. This is because of the philosophy behind the package, called the Layered Grammar of Graphics. In fact, Grammar of Graphics is where you get the GG in ggplot!\n\n\n\n\n\n15.1.2 Function arguments\nNote that the data and mapping arguments don’t have to be named explicitly (with data = and mapping =) in the ggplot() function, since they are obligatory and they are specified in that order. So you can write:\n\nggplot(\n  polite,\n  aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\nIn fact, you can also leave out x = and y =.\n\nggplot(\n  polite,\n  aes(f0mn, H1H2)\n) +\n  geom_point()\n\nBut we can go further. You can use the pipe |&gt;, which you have encountered in Chapter 11.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()\n\nYou can of course stack multiple functions in the pipeline, like for example filtering the data before plotting it, like so:\n\npolite |&gt;\n  # include only rows where f0mn &lt; 300\n  filter(f0mn &lt; 300) |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()\n\n\n\n\n\n\n\nFigure 15.3: Scatter plot of mean f0 and H1-H2 difference (filtered).\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nRun ?ggplot in the Console and check the documentation of the function. Pay special attention to the arguments of the function and the order they appear in.\n\n\n\n\n\n\n\n\nQuiz 3\n\n\n\nWhich of the following will produce the same plot as Figure 15.2? Reason through it first without running the code, then run all of these to check whether they look the way you expected.\n\n ggplot(polite, aes(H1H2, f0mn)) + geom_point() ggplot(polite, aes(y = H1H2, x = f0mn)) + geom_point() ggplot(polite, aes(y = f0mn, x = H1H2)) + geom_point()\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen specifying arguments, the order matters when not using the argument names.\nSo aes(a, b) is different from aes(b, a).\nBut aes(y = b, x = a) is the same as aes(a, b).",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#working-with-aesthetics",
    "href": "ch-plotting.html#working-with-aesthetics",
    "title": "15  Plotting",
    "section": "15.2 Working with aesthetics",
    "text": "15.2 Working with aesthetics\nSo far, the only aesthetics you have been using were the x and y aesthetics, which correspond to the x and y axes. ggplot2 has many other aesthetics that can be employed to represent other variables in the plot: in this section you will learn about colour (which is used to colour geometries, like points) and alpha (which is used to set the transparency of geometries).\n\n15.2.1 colour aesthetic\nAs mentioned above, there seems to be two subgroups within the data: one below about 175 Hz and one above it. These subgroups are in fact related to the gender of the participants. We can colour the points by gender, using the colour aesthetic.2 Figure 15.4 shows a scatter plot of mean f0 and the H1-H2 difference, with points coloured depending on the gender of the speaker. Now the two subgroups are quite visible, although we can also appreciate some overlap between the two gender subgroups (some blue points overlap with the red points and there is one red point that has a very low mean f0).\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 15.4: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nNotice how colour = gender must be inside the aes() function, because we are trying to map colour to the values of the column gender (when you map values to aesthetics, the aesthetics have to be inside aes()). Colours are automatically assigned to each level in gender (here, F for female which gets red and M for male which gets blue).\nThe default colour palette is used, but you can customise it. One way to quickly change the palette it to use one of the scale_colour_*() functions. A good option for our plot is scale_colour_brewer(). This function creates palettes based on ColorBrewer 2.0. There are three types of palettes (see the linked website for examples):\n\nSequential (seq): a gradient sequence of hues from lighter to darker.\nDiverging (div): useful when you need a neutral middle colour and sequential colours on either side of the neutral colour.\nQualitative (qual): useful for categorical variables.\n\nLet’s use the default qualitative palette, since gender is a categorical variable in the data. Figure 15.5 is the same as Figure 15.4, but we are now using a qualitative ColorBrewer palette.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_brewer(type = \"qual\")\n\n\n\n\n\n\n\nFigure 15.5: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nChange the palette argument of the scale_colour_brewer() function to different palettes. Check the function documentation for a list of available palettes.\n\n\nAnother set of palettes is provided by scale_colour_viridis_d() (the d stands for “discrete” palette, to be used for categorical variables like gender). Figure 15.6 uses the “B” palette from the viridis palettes.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_viridis_d(option = \"B\")\n\n\n\n\n\n\n\nFigure 15.6: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\nR Note: The default colour palette\n\n\n\n\n\nIf you want to know more about the default colour palette, check this blog post out.\n\n\n\n\n\n15.2.2 alpha aesthetic\nAnother useful ggplot2 aesthetic is alpha. This aesthetic sets the transparency of the geometry: 0 means completely transparent and 1 means completely opaque. When you are setting the value of an aesthetic yourself that should apply to all instances of some geometry, rather than mapping an aesthetic to values in a specific column (like we did above with colour), you should add the aesthetic outside of aes() and usually in the geom function you want to set the aesthetic for. Set alpha for the point geometry to 0.5.\n\n\n\n\n\n\nHint\n\n\n\n\n\ngeom_point(alpha = ...)\n\n\n\nSetting a lower alpha is useful when there are a lot of points or other geometries that overlap with each other and it just looks like a blob of colour (so that, for example, you can’t really see the individual points). It is not the case here, and in fact reducing the alpha makes the plot quite illegible!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#labels",
    "href": "ch-plotting.html#labels",
    "title": "15  Plotting",
    "section": "15.3 Labels",
    "text": "15.3 Labels\nThe labels of the plot, like the axes labels and the legend, are automatically included by ggplot2 based on the names of the variables/columns. If you want to change the labels to something you set yourself, you can use the labs() function, like in Figure 15.7 below.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 15.7: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nLet’s rewrite out description of the plot from above to reflect the updates.\n\nFigure 15.7 shows a scatter plot of mean f0 on the x-axis and H1-H2 difference on the y-axis, with points coloured by gender. The plot suggest an overall negative relationship between mean f0 and H1-H2 difference. However, the negative relationship appears to be an artefact of the presence of the two gender subgroups: male participants have lower mean f0 and higher H1-H2 difference (less breathiness), while female participants have higher f0 and lower H1-H2 difference (more breathiness).\n\n\n\n\n\n\n\nExercise 3\n\n\n\nAdd a title and a subtitle (use these two arguments within the labs() function).\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor example, labs(title = \"...\", ...).",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#summary",
    "href": "ch-plotting.html#summary",
    "title": "15  Plotting",
    "section": "15.4 Summary",
    "text": "15.4 Summary\n\n\n\n\n\n\n\nggplot2 is a plotting package from the tidyverse.\nTo create a basic plot, you use the ggplot() function and specify data and mapping.\n\nThe aes() function allows you to specify aesthetics (like axes, colours, …) in the mapping argument.\nGeometries map data values onto shapes in the plot. All geometry functions are of the type geom_*().\n\nScatter plots are created with geom_point() and can be used with two numeric variables set as the x and y aesthetics.\nThe colour and alpha aesthetics set the geometry’s colour and transparency.\nIf you need to set an aesthetic to be applied to the entire geometry, you can specify the aesthetic in the geometry, without the aes() function.\n\n\n\n\n\n\n\n\nWinter, Bodo, and Sven Grawunder. 2012. “The Phonetic Profile of Korean Formal and Informal Speech Registers.” Journal of Phonetics 40 (6): 808–15. https://doi.org/10.1016/j.wocn.2012.08.006.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-plotting.html#footnotes",
    "href": "ch-plotting.html#footnotes",
    "title": "15  Plotting",
    "section": "",
    "text": "Note that going on the next line is just for reasons of code clarity and you could write the entire code for a plot on a single line.↩︎\nTo make ggplot inclusive, it’s possible to write the colour aesthetic either as the British-style colour or the American-style color! Both will get the job done.↩︎",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ch-research-cycle.html",
    "href": "ch-research-cycle.html",
    "title": "16  Research cycle",
    "section": "",
    "text": "16.1 Researcher’s degrees of freedom\nIn Chapter 1, you learned about the research process, which includes the research context, data acquisition, data analysis and communication. A different perspective on the research process that highlights the temporal succession of the process steps is the research cycle, represented in an idealised form in Figure 16.1.\nThe cycle starts with the development of research questions and hypotheses. This step involves a thorough literature review and the identification of the topic, research problem, goal, questions and, possibly, hypotheses (as described in Chapter 2). Once the research questions and hypotheses have been determined, the researcher proceeds with the design of the study which sets out to answer the research questions and assess the research hypotheses. The study design process includes determining a large number of interconnected aspects, like materials, procedures, data management and data analysis plans, target population, sampling method and so on. At times the study design process reveals shortcomings or unforeseen aspects of the research questions/hypotheses which can be updated accordingly.\nOnce the study design has been finalised, one proceeds with acquiring data based on the protocols detailed in their plan. After the completion of data acquisition, researchers analyse data and interpret the results in light of the research questions and hypotheses. Finally, the outcomes of the study are published in some form and the next study cycle begins once again.\nThis sounds all very reasonable, but in reality, the researchers’ practice is quite different. This chapter introduces the concept of “researcher’s degrees of freedom” and describes the so-called Questionable Research Practices (QRPs). We will review literature that shows the grim reality of how common QRPs are. In ?sec-open-research, you will learn about principles and tools that are designed to help minimise the presence and impact of QRPs in one’s own research.\nData analysis involves many decisions, such as how to operationalise and measure a given phenomenon or behaviour, which data to submit to statistical modelling and which to exclude in the final analysis, or which inferential approach to employ. This “freedom” can be problematic because humans show cognitive biases that can lead to erroneous inferences (Tversky and Kahneman 1974). For example, humans are prone to see coherent patterns even in the absence of them (Brugger 2001), convince themselves of the validity of prior expectations by cherry-picking evidence (aka confirmation bias, “I knew it,” Nickerson 1998), and perceive events as being plausible in hindsight (“I knew it all along,” Fischhoff 1975). In conjunction with an academic incentive system that rewards certain discovery processes more than others (Koole and Lakens 2012; Sterling 1959), we often find ourselves exploring many possible analytic pathways but reporting only a selected few depending on the quality of the narrative that we can achieve with them.\nThis issue is particularly amplified in fields in which the raw data lend themselves to many possible ways of being measured (Roettger 2019). Combined with a wide variety of conceptual and methodological traditions as well as varying levels of quantitative training across sub-fields, the inherent flexibility of data analysis might lead to a vast plurality of analytic approaches that can itself lead to different scientific conclusions (Roettger, Winter, and Baayen 2019). Analytic flexibility has been widely discussed from a conceptual point of view (Nosek and Lakens 2014; Simmons, Nelson, and Simonsohn 2011; Wagenmakers et al. 2012) and in regard to its application in individual scientific fields (e.g., Charles et al. 2019; Roettger, Winter, and Baayen 2019; Wicherts et al. 2016). This notwithstanding, there are still many unknowns regarding the extent of analytic plurality in practice.\nConsequently, a substantial body of published articles likely present overconfident interpretations of data and statistical results based on idiosyncratic analytic strategies (e.g., Gelman and Loken (2014); Simmons, Nelson, and Simonsohn (2011)). These interpretations, and the conclusions that derive from them, are thus associated with an unknown degree of uncertainty (dependent on the strength of evidence provided) and with an unknown degree of generalizability (dependent on the chosen analysis). Moreover, the same data could lead to very different conclusions depending on the analytic path taken by the researcher. However, instead of being critically evaluated, scientific results often remain unchallenged in the publication record.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Research cycle</span>"
    ]
  },
  {
    "objectID": "ch-research-cycle.html#researchers-degrees-of-freedom",
    "href": "ch-research-cycle.html#researchers-degrees-of-freedom",
    "title": "16  Research cycle",
    "section": "",
    "text": "This section is reproduced from Coretta et al. (2023) (CC-BY-NC) with minor edits.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Research cycle</span>"
    ]
  },
  {
    "objectID": "ch-research-cycle.html#questionable-research-practices",
    "href": "ch-research-cycle.html#questionable-research-practices",
    "title": "16  Research cycle",
    "section": "16.2 Questionable Research Practices",
    "text": "16.2 Questionable Research Practices\n\n\n\n\n\n\nThis section contains text from Coretta (2020) (CC-BY-NC) .\n\n\n\n\n\n\n\n\n\nFigure 16.2: The research cycle and questionable research practices\n\n\n\nQuestionable research practices are practices, whether intentional or not, that undermine the robustness of research (Simmons, Nelson, and Simonsohn 2011; Morin 2015; Flake and Fried 2020). Questionable research practices are practices that negatively affect the research enterprise, but that are employed (most of the time unintentionally) by a surprisingly high number of researchers (John, Loewenstein, and Prelec 2012). For each step in the research cycle, questionable practices are available to researchers. These are part of the researcher’s degrees of freedom, introduced in the previous section. In this section, we will briefly review some of the most common questionable research practices identified in the literature.\nMakel, Plucker, and Hegarty (2012) looked at the publication history of 100 psychological journals since 1900. They found that only 1.07% of the papers (that is, 1 in 100 papers) were replications of previous studies. This means that the vast majority of studies are run only once and the field moves on. As Tukey (1969, 84) said, “Confirmation comes from repetition. Any attempt to avoid this statement leads to failure and more probably to destruction”. This lack of replication attempts is problematic, given than we can’t be certain the results obtain from the one study would replicate if the study is run again. While the study in Makel, Plucker, and Hegarty (2012) focused on psychology, Kobrock and Roettger (2023) find that linguistics shows a more dire situation: only 0.08% of experimental articles contains an independent direct replication (1 in 1250).\nAnother issue that affects modern research regards study design, including aspects related to sample size. Several studies have found that most research employs study designs that grant a 50% probability of being able to find effects of medium size (Cohen 1962; Sedlmeier and Gigerenzer 1992; Bezeau and Graves 2001). Gaeta and Brydges (2020) find a similar scenario in speech, language and hearing research: the majority of studies they screened did not have an adequate sample size to be able to detect medium-sized effects.\nIn a study about the prevalence of questionable research practices, John, Loewenstein, and Prelec (2012) found that about 50% of the researchers interviewed admitted to selective reporting, i.e. reporting only some of the statistical analyses or studies conducted. Combined with a theoretical admission rate, the authors argue for a 100% rate of selective reporting (in other words, we can expect all published studies to be affected by selective reporting). They also found that about 35% of the researchers admitted to having changed the research question/hypothesis after seeing the results (or “claiming to have predicted an unexpected finding”), also known as HARKing (Hypothesising After the Results are Known, Kerr 1998). Combined with the theoretical admission rate, they estimate an actual rate of 90%.\nWe will talk more about sharing research data when you will learn about Open Research practices in ?sec-open-research, but Wicherts et al. (2006) contacted the authors of 141 articles in psychology asking to share the research data with them and a worrying 73% of the authors never shared their data. Bochynska et al. (2023) surveyed 600 linguistic articles and less than 10% of them shared their data as part of the publication.\nPublication bias is used to refer to the bias towards publishing “positive” results (i.e. results that indicate the presence of an effect). Fanelli (2010); Fanelli (2012) found that about 80% of published results are positive results across disciplines, while the prevalence of positive results was higher in fields like psychology and economics (about 90%). Of course, the very high prevalence of positive results indicates that a lot of “negative” results (i.e. results that don’t suggest the presence of an effect) are not published, because in a neutral scenario (where researchers propose and test hypotheses, in an iterative process), there should be many more negative results. Ioannidis (2005), for example, shows through computational modelling that a prevalence rate of positive results of 50% or above would be very difficult to obtain and concludes that “most published research findings are false”. Relatedly, Nissen et al. (2016) also use computational modelling to show how false claims can frequently become canonized as fact, in the absence of sufficient negative results. Further to these points, Scheel (2022) stresses that “most psychological research findings are not even wrong”, in that most claims made in the literature are “so critically underspecified that attempts to empirically evaluate them are doomed to failure” (Scheel 2022, 1).\n\n\n\n\n\n\nQuiz 1\n\n\n\na. True or false?\n\nIn the research cycle, hypotheses are always fixed after the study design is finalised and cannot be changed. TRUEFALSE\nResearcher’s degrees of freedom refers to the many decisions involved in data analysis, which can influence outcomes and interpretations. TRUEFALSE\nPublication bias describes the tendency for journals to publish studies with negative results more often than those with positive results. TRUEFALSE\n\nb. Which of the following is considered a Questionable Research Practice.\n\n Running a replication study to confirm findings. Selectively reporting only some of the analyses conducted. Increasing sample size to ensure adequate statistical power. Publishing negative results alongside positive ones.\n\n\n\n\n\n\n\nBezeau, Scott, and Roger Graves. 2001. “Statistical Power and Effect Sizes of Clinical Neuropsychology Research.” Journal of Clinical and Experimental Neuropsychology 23 (3): 399–406. https://doi.org/10.1076/jcen.23.3.399.1181.\n\n\nBochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas, Irys-Amélie Champagne, Kaidi Chen, Melanie Röthlisberger, Erin M. Buchanan, and Timo B. Roettger. 2023. “Reproducible Research Practices and Transparency Across Linguistics.” Glossa Psycholinguistics 2 (1). https://doi.org/10.5070/g6011239.\n\n\nBrugger, Peter. 2001. “From Haunted Brain to Haunted Science: A Cognitive Neuroscience View of Paranormal and Pseudoscientific Thought.” Hauntings and Poltergeists: Multidisciplinary Perspectives, 195213.\n\n\nCharles, Sarah J., James E. Bartlett, Kyle J. Messick, Thomas J. Coleman, and Alex Uzdavines. 2019. “Researcher Degrees of Freedom in the Psychology of Religion.” The International Journal for the Psychology of Religion 29 (4): 230245.\n\n\nCohen, Jacob. 1962. “The Statistical Power of Abnormal-Social Psychological Research: A Review.” The Journal of Abnormal and Social Psychology 65 (3): 145–53. https://doi.org/10.1037/h0045186.\n\n\nCoretta, Stefano. 2020. “Open Science in Phonetics and Phonology.” https://doi.org/10.31219/osf.io/4dz5t.\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke, Byron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023. “Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses.” Advances in Methods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nFanelli, Daniele. 2010. “Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data.” Edited by Enrico Scalas. PLoS ONE 5 (4): e10271. https://doi.org/10.1371/journal.pone.0010271.\n\n\n———. 2012. “Negative Results Are Disappearing from Most Disciplines and Countries.” Scientometrics 90 (3): 891–904. https://doi.org/10.1007/s11192-011-0494-7.\n\n\nFischhoff, Baruch. 1975. “Hindsight Is Not Equal to Foresight: The Effect of Outcome Knowledge on Judgment Under Uncertainty.” Journal of Experimental Psychology: Human Perception and Performance 1 (3): 288.\n\n\nFlake, Jessica Kay, and Eiko I. Fried. 2020. “Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them.” Advances in Methods and Practices in Psychological Science 3 (4): 456465. https://doi.org/10.1177/2515245920952393.\n\n\nGaeta, Laura, and Christopher R. Brydges. 2020. “An Examination of Effect Sizes and Statistical Power in Speech, Language, and Hearing Research.” Journal of Speech, Language, and Hearing Research 63 (5): 15721580. https://doi.org/10.1044/2020_jslhr-19-00299.\n\n\nGelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in Science: Data-Dependent Analysis. A “Garden of Forking Paths”explains Why Many Statistically Significant Comparisons Don’t Hold Up.” American Scientist 102 (6): 460466.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1080/09332480.2019.1579573.\n\n\nJohn, Leslie K., George Loewenstein, and Drazen Prelec. 2012. “Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.” Psychological Science 23 (5): 524532. https://doi.org/10.1177/0956797611430953.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” Personality and Social Psychology Review 2 (3): 196217. https://doi.org/10.1207/s15327957pspr0203_4.\n\n\nKobrock, Kristina, and Timo B. Roettger. 2023. “Assessing the Replication Landscape in Experimental Linguistics.” Glossa Psycholinguistics 2 (1). https://doi.org/10.5070/g6011135.\n\n\nKoole, Sander L, and Daniël Lakens. 2012. “Rewarding Replications: A Sure and Simple Way to Improve Psychological Science.” Perspectives on Psychological Science 7 (6): 608614.\n\n\nMakel, Matthew C., Jonathan A. Plucker, and Boyd Hegarty. 2012. “Replications in Psychology Research: How Often Do They Really Occur?” Perspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMorin, Olivier. 2015. “A Plea for “Shmeasurement” in the Social Sciences.” Biological Theory 10 (3): 237245. https://doi.org/10.1007/s13752-015-0217-z.\n\n\nNickerson, Raymond S. 1998. “Confirmation Bias: A Ubiquitous Phenomenon in Many Guises.” Review of General Psychology 2 (2): 175220. https://doi.org/10.1037/1089-2680.2.2.175.\n\n\nNissen, Silas Boye, Tali Magidson, Kevin Gross, and Carl T. Bergstrom. 2016. “Publication Bias and the Canonization of False Facts.” Elife 5: e21451. https://doi.org/10.7554/eLife.21451.\n\n\nNosek, Brian A, and Daniël Lakens. 2014. “A Method to Increase the Credibility of Published Results.” Social Psychology 45 (3): 137141.\n\n\nRoettger, Timo B. 2019. “Researcher Degrees of Freedom in Phonetic Sciences.” Laboratory Phonology: Journal of the Association for Laboratory Phonology 10 (1): 127.\n\n\nRoettger, Timo B., Bodo Winter, and Harald Baayen. 2019. “Emergent Data Analysis in Phonetic Sciences: Towards Pluralism and Reproducibility.” Journal of Phonetics 73: 17. https://doi.org/10.1016/j.wocn.2018.12.001.\n\n\nScheel, Anne M. 2022. “Why Most Psychological Research Findings Are Not Even Wrong.” Infant and Child Development 31 (1): e2295. https://doi.org/10.1002/icd.2295.\n\n\nSedlmeier, Peter, and Gerd Gigerenzer. 1992. “Do Studies of Statistical Power Have an Effect on the Power of Studies?” In, 389–406. Washington: American Psychological Association. https://doi.org/10.1037/10109-032.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” Psychological Science 22 (11): 13591366.\n\n\nSterling, Theodore D. 1959. “Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significanceor Vice Versa.” Journal of the American Statistical Association 54 (285): 3034.\n\n\nTukey, John W. 1969. “Analyzing Data: Sanctification or Detective Work?” American Psychologist 24 (2): 83–91. https://doi.org/10.1037/h0027108.\n\n\nTversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases: Biases in Judgments Reveal Some Heuristics of Thinking Under Uncertainty.” Science 185 (4157): 11241131. https://doi.org/10.1126/science.185.4157.1124.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der Maas, and Rogier A. Kievit. 2012. “An Agenda for Purely Confirmatory Research.” Perspectives on Psychological Science 7 (6): 632638. https://doi.org/10.1177/1745691612463078.\n\n\nWicherts, Jelte M., Denny Borsboom, Judith Kats, and Dylan Molenaar. 2006. “The Poor Availability of Psychological Research Data for Reanalysis.” American Psychologist 61 (7): 726.\n\n\nWicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen. 2016. “Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking.” Frontiers in Psychology 7. https://doi.org/10.3389/fpsyg.2016.01832.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Research cycle</span>"
    ]
  },
  {
    "objectID": "ch-probability.html",
    "href": "ch-probability.html",
    "title": "17  Probability distributions",
    "section": "",
    "text": "17.1 Inference: From sample to population\nLet’s start by reading the RT data from the MALD dataset (Tucker et al. 2019). The study the data comes from used an auditory lexical decision task to elicit RTs and accuracy data. In each trial, participants would listen to a word and would have to press a button to say if the word is a real English word or not. The RT is the time lag between the offset of the auditory stimulus (the target word) and the button press. Note that to keep things more manageable, the data we will read is just a subset of the full data.\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\nmald |&gt; \n  ggplot(aes(RT)) +\n  geom_density(fill = \"purple\", alpha = 0.5) +\n  geom_rug(alpha = 0.1) +\n  labs(x = \"Reaction Times (ms)\")\nmald |&gt; \n  summarise(\n    mean_RT = round(mean(RT)), sd_RT = round(sd(RT)), median_RT = round(median(RT))\n  )\nset.seed(9899)\nrt_l &lt;- list()\nfor (i in 1:10) {\n  rt_l[i] &lt;- list(rnorm(n = 20, mean = 1010, sd = 318))\n}\nrt_l[[1]]\n\n [1] 1067.9821 1167.1525  928.9566  713.3736 1119.1665  887.0371  805.6991\n [8] 1547.8105  754.5677  664.0506 1051.0208  832.9883 1132.8416 1579.2453\n[15]  764.4773 1288.4485  924.1961  648.3479  605.6857  827.2246\n# TODO: make into table\ncat(\"Mean:\\n\", round(unlist(lapply(rt_l, mean))), \"\\n\")\n\nMean:\n 966 1177 1055 1012 971 917 972 1089 1144 1116 \n\ncat(\"SD:\\n\", round(unlist(lapply(rt_l, sd))))\n\nSD:\n 278 342 220 348 303 249 281 389 353 361",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#probabilities",
    "href": "ch-probability.html#probabilities",
    "title": "17  Probability distributions",
    "section": "17.2 Probabilities",
    "text": "17.2 Probabilities\n\n\n\n\n\n\nQuiz 1\n\n\n\nTrue or false?\n\nA certain event is an event that has a probability equal to or greater than 1. TRUEFALSE\nAn event that has probability of 0 is an impossible event. TRUEFALSE\nProbabilities are expressed with a number between 0 and 1 (inclusive). TRUEFALSE\nWhen one event cannot occur if another does occur, these are called equally likely events. TRUEFALSE",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#probability-distributions",
    "href": "ch-probability.html#probability-distributions",
    "title": "17  Probability distributions",
    "section": "17.3 Probability distributions",
    "text": "17.3 Probability distributions",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#the-gaussian-distribution",
    "href": "ch-probability.html#the-gaussian-distribution",
    "title": "17  Probability distributions",
    "section": "17.4 The Gaussian distribution",
    "text": "17.4 The Gaussian distribution\nIn the previous section, we have seen that you can visualise probability distributions by plotting the values of the probability function applied to sampled values. For discrete variables, the probability mass function is used while the probability density function is used for continuous variables. Visualising probability distributions is more practical than listing all the possible values and their probability (especially with continuous variables—since they are continuous there is an infinite number of values!). Another convenient way to express probability distributions is by specifying the values of set of parameters, which can reconstruct the entire distribution. Different probability distribution families have a different number of parameters and different parameters. A probability family is an abstraction of specific probability distributions (i.e. probability distributions of observed values).\nA convenient probability distribution family is the Gaussian [ˈgaʊsɪən] probability distribution. The Gaussian distribution is a continuous probability distribution and it has two parameters:\n\nThe mean, represented with the Greek letter \\(\\mu\\) [mjuː]. This parameter is the probability’s central tendency. Values around the mean have higher probability than values further away from the mean.\nThe standard deviation, represented with the Greek letter \\(\\sigma\\) [ˈsɪgmə]. This parameter is the probability’s dispersion around the mean. The higher \\(\\sigma\\) the greater the spread (i.e. the dispersion) of values around the mean.\n\nIn statistical notation, we write the Gaussian distribution family like this:\n\\[\nGaussian(\\mu, \\sigma)\n\\]\nSpecific types of Gaussian distributions will have specific values for the parameters \\(\\mu\\) and \\(\\sigma\\): for example \\(Gaussian(0, 1)\\), \\(Gaussian(50, 32)\\), \\(Gaussian(2.5, 6.25)\\), and so on. All of these specific probability distributions belong to the Gaussian family.\n\n\n\n\n\n\nExercise\n\n\n\n\nGo to Seeing Theory, Chapter 3 Probability distributions.\nSelect “Continuous”.\nIn the drop-down menu, select “Normal”. This is another term for Gaussian.\nScroll down and change the mean \\(\\mu\\). What happens to the probability density (shown on the right)?\nChange the standard deviation \\(\\sigma\\). What happens to the probability density?",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-probability.html#intervals",
    "href": "ch-probability.html#intervals",
    "title": "17  Probability distributions",
    "section": "17.5 Intervals",
    "text": "17.5 Intervals\n\n\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "ch-freq-bayes.html",
    "href": "ch-freq-bayes.html",
    "title": "18  Frequentist and Bayesian inference",
    "section": "",
    "text": "18.1 Frequentist vs Bayesian probability\nThe two major approaches to statistical inference are the frequentist and the Bayesian approaches. The difference between these approaches is how probabilities are conceived, from a philosophical perspective if you will.\nProbabilities in a frequentist framework are about average occurrences of events in a hypothetical series of repetitions of those events. Imagine you observe a volcano for a long period of time. The number of times the volcano erupts within that time tells us the frequency of occurrence of the event of volcanic eruption. In other words, it tells us its (frequentist) probability.\nIn the Bayesian framework, probabilities are about the level of (un)certainty that an event will occur at any specific time given certain conditions. This is probably the way we normally think about probabilities: like in the weather forecast, if somebody tells you tomorrow it will rain with a probability of 85%, you intuitively know that it is very likely that it will rain tomorrow although not certain.\nIn the context of research, a frequentist probability tells you the probability of obtaining the same result again given an imaginary series of replications of the study that generated that probability. On the other hand, a Bayesian probability tells you the probability that the result of the study is the actual result you should have gotten.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Frequentist and Bayesian inference</span>"
    ]
  },
  {
    "objectID": "ch-freq-bayes.html#frequentist-inference",
    "href": "ch-freq-bayes.html#frequentist-inference",
    "title": "18  Frequentist and Bayesian inference",
    "section": "18.2 Frequentist inference",
    "text": "18.2 Frequentist inference\nMost of current research is carried out with frequentist methods. This is a historical accident, based on both a misunderstanding of Bayesian statistics (which is, by the way, older than frequentist statistics) and the fact that frequentist maths was much easier (and personal computers did not exist).\nThe commonly accepted approach to frequentist inference is the so-called Null Hypothesis Significance Testing, or NHST. As practised by researchers, the NHST approach is a (sometimes incoherent) mix of the frequentist work of Fisher on one hand, and Neyman and Pearson on the other. The inconsistent nature of NHST as practised has led to the elaboration of the concept and label “Null Ritual” (Gigerenzer 2004, 2018; Gigerenzer, Krauss, and Vitouch 2004) and the slogan-titled paper The difference between “significant” and “not significant” is not itself statistically significant (Gelman and Stern 2006). The Null Ritual has been criticised by frequentist and Bayesian statisticians alike and has resulted in the proposal or alternative, stricter, versions of NHST, like Statistical Inference as Severe Testing (SIST, Mayo 2018; for a critique see Gelman et al. 2019).\nThe main tenet of NHST (or better, the Null Ritual) is that you set a null hypothesis and you try to reject it. A null hypothesis is, in practice, always a nil hypothesis: in other words, it is the hypothesis that there is no difference between two estimands (these usually being means of two or more groups of interest). Using a variety of numerical techniques, one obtains a p-value, i.e. a frequentist probability. The p-value is used for inference: if the p-value is smaller than a threshold, one can reject the nil hypothesis; if the p-value is equal to or greater than the threshold, you cannot reject the null hypothesis. p-values are very commonly mistaken for Bayesian probabilities (Cassidy et al. 2019) and this results in various misinterpretations of reported results. You will learn about the meaning (and misunderstandings) of p-values in Chapter 28.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Frequentist and Bayesian inference</span>"
    ]
  },
  {
    "objectID": "ch-freq-bayes.html#bayesian-inference",
    "href": "ch-freq-bayes.html#bayesian-inference",
    "title": "18  Frequentist and Bayesian inference",
    "section": "18.3 Bayesian inference",
    "text": "18.3 Bayesian inference\nBayesian inference approaches are now gaining momentum in many fields, including linguistics. The main advantage of Bayesian inference is that it allows researchers to answer research questions in a more straightforward way, using a more intuitive take on uncertainty and probability than what frequentist methods can offer.\nBayesian inference is based on the concept of updating prior beliefs in light of new data. Given a set of prior probabilities and observations, Bayesian inference allows us to revise those prior probabilities and produce posterior probabilities. This is possible through the Bayesian interpretation of probabilities in the context of Bayes’ Theorem.\nIn simple conceptual terms, the Bayesian interpretation of Bayes’ Theorem states that the probability of a hypothesis \\(h\\) given the observe data \\(d\\) is proportional to the product of the prior probability of \\(h\\) and the probability of \\(d\\) given \\(h\\).\n\\[\nP(h|d) \\sim P(h) \\cdot P(d|h)\n\\]\nThe prior probability \\(P(h)\\) represents the researcher’s beliefs towards \\(h\\). These beliefs can be based on expert knowledge, previous studies or mathematical principles. For a more hands-on overview, I recommend Chapter 2 of the Statistical Rethinking textbook.\nThe following section goes through a few reasons to use Bayesian inference for research. Note that some of these reasons presuppose understanding of regression modelling, both frequentist and Bayesian, so don’t worry if they are not clear yet.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Frequentist and Bayesian inference</span>"
    ]
  },
  {
    "objectID": "ch-freq-bayes.html#why-bayesian-inference",
    "href": "ch-freq-bayes.html#why-bayesian-inference",
    "title": "18  Frequentist and Bayesian inference",
    "section": "18.4 Why Bayesian inference?",
    "text": "18.4 Why Bayesian inference?\nHere are a few practical and conceptual reasons for why you should consider switching to Bayesian statistics for your research.\n\n\n\n\n\n\nPractical reasons\n\n\n\n\nFitting frequentist models can lead to anti-conservative p-values (i.e. increased false positive rates, Type-I error rates: there is no effect but yet you got a significant p-value). An interesting example of this for the non-technically inclined reader can be found here. LMER tends to be more sensitive to small sample sizes than Bayesian models (with small sample sizes, Bayesian models return estimates with greater uncertainty, which is a more conservative approach).\nWhile very simple models will return very similar estimates in frequentist and Bayesian statistics, in most cases more complex models won’t fit if run with frequentist packages like lme4, especially without adequate enough sample sizes. Bayesian regression models always converge, while frequentist ones don’t always do.\nFrequentist regression models require as much work as Bayesian ones, although it is common practice to skip necessary steps when fitting the former, which gives the impression of it being a quicker process. Factoring out the time needed to run Markov Chain Monte Carlo chains in Bayesian regressions, in frequentist regressions you still have to perform robust perspective power analyses and post-hoc model checks.\nWith Bayesian models, you can reuse posterior distributions from previous work and include that knowledge as priors into your Bayesian analysis. This feature effectively speeds up the discovery process (getting to the real value estimate of interest faster). You can embed previous knowledge in Bayesian models while you can’t in frequentist ones.\n\n\n\n\n\n\n\n\n\nConceptual reasons\n\n\n\n\nFrequentist regression models cannot provide evidence for a difference between groups, only evidence to reject the null (i.e. nil) hypothesis.\nA frequentist Confidence Interval (CI) can only tell us that, if we run the same study multiple times, n percent of the time the CI will include the real value (but we don’t know whether the CI we got in our study is one from the 100-n percent of CIs that DO NOT CONTAIN the real value). On the other hand, a Bayesian Credible Interval (CrI) ALWAYS tells us that the real value is within a certain range at n percent probability. (Of course all conditional on model and data, which is true both for frequentist and Bayesian models alike). So, frequentist models really just gives you a point estimate, while Bayesian models give a range of values.\nWith Bayesian regressions you can compare any hypothesis, not just null vs alternative. (Although you can use information criteria with frequentist models).\nFrequentist regression models are based on an imaginary set of experiments that you never actually carry out.\nBayesian regression models will converge towards the true value in the long run. Frequentist models do not.\n\n\n\nOf course, there are merits in fitting frequentist models, for example in corporate decisions, but you’ll still have to do a lot of work. The main conceptual difference then is that frequentist and Bayesian regression models answer very different questions and as (basic) researchers we are generally interested in questions that the latter can answer and the former cannot.\n\n\n\n\nCassidy, Scott A., Ralitza Dimova, Benjamin Giguère, Jeffrey R. Spence, and David J. Stanley. 2019. “Failing Grade: 89 Per-Cent of Introduction to Psychology Textbooks That Define/Explain Statistical Significance Do so Incorrectly.” Advances in Methods and Practices in Psychological Science. https://doi.org/10.1177/2515245919858072.\n\n\nGelman, Andrew, Daniel Lakeland, Brian Haig, Christian Hennig, Art Owen, Robert Cousins, Stan Young, et al. 2019. “Many Perspectives on Deborah Mayo’s “Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars”.” https://doi.org/10.48550/arXiv.1905.08876.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between “Significant” and “Not Significant” Is Not Itself Statistically Significant.” The American Statistician 60 (4): 328331. https://doi.org/10.1198/000313006X152649.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The Journal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\n———. 2018. “Statistical Rituals: The Replication Delusion and How We Got There.” Advances in Methods and Practices in Psychological Science 1 (2): 198218. https://doi.org/10.1177/2515245918771329.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual. What You Always Wanted to Know about Significance Testing but Were Afraid to Ask.” In, 391408.\n\n\nMayo, Deborah G. 2018. Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781107286184.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Frequentist and Bayesian inference</span>"
    ]
  },
  {
    "objectID": "ch-gauss-model.html",
    "href": "ch-gauss-model.html",
    "title": "19  Gaussian models",
    "section": "",
    "text": "19.1 Prior probability distributions\nIn the context of a quantitative research study, one simple objectives could be to figure out the probability distribution of the variable of interest: Voice Onset Time, number of telic verbs, informativity score, acceptability ratings, reaction times, and so on. Let’s imagine we are interested in understanding more about the nature of reaction times in auditory lexical decision tasks (lexical decision tasks in which the target is presented aurally rather than in writing). We can revisit the RT data from above to try and address the following research question:\nNow, you might wonder why the mean and the standard deviation? This is because we are assuming that reaction times (i.e the population of reaction times, rather than our specific sample) are distributed according to a Gaussian probability distribution. It is usually the onus of the researcher to assume a probability distribution family. You will learn some heuristics for picking a distribution family later, but for now the Gaussian family will be a safe assumption to make. In statistical notation, we can write:\n\\[\n\\text{RT} \\sim Gaussian(\\mu, \\sigma)\n\\]\nwhich you can read as: “reaction times are distributed according to a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)”. So the research question above is about finding the values of \\(\\mu\\) and \\(\\sigma\\).\nFor illustration’s sake, let’s assume the sample mean and standard deviation are also the population \\(\\mu\\) and \\(\\sigma\\): \\(Gaussian(\\mu = 1010, \\sigma = 318)\\). Figure 19.1 shows the empirical sample probability distribution (in grey) and the theoretical sample probability distribution (in purple) based on the sample mean and SD: in other words, the purple curve is the density curve of the probability distribution \\(Gaussian(1010, 318)\\), We have seen earlier that the sample mean and SD of the RTs from the mald data are biased, due to uncertainty and variability. What we are really after is the values of \\(\\mu\\) and \\(\\sigma\\) which are the mean and standard deviation of the Gaussian distribution of the population of RTs in auditory lexical decision tasks. In other words, we want to make inference from the sample to the population of RTs.\nA statistical tool we can use to obtain an estimate of \\(\\mu\\) and \\(\\sigma\\) is a Gaussian model. A Gaussian model is a statistical model that estimates the values of the parameters of a Gaussian distribution, i.e. \\(\\mu\\) and \\(\\sigma\\). Since the values of the parameters are uncertain, we can estimate a probability distribution of the parameters from the data, rather than just their values. This is what a Bayesian Gaussian model does.\nAs mentioned in Chapter 18, the essence of Bayesian inference is updating prior knowledge with . the estimation of posterior probability distributions from prior probability distributions and data. In practical terms, you need prior probability distributions, or priors for short, and data.\n[XXX illustration of prior to posterior]\nPriors have to be specified by the researcher.\nParameter values can be modelled to come from Gaussian distributions themselves. So, for the mean \\(\\mu\\), we can estimate a Gaussian distribution, with its own mean and standard deviation, \\(\\mu_1\\) and \\(\\sigma_1\\). For the standard deviation \\(\\sigma\\), we can estimate a half Gaussian distribution: why half? Because standard deviations cannot be negative, we can simply include the positive side of a Gaussian distribution centred at 0 (i.e. with mean = 0). In other words, we assume \\(\\sigma\\) to be distributed according to \\(Gaussian(0, \\sigma_2)\\). We can extend the model mathematical notation above to include the distributions of \\(\\mu\\) and \\(\\sigma\\).\n\\[\n\\begin{align}\n\\text{RT} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & \\sim Gaussian(\\mu_1, \\sigma_1)\\\\\n\\sigma & \\sim HalfGaussian(\\mu_2 = 0, \\sigma_2)\\\\\n\\end{align}\n\\]",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html",
    "href": "ch-fit-model.html",
    "title": "20  Running Gaussian models",
    "section": "",
    "text": "20.1 Posterior probability distributions\nYou can fit a Gaussian model to data using the brms package (the name is an initialism of Bayesian Regression Models using Stan; Gaussian models are a special type of regression models, which will be introduced in Chapter 21).\nThe brms package can run a variety of Bayesian (regression) models. It is a very flexible package that allows you to model a lot of different types of variables. You don’t really need to understand all of the technical details to be able to effectively use the package and interpret the results, so this textbook will focus on how to use the package in the context of research. We will cover some of the technicalities, but if you are are particularly interested in the inner workings of the package, feel free to do so (you can find materials on specific aspects by searching online). One useful thing to know is that brms is a bridge between R and the statistical programming software Stan. Stan is a powerful piece of software that can run any type of Bayesian models. What brms does is that it allows you to write Bayesian models in R, which are translated into Stan models and run with Stan under the hood. You can safely use brms without learning Stan, but if you are interested in the computational aspects of Stan, you can check the Stan documentation.\nThe first thing to do is of course to attach the brms package (see code below). You can then run a regression model with the brm() function, short for Bayesian Regression Model (a Gaussian model is a special type of regression model). The mandatory arguments of brm() are a model formula, a distribution family (of the outcome variable), and the data you want to run the model with. Running a model with data is also formally known as fitting the model to the data. Remember the model mathematical formula from above: \\(\\text{RT} \\sim Gaussian(\\mu, \\sigma)\\). It would be nice if brms() allowed you to write the formula like that.\nAlas, due to historical and technical reasons of how other R packages write model formulae, you need to use a special way of specifying the model. As mentioned, you need three arguments: a model formula, the distribution family of the outcome, and the data. So the mathematical formula is split in two parts (corresponding to two arguments of the brm() function): formula and family.\nAs with other R functions, we want to save the output of brm() to a variable, here rt_bm. We will then be able to inspect the output in rt_bm. Now run the Gaussian model of RTs (don’t forget to attach the brms package, like in the following code).\nWhen you run the code, text will be printed in the R Console or below the code if you are using a Quarto document (you should!). This is what it looks like.\nThe messages in the text are related to Stan and the statistical algorithm used by Stan to estimate the parameters of the model (in this model, these are the mean and the standard deviation of height). Compiling Stan program... tells you that brms has instructed Stan to compile the model specified in R and that Stan is now compiling the model to be run on the data (don’t worry if this does not make sense). Start sampling tells us that the statistical algorithm used for estimation has started. This algorithm is the Markov Chain Monte Carlo algorithm, or MCMC for short. The algorithm is run by default four times; in technical terms, four MCMC chains are run. Hence why you will see information on Chain 1, 2, 3, and 4. If you want to learn more about the MCMC algorithm, check Finding posteriors through sampling by Elizabeth Pankratz and the resources linked there.\nNow that the model has finished running and that the output has been saved in rt_bm, we can inspect it with the summary() function (note that this is difference from summarise()!).\nLet’s break down the summary bit by bit:\nThe main characteristic of Bayesian models is that they don’t just provide you with a single numeric estimate for the model parameters. Rather, the model estimates a full probability distribution for each parameter/coefficient. These probability distributions are called posterior probability distributions (or posteriors for short). They are called posterior because they are derived from the data and the prior probability distributions (you will learn about priors in XXX). The model we fitted has two parameters: the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). For reasons that will become clear in Chapter 21, the \\(\\mu\\) parameter is called Intercept in the summary: you can find it in the Regression Coefficients table. The standard deviation \\(\\sigma\\) is in the Further distributional parameters table.\nThe Regression Coefficients table reports for each estimated coefficient a few summary measures of the posterior distributions of the estimated coefficients. Here, we only have the summary measures of one posterior: the posterior of the model’s mean \\(\\mu\\). The table also has three diagnostic measures, which you can ignore for now.\nHere’s a break down of the table’s columns:\nThe Further distributional parameters table has the same structure. In this model, the posterior mean of \\(\\sigma\\) is 317.88 ms and the posterior SD of \\(\\sigma\\) is 3.17 ms.\nPutting all this together in mathematical notation:\n\\[\n\\begin{align}\n\\text{RT} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & \\sim Gaussian(1010.5, 4.45)\\\\\n\\sigma & \\sim Gaussian(317.88, 3.17)\n\\end{align}\n\\]\nIn other words, according to the model and data, the mean or RTs is a value from the distribution \\(Gaussian(1010.5, 4.45)\\) and the SD of RTs is a value from the distribution \\(Gaussian(317.88, 3.17)\\). The model has quantified the uncertainty around the value of the \\(\\mu\\) and \\(\\sigma\\) parameters and this uncertainty is reflected by the fact that we get a full posterior probability distribution (summarised by its mean and SD) for each of the parameters. We don’t know exactly the values of the parameters of the Gaussian distribution assumed to have generated the sampled RTs.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Running Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#posterior-probability-distributions",
    "href": "ch-fit-model.html#posterior-probability-distributions",
    "title": "20  Running Gaussian models",
    "section": "",
    "text": "Estimate, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient). In our model, this is the mean of the posterior distribution of the mean \\(\\mu\\) (Intercept). Yes, you read right: the mean of the mean! Remember, Bayesian models estimate a full probability distribution and this can be summarised with a mean and a standard deviation. In this model, the posterior mean (short for mean of the posterior probability distribution) of \\(\\mu\\) is 1010.5 ms.\nEst.error, the error of the mean estimate, or estimate error. The estimate error is the standard deviation of the posterior distribution of the coefficient (here the mean \\(\\mu\\)). Yes, the standard deviation of the mean! Again, since we are estimating a full probability distribution for \\(\\mu\\), we can summarise it with a mean and SD as we do for any Gaussian distribution. In this model, the posterior SD of \\(\\mu\\) is 4.45 ms.\nl-95% CI and u-95% CI, the lower and upper limits of the 95% Bayesian Credible Interval (more on these below).\nFinally, Rhat, Bulk_ESS, Tail_ESS are diagnostics of the MCMC chains. We will treat model diagnostics in XXX.\n\n\n\n\n\n\n\n\n\n\n\nSpotlight: Student-t",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Running Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#plotting-the-posterior-distributions",
    "href": "ch-fit-model.html#plotting-the-posterior-distributions",
    "title": "20  Running Gaussian models",
    "section": "20.2 Plotting the posterior distributions",
    "text": "20.2 Plotting the posterior distributions\nWhile the model summary reports summaries of the posterior distributions, it is always helpful to plot the posteriors. We can easily do so with the base R plot() function, like in Figure 20.1. The density plots of the posteriors distributions of the two parameters estimated in the model are shown on the left of the figure: b_Intercept which corresponds to Intercept from the summary and sigma (the reason for why it’s b_Intercept will become clear in Chapter 21).\n\nplot(rt_bm, combo = c(\"dens\", \"trace\"))\n\n\n\n\n\n\n\nFigure 20.1: Posterior plots of the rt_bm Gaussian model\n\n\n\n\n\nFor the b_Intercept coefficient, i.e. the mean \\(\\mu\\), the posterior probability encompasses values between 1000 and 1020 ms, approximately. But some values are more probable then others: the values in the centre of the distribution have a higher probability density then the values on the sides. The mean of that posterior probability distribution is the Estimate value in the model summary: 1010.5 ms. Its standard deviation is the Est.error: 4.45 ms. The mean indicates the value with the highest probability density, which corresponds to the value on the horizontal axis of the density plot below the highest peak of the density curve. Based on these properties, values around 1010.5 ms are more probable than values further away from it.\nFor sigma, i.e. \\(\\sigma\\), the posterior probability covers values between 310 and 325. What is the mean of the posterior probability of \\(\\sigma\\)? The answer is in the summary, in Further distributional parameters. There you will also find the standard deviation of the posterior of \\(\\sigma\\). That’s a standard deviation of a standard deviation! As before, this is because we are not estimating a simple value for \\(\\sigma\\) but a full (posterior) probability distribution and we can summarise this distribution with a mean and a standard deviation. Again, the highest peak in the distribution corresponds to the Estimate value.\nNow, looking at a full probability distribution like that is not very straightforward and summary measures can be even less straightforward. Credible Intervals (CrIs) help summarise the posterior distributions so that interpretation is more straightforward.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Running Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#interpreting-credible-intervals",
    "href": "ch-fit-model.html#interpreting-credible-intervals",
    "title": "20  Running Gaussian models",
    "section": "20.3 Interpreting Credible Intervals",
    "text": "20.3 Interpreting Credible Intervals\nThe model summary reports the Bayesian Credible Intervals (CrIs) of the posterior distributions. Another way of obtaining summaries of the coefficients is to use the posterior_summary() function which returns a table (technically, a matrix, another type of R objects; we print only the first two rows with [1:2,] because we can ignore the other ones).\n\nposterior_summary(rt_bm)[1:2,]\n\n             Estimate Est.Error      Q2.5     Q97.5\nb_Intercept 1010.5015  4.452312 1001.6555 1019.2559\nsigma        317.8845  3.171714  311.7382  324.1688\n\n\nThe 95% CrI of the b_Intercept is between 1002 and 1019. This means that there is a 95% probability, or (equivalently) that we can be 95% confident, that the Intercept, i.e. \\(\\mu\\), is within that range. For sigma, i.e. \\(\\sigma\\), the 95% CrI is between 311.7 and 324.2. So, again, there is a 95% probability that the sigma value is between those values. So, to summarise, a 95% CrI tells us that we can be 95% confident, or in other words that there is a 95% probability, that the value of the coefficient is between the values of the CrI.\nThere is nothing special about 95% CrI and in fact it is recommended to calculate and report a few. Personally, I use 90, 80, 70 and 60% CrIs. You can get any CrI with the summary() and the posterior_summary() functions, but you will also learn an alternative way in Chapter 22. Here is how to get an 80% CrI with summary().\n\nsummary(rt_bm, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 1 \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept  1010.50      4.45  1004.74  1016.14 1.00     3628     2476\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma   317.88      3.17   313.84   321.89 1.00     4064     2571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor posterior_summary(), you specify the quantiles rather than the probability level.\n\nposterior_summary(rt_bm, probs = c(0.1, 0.9))[1:2,]\n\n             Estimate Est.Error       Q10       Q90\nb_Intercept 1010.5015  4.452312 1004.7400 1016.1351\nsigma        317.8845  3.171714  313.8399  321.8941\n\n\nHere you have the results from the regression model. Really, the results of the model are the full posterior probabilities, but it makes things easier to focus on the CrI.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Running Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#reporting",
    "href": "ch-fit-model.html#reporting",
    "title": "20  Running Gaussian models",
    "section": "20.4 Reporting",
    "text": "20.4 Reporting\nWhat about reporting the model in writing? We could report the model and the results like this (for simplicity, we report only the 95% CrIs here. You will learn how to report multiple CrIs in tables in XXX).1\n\nWe fitted a Bayesian Gaussian model of reaction times (RTs) using the brms package (Bürkner 2017) in R (R Core Team 2024). The model estimates the mean and standard deviation of RTs.\nBased on the model results, there is a 95% probability that the mean is between 1002 and 1019 ms (mean = 1011, SD = 4) and that the standard deviation is between 312 and 324 ms (mean = 318, SD = 3).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Running Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-fit-model.html#footnotes",
    "href": "ch-fit-model.html#footnotes",
    "title": "20  Running Gaussian models",
    "section": "",
    "text": "To know how to add a citation for any R package, simply run citation(\"package\") in the R Console, where \"package\" is the package name between double quotes.↩︎",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Running Gaussian models</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html",
    "href": "ch-regression-intro.html",
    "title": "21  Introduction to regression",
    "section": "",
    "text": "21.1 A straight line\nIn Chapter 17, you have learned the basics of probability and how to run Gaussian models to estimate the mean and standard deviation (\\(\\mu\\) and \\(\\sigma\\)) of a variable. This chapter extends the Gaussian model to what is commonly called a Gaussian regression model. Regression models (including the Gaussian) are models based on the equation of a straight line. They allow you to model the relationship between two or more variables. This textbook introduces you to regression models of varying complexity which can model variables frequently encountered in linguistics. Note that regression models are very powerful and flexible statistical models which can deal with a great variety of types of variables. [TODO: Appendix of types of regression models cheatsheet].\nA regression model is a statistical model that estimates the relationship between an outcome variable and one or more predictor variables (more on outcome/predictor below). Regression models are based on the equation of a straight line.\n\\[\ny = mx + c\n\\]\nAn alternative notation of the equation is:\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\nWhere \\(\\beta_0\\) is \\(c\\) and \\(\\beta_1\\) is \\(m\\) in the first notation. We will use the second notation (with \\(\\beta_0\\) and \\(beta_1\\)) in this book, since using \\(\\beta\\)’s with subscript indexes will help understand the process of extracting information from regression models later.1",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html#back-to-school",
    "href": "ch-regression-intro.html#back-to-school",
    "title": "21  Introduction to regression",
    "section": "21.2 Back to school",
    "text": "21.2 Back to school\nYou might remember from school when you were asked to find the values of \\(y\\) given certain values of \\(x\\) and specific values of \\(\\beta_0\\) and \\(\\beta_1\\). For example, you were given the following formula (the dot \\(\\cdot\\) stands for multiplication; it can be dropped so \\(2 \\cdot x\\) and \\(2x\\) are equivalent):\n\\[\ny = 3 + 2 \\cdot x\n\\]\nand the values \\(x = (2, 4, 5, 8, 10, 23, 36)\\). The homework was to calculate the values of \\(y\\) and maybe plot them on a Cartesian coordinate space.\n\n\nCode\nlibrary(tidyverse)\n\nline &lt;- tibble(\n  x = c(2, 4, 5, 8, 10, 23, 36),\n  y = 3 + 2 * x\n)\n\nggplot(line, aes(x, y)) +\n  geom_point(size = 4) +\n  geom_line(colour = \"red\") +\n  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))\n\n\n\n\n\n\n\n\n\nUsing the provided formula, we are able to find the values of \\(y\\). Note that in \\(y = 3 + 2 * x\\), \\(\\beta_0 = 3\\) and \\(\\beta_1 = 2\\). Importantly, \\(\\beta_0\\) is the value of \\(y\\) when \\(x = 0\\). \\(\\beta_0\\) is commonly called the intercept of the line. The intercept is the value where the line crosses the y-axis (the value where the line “intercepts” the y-axis).\n\\[\n\\begin{align}\ny & = 3 + 2 * x\\\\\n& = 3 + 2 * 0\\\\\n& = 3\\\\\n\\end{align}\n\\]\nAnd \\(\\beta_1\\) is the number to add to the intercept for each unit increase of \\(x\\). \\(\\beta_1\\) is commonly called the slope of the line.2\n\\[\n\\begin{align}\ny = 3 + 2x = 3 + 2 * 1 = 3 + 2 = 5\\\\\ny = 3 + 2 * 2 = 3 + (2 + 2) = 7\\\\\ny = 3 + 2 * 3 = 3 + (2 + 2 + 2) = 9\\\\\n\\end{align}\n\\]\nFigure 21.1 should clarify this. The dashed line indicates the increase in \\(y\\) for every unit increase of \\(x\\) (i.e., every time \\(x\\) increases by 1, \\(y\\) increases by 2).\n\n\nCode\nline &lt;- tibble(\n  x = 0:3,\n  y = 3 + 2 * x\n)\n\nggplot(line, aes(x, y)) +\n  geom_point(size = 4) +\n  geom_line(colour = \"red\") +\n  annotate(\"path\", x = c(0, 0, 1), y = c(3, 5, 5), linetype = \"dashed\") +\n  annotate(\"path\", x = c(1, 1, 2), y = c(5, 7, 7), linetype = \"dashed\") +\n  annotate(\"path\", x = c(2, 2, 3), y = c(7, 9, 9), linetype = \"dashed\") +\n  annotate(\"text\", x = 0.25, y = 4.25, label = \"+2\") +\n  annotate(\"text\", x = 1.25, y = 6.25, label = \"+2\") +\n  annotate(\"text\", x = 2.25, y = 8.25, label = \"+2\") +\n  scale_y_continuous(breaks = 0:15) +\n  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))\n\n\n\n\n\n\n\n\nFigure 21.1: Illustration of the meaning of the slope: with a slope of 2, for each unit increase of \\(x\\), \\(y\\) increases by 2.\n\n\n\n\n\nNow, in the context of research, you usually start with a sample of measures (values) of \\(x\\) (the predictor variable) and \\(y\\) (the outcome variable). Then you have to estimate (i.e. to find the values of) \\(\\beta_0\\) and \\(\\beta_1\\) from the formula. This is what regression models are for: given the sampled values of \\(y\\) and \\(x\\), the model estimates \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\n\n\n\nExercise\n\n\n\n\nGo to the web app Linear Models Illustrated.\nIn the first tab, “Continuous”, you will find instructions on the left and a plot on the right. The plot on the right is the plot resulting from the parameters specified to the left.\nPlay around with the intercept and slope parameters to see what happens to the line with different values of \\(\\beta_0\\) and \\(\\beta_1\\).",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html#add-error",
    "href": "ch-regression-intro.html#add-error",
    "title": "21  Introduction to regression",
    "section": "21.3 Add error",
    "text": "21.3 Add error\nMeasurements are noisy: they usually contain errors. Error can have many different causes (for example, measurement error due to technical limitations or variability in human behaviour), but we are usually not that interested in learning about those causes. Rather, we just want our model to be able to deal with error. Let’s see what errors looks like. Figure 21.2 shows values of \\(y\\) simulated with the equation \\(y = 1 + 1.5x\\) (with \\(x\\) equal 1 to 10), to which the random error \\(\\epsilon\\) was added. Due to the added error, the points are almost on the straight line defined by \\(y = 1 + 1.5x\\), but not quite. The vertical distance between the observed points and the expected line, called the regression line, is the residual error (red lines in the plot).\n\n\nCode\nset.seed(4321)\nx &lt;- 1:10\ny &lt;- (1 + 1.5 * x) + rnorm(10, 0, 2)\n\nline &lt;- tibble(\n  x = x,\n  y = y\n)\n\nm &lt;- lm(y ~ x)\nyhat &lt;- m$fitted.values\ndiff &lt;- y - yhat\nggplot(line, aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = y, yend = yhat), colour = \"red\") +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(title = bquote(italic(y) == 1 + 1.5 * italic(x) + epsilon))\n\n\n\n\n\n\n\n\nFigure 21.2: Illustration of residual error.\n\n\n\n\n\nWhen taking into account error, the equation of a regression model becomes the following:\n\\[\ny = \\beta_0 + \\beta_1 * x + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is the error. In other words, \\(y\\) is the sum of \\(\\beta_0\\), \\(\\beta_1 * x\\) and some error. In regression modelling, the error \\(\\epsilon\\) is assumed to come from a Gaussian distribution with mean 0 and standard deviation \\(\\sigma\\) when \\(y\\) is assumed to be generated by a Gaussian distribution: \\(Gaussian(\\mu = 0, \\sigma)\\).\n\\[\ny = \\beta_0 + \\beta_1 * x + Gaussian(0, \\sigma)\n\\]\nThis equation can be rewritten like so (since the mean of the Gaussian error is 0):\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 * x\\\\\n\\end{align}\n\\]\nYou can read those formulae like so: “The variable \\(y\\) is distributed according to a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The mean \\(\\mu\\) is equal to the intercept \\(\\beta_0\\) plus the slope \\(\\beta_1\\) times the variable \\(x\\).” This is a Gaussian regression model, because the assumed family of the outcome \\(y\\) is Gaussian. Now, the goal of a (Gaussian) regression model is to estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) from the data (i.e. from the values of \\(x\\) and \\(y\\)). Of course, regression models are not limited to the Gaussian distribution family and in fact regression models can be fit with virtually any other distribution family (the other most common families are Bernoulli, Poisson, beta and cumulative). In the following chapters, you will learn how to fit regression models to a variety of data to answer linguistic research questions.\n\n\n\n\n\n\nSpotlight: Regression, eugenics and racism\n\n\n\n\n\nGalton and regression\nThe basic logic of regression models is attributed to Francis Galton (1822–1911). Galton studied the relationship between the heights of parents and their children (Galton 1980, 1886). He noticed that while tall parents tended to have tall children, the children’s heights were often closer to the average height of the population. This phenomenon, which he called “regression toward mediocrity” (now known as “regression to the mean”), showed that extreme values (e.g., very tall or very short parents) were less likely to be perfectly transmitted to the next generation.\n\n\n\n\n\nThe core idea of Galton’s framework can be expressed as a regression model:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot x + \\epsilon\n\\]\nwhere:\n\n\\(y\\): the child’s height (response variable),\n\\(x\\): the average of the parents’ heights (predictor variable),\n\\(\\beta_0\\): the intercept (the expected height of a child when the parents’ height is at the mean),\n\\(\\beta_1\\)​: the slope, representing the rate of change in the child’s height with respect to the parents’ height,\n\\(\\epsilon\\): the error term, accounting for random variability.\n\nGalton found that the slope \\(\\beta_1\\) was less than 1, meaning that the children’s heights were not as extreme as their parents’ heights. For example, if tall parents (above the mean) had an average child height increase of \\(\\beta_1 &lt; 1\\), it indicated a “regression” toward the population mean. The intercept \\(\\beta_0\\) ensured the line passed through the mean of both parents’ and children’s heights.\nGalton, eugenics and racism\nGalton is considered one of the founders of modern statistics and is widely recognized for his contributions to fields such as regression, correlation, and the study of heredity. However, his work is also deeply intertwined with controversial and now discredited views on race and eugenics. Galton coined the term eugenics in 1883, defining it as the “science of improving the genetic quality of the human population”. His goal was to encourage the reproduction of individuals he deemed “fit” and discourage that of those he considered “unfit”. He promoted selective breeding among humans, drawing inspiration from animal breeding practices.\nGalton believed in a hierarchy of intelligence and ability among “races”, a belief that was common among many European intellectuals of his time. In works like Hereditary Genius (1869), he argued that intelligence and other traits were hereditary and that Europeans were superior to other racial groups. These conclusions were based on flawed assumptions and biased interpretations of data. His ideas contributed to the spread of pseudo-scientific racism, which attempted to justify inequality and colonialism.\nGalton’s eugenic ideas were later used to justify discriminatory policies, including forced sterilization programs and racial segregation in various countries. While Galton himself did not directly advocate for many of the extreme measures implemented in the 20th century, his work laid the groundwork for such abuses. His promotion of eugenics and racial hierarchies has left a damaging legacy.\n\n\n\n\n\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246. https://doi.org/10.2307/2841583.\n\n\n———. 1980. “Kinship and Correlation.” The North American Review 150 (401): 419431.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression-intro.html#footnotes",
    "href": "ch-regression-intro.html#footnotes",
    "title": "21  Introduction to regression",
    "section": "",
    "text": "Yet other notations are \\(y = a + bx\\) and \\(y = \\alpha + \\beta x\\).↩︎\nMathematically, it is called the gradient, but in regression modelling the word slope is commonly used.↩︎",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to regression</span>"
    ]
  },
  {
    "objectID": "ch-regression.html",
    "href": "ch-regression.html",
    "title": "22  Regression models",
    "section": "",
    "text": "22.1 Vowel duration in Italian: the data\nIn Chapter 21 you were introduced to regression models. Regression is a statistical model based on the equation of a straight line, with added error.\n\\[\ny = \\beta_0 + \\beta_1 \\cdot x + \\epsilon\n\\]\n\\(\\beta_0\\) is the regression line’s intercept and \\(\\beta_1\\) is the slope of the line. We have seen that \\(\\epsilon\\) is assumed to be from a Gaussian distribution with mean 0 and standard deviation \\(\\sigma\\).\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 \\cdot x\\\\\n\\end{align}\n\\]\nFrom now on, we will use the latter way of expressing regression models, because it makes it clear which distribution we assume the variable \\(y\\) to be generated by (here, a Gaussian distribution). Note that in the wild, variables very rarely are generated by Gaussian distributions. It is just pedagogically convenient to start with Gaussian regression models (i.e. regression models with a Gaussian distribution as the distribution of the outcome variable \\(y\\)) because the parameters of the Gaussian distribution, \\(\\mu\\) and \\(\\sigma\\) can be interpreted straightforwardly on the same scale as the outcome variable \\(y\\): so for example if \\(y\\) is in centimetres, then the mean and standard deviation are in centimetres, if \\(y\\) is in Hz, then the mean and SD are in Hz, and so on. Similarly, the regression \\(\\beta\\) coefficients will be on the same scale as the outcome variable \\(y\\). You will be introduced later to regression models with distributions other than the Gaussian, where the regression parameters are estimated on a different scale than that of the outcome variable \\(y\\).\nThe goal of the Gaussian regression model expressed in the formulae above is to estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) from observed data. Now, since truly Gaussian data is difficult to come by, especially in linguistics, for the sake of pedagogical simplicity we will start the learning journey on fitting regression models using data for which a Gaussian regression is generally not appropriate. You will learn in later chapters more appropriate distribution families for this data.\nWe will analyse the duration of vowels in Italian from Coretta (n.d.) and how speech rate affects vowel duration. The expectation we might have is that vowels get shorter with increasing speech rate. You will notice how this is a very vague hypothesis: how shorter do they get? Is the shortening the same across all speech rates, or does it get weaker with higher speech rates? Our expectation/hypothesis simply states that vowels get shorter with increasing speech rate. Maybe we could do better and use what we know from speech production and come up with something more precise, but this type of vague hypothesis are very common, if not standard, in language research, so we will stick to it for practical and pedagogical reasons. Remember, however, that robust research should strive for precision.\nLet’s load the R data file coretta2018a/ita_egg.rda. It contains several phonetic measurements obtained from audio and electroglottographic recordings. You can find the information on the data here: Electroglottographic data on Italian.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(\"data/coretta2018a/ita_egg.rda\")\n\nita_egg\nLet’s plot vowel duration and speech rate in a scatter plot. The relevant columns in the tibble are v1_duration and speech_rate. TODO: check that geom_smooth was done earlier. Figure 22.1 shows speech rate on the x-axis and vowel duration on the y-axis. The points in the plot are the individual observations (measurements) of vowels in the 19 speakers of Italian. The plot also includes a regression line, generated by geom_smooth(method = \"lm\"). By glancing at the individual points, we can see a negative relationship between speech rate and vowel duration: vowels get shorter with greater speech rate. This is reflected by the regression line, which has a negative slope.\nita_egg |&gt; \n  ggplot(aes(speech_rate, v1_duration)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Speech rate (syllables per second)\",\n    y = \"Vowel duration (ms)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 15 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 22.1: Relationship between speech rate (as number of syllables per second) and vowel duration (in milliseconds) in 19 Italian speakers.\nYou might have noticed a warning about missing values. This is because some observations of vowel duration (v1_duration) in the data are missing (i.e. they are NA, “Not Available”). Let’s drop them from the tibble with drop_na(). We will use ita_egg_clean for the rest of the tutorial.\nita_egg_clean &lt;- ita_egg |&gt; \n  drop_na(v1_duration)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#vowel-duration-in-italian-the-data",
    "href": "ch-regression.html#vowel-duration-in-italian-the-data",
    "title": "22  Regression models",
    "section": "",
    "text": "22.1.1 The model\nLet’s move on onto fitting a Gaussian regression model to vowel duration as the outcome variable and speech rate as the predictor. We are assuming that vowel duration follows a Gaussian distribution (although as mentioned above this is not the case, but it will do for now). Here is the model we will fit in mathematical notation.\n\\[\n\\begin{align}\n\\text{vdur} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 \\cdot \\text{sr}\\\\\n\\end{align}\n\\]\n\nVowel duration (\\(\\text{vdur}\\)) is distributed (\\(\\sim\\)) according to a Gaussian distribution (\\(Gaussian(\\mu, \\sigma)\\)).\nThe mean \\(\\mu\\) is equal to the sum of \\(\\beta_0\\) (the intercept) and the product of \\(\\beta_1\\) and speech rate (\\(\\beta_1 \\cdot \\text{sr}\\)). The formula of \\(\\mu\\) is the equation of a straight line, aka the linear equation. This is why regression models are also called linear models.\n\nThe regression model estimates the parameters in the mathematical formulae: parameters to be estimated in regression models are usually represented with Greek letters (hence why we adopted this notation for the linear equation). Since \\(\\mu\\) is the sum of terms with parameters, the model estimates those parameters directly. So in total, the regression model represented in the formulae above has to estimate the following three parameters:\n\nThe regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nThe standard deviation of the Gaussian distribution, \\(\\sigma\\).\n\nTo instruct R to model vowel duration as a function of the numeric predictor speech rate you simply add it to the 1 we have used in the right-hand side of the tilde in Chapter 20 (i.e. v1_duration ~ 1): so v1_duration ~ 1 + speech_rate. Now it should be clear what the 1 is for: it stands for the intercept, \\(\\beta_0\\). You can also think about it as the \\(1\\) in the revised formula below:\n\\[\n\\begin{align}\n\\text{vdur} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 \\cdot 1 + \\beta_1 \\cdot \\text{sr}\\\\\n\\end{align}\n\\]\nOf course, \\(\\beta_0 \\cdot 1\\) is the same as \\(\\beta_0\\). The R formula is based on what you multiply the coefficients with in the mathematical formula: \\(1\\) and \\(sr\\). While the predictor \\(sr\\) can take different values, the \\(1\\) is constant so it is also called the constant term, or the intercept term. In the R formula, you don’t need to explicitly include the coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Put all this together and you get 1 + speech_rate. There is more: in R, since the \\(1\\) is constant you can omit it! So v1_duration ~ 1 + speech_rate can also be written as v1_duration ~ speech_rate. They are equivalent.\nNow that we have clarified how the R formula is set up, here is the full code to fit a Gaussian regression model of vowel duration with brms.\n\nlibrary(brms)\n\nvow_bm &lt;- brm(\n  # `1 +` can be omitted.\n  v1_duration ~ 1 + speech_rate,\n  # v1_duration ~ speech rate,\n  family = gaussian,\n  data = ita_egg_clean\n)\n\n\n\n\n\n\n\nR Note: The rethinking package\n\n\n\n\n\nXXX",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#interpret-the-model-summary",
    "href": "ch-regression.html#interpret-the-model-summary",
    "title": "22  Regression models",
    "section": "22.2 Interpret the model summary",
    "text": "22.2 Interpret the model summary\nTo obtain a summary of the model, use the summary() function.\n\nsummary(vow_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: v1_duration ~ speech_rate \n   Data: ita_egg_clean (Number of observations: 3253) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     198.47      3.33   191.76   204.97 1.00     3681     2274\nspeech_rate   -21.73      0.62   -22.93   -20.49 1.00     3623     2421\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    21.66      0.27    21.14    22.19 1.00     3855     2615\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s focus on the “Regression Coefficients” table of the summary. To understand what they are, just remember the equation of line and the model formula above.\n\nIntercept is \\(\\beta_0\\): this is the mean vowel duration, when speech rate is 0.\nspeech_rate is \\(\\beta_1\\): this is the change in vowel duration for each unit increase of speech rate.\n\nThis should make sense, if you understand the equation of a line: \\(y = \\beta_0 + \\beta_1 x\\). If you are still uncertain, play around with the web app.\nRecall that the Estimate and Est.Error column are simply the mean and standard deviation of the posterior probability distributions of the estimate of Intercept and speech_rate respectively.\nLooking at the 95% Credible Intervals (CrIs), we can say that based on the model and data:\n\nThe mean vowel duration, when speech rate is 0 syl/s, is between 192 and 205 ms, at 95% confidence.\nWe can be 95% confident that, for each unit increase of speech rate (i.e. for each increase of one syllable per second), the duration of the vowel decreases by 20.5-23 ms.\n\nTo see what the posterior probability densities of \\beta_0, \\beta_1 and \\sigma look like, you can quickly plot them with the plot() function.\n\nplot(vow_bm)\n\n\n\n\n\n\n\n\nIf you prefer to see density plots instead of histograms, you can specify the combo argument.\n\nplot(vow_bm, combo = c(\"dens\", \"trace\"))",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#plot-the-model-predictions",
    "href": "ch-regression.html#plot-the-model-predictions",
    "title": "22  Regression models",
    "section": "22.3 Plot the model predictions",
    "text": "22.3 Plot the model predictions\nYou should always also plot the model predictions, i.e. the predicted values of vowel duration based on the model predictors (here just speech_rate).\nYou will learn more advanced methods later on, but for now you can use conditional_effects() from the brms package.\n\nconditional_effects(vow_bm, effects = \"speech_rate\")\n\n\n\n\n\n\n\n\nIf you wish to include the raw data in the plot, you can wrap conditional_effects() in plot() and specify points = TRUE. Any argument that needs to be passed to geom_point() (these are all ggplot2 plots!) can be specified in a list as the argument point_args. Here we are making the points transparent.\n\nplot(conditional_effects(vow_bm, effects = \"speech_rate\"), points = TRUE, point_args = list(alpha = 0.1))",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#reproducible-model-fit",
    "href": "ch-regression.html#reproducible-model-fit",
    "title": "22  Regression models",
    "section": "22.4 Reproducible model fit",
    "text": "22.4 Reproducible model fit\nXXX Before you fit the model, create a folder called cache/ folder in the RStudio project folder. We will use this folder to save the output of model fitting so that you don’t have to refit the model every time. This is useful because as models get more and more complex, they can take quite a while to fit.\nThe model will be fitted and saved in cache/ with the file name vow_bm.rds. If you now re-run the same code again, you will notice that brm() does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works! Check the contents of cache/ to see for yourself.).\n\n\n\n\n\n\nImportant\n\n\n\nWhen you save the model fit to a file, R does not keep track of changes in the model specification, so if you make changes to the formula or data, you need to delete the saved model file before re-running the code for the changes to have effect!",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#simulating-gaussian-data",
    "href": "ch-regression.html#simulating-gaussian-data",
    "title": "22  Regression models",
    "section": "22.5 Simulating Gaussian data",
    "text": "22.5 Simulating Gaussian data\nTo make things a little bit more worldly, we will simulate data of human adult weight and height. We assume that height is distributed according to a Gaussian distribution with \\(\\mu = 165\\) cm and \\(\\sigma = 8\\). Based on height, we simulate weight as \\(0.4 * height\\) plus Gaussian error with mean 0 and standard deviation 2.\n\\[\n\\begin{align}\nw & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = 0.4 * h\\\\\n\\sigma & = 2\n\\end{align}\n\\]\n\nlibrary(tidyverse)\n\nset.seed(62854)\n\nh &lt;- round(rnorm(200, 165, 8))\nw &lt;- 0.4 * h + rnorm(200, 0, 2)\n\nweight &lt;- tibble(h, w)\n\nThe code uses the rnorm() function which generates a random sample of values from a Gaussian distribution. The function takes three arguments:\n\nThe number of values to sample, here 200.\nThe mean of the Gaussian distribution, here 165.\nThe standard deviation of the Gaussian distribution, here 8.\n\n\n\n\n\n\n\nNote\n\n\n\nThe name of the rnorm() function is composed of:\n\nr for random.\nnorm for “normal”, the Gaussian distribution.\n\n\n\nWe use the round() function to round the values generated by rnorm() to the nearest integer.\nThe following plot shows the density curve of the simulated height data. The purple vertical line is the mean.\n\nweight |&gt; \n  ggplot(aes(h)) +\n  geom_density(fill = \"darkgreen\", alpha = 0.2) +\n  geom_rug() +\n  geom_vline(aes(xintercept = mean(h)), colour = \"purple\", linewidth = 1)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression.html#whats-next",
    "href": "ch-regression.html#whats-next",
    "title": "22  Regression models",
    "section": "22.6 What’s next",
    "text": "22.6 What’s next\nIn this post you have learned the very basics of Bayesian regression models. As mentioned above, regression models with brms are very flexible and you can easily fit very complex models with a variety of distribution families (for a list, see ?brmsfamily; you can even define your own distributions!).\nThe perk of using brms is that you can just learn the basics of one package and one approach and use it to fit a large variety of regression models.\nThis is different from the standard frequentist approach, where different models require different packages or functions, with their different syntax and quirks.\nIn the following posts, you will build your understanding of Bayesian regression models, which will enable you to approach even the most complex models! However, due to time limits you won’t learn everything there is to learn.\nDeveloping conceptual and practical skills in quantitative methods is a long-term process and unfortunately one semester will not be enough. So be prepared to continue your learning journey for years to come!\n\n\n\n\nCoretta, Stefano. n.d. “Vowel Duration, Voicing Duration, and Vowel Height: Acoustic and Articulatory Data from Italian [Research Compendium].” https://doi.org/10.17605/OSF.IO/XDGFZ.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regression models</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html",
    "href": "ch-regression-draws.html",
    "title": "23  Wrangling MCMC draws",
    "section": "",
    "text": "23.1 MCMC what?\nBayesian regression models fitted with brms/Stan use the Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate the probability distributions of the model’s coefficient. You have first encountered MCMC in Chapter 20. The MCMC algorithm does two things: first it finds a probability area that is the most compatible with the prior probabilities and the probability of the data given the priors; then, it sample values (called draws) from this high-probability area to reconstruct the posterior distribution. Since the high-probability area is assumed to be representative of the posterior distribution (that’s why we sample from it), the MCMC draws are also called posterior draws. Elizabeth Pankratz has written a nice short introduction to MCMC algorithms here: Finding posteriors through sampling. I strongly recommend that you read that before proceeding (if you want to further your understanding of MCMC, the resources linked on that page are an excellent starting point). Note that to be a proficient user of brms and Bayesian regression models, you don’t need to fully understand the mathematics behind MCMC algorithms, as long as you understand them conceptually.\nWhen you run a model with brms, the draws (i.e. the sampled values) are stored in the model object. All operations on a model, like obtaining the summary(), are actually operations on those draws. We normally fit regression models with four MCMC chains. The sampling algorithm within each chain runs for 2000 iterations by default. The first half (1000 iterations) are used to “warm up” the algorithm (i.e. find the high-probability area) while the second half (1000 iterations) are the ones that sample from the high-probability area to reconstruct the posterior distribution. For four chains ran with 2000 iterations of which 1000 for warm up, we end up with 4000 iterations we can use to learn details about the posterior.\nThe rest of the post will teach you how to extract and manipulate the model’s draws. We will first revisit the model fitted in Chapter 25.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#extract-mcmc-posterior-draws",
    "href": "ch-regression-draws.html#extract-mcmc-posterior-draws",
    "title": "23  Wrangling MCMC draws",
    "section": "23.2 Extract MCMC posterior draws",
    "text": "23.2 Extract MCMC posterior draws\nThere are different ways to extract the MCMC posterior draws from a fitted model. In this book, we will use the as_draws_df() function from the posterior package. The function extracts the draws from a Bayesian regression model and outputs them as a data frame. Let’s take the rt_bm_1 model from Chapter 25 and extract the draws. Let’s first read the tucker2019/mald_1_1.rds data again.\n\nlibrary(tidyverse)\n\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nNow we reload the Bayesian regression model from Chapter 25. We simply use the same code we used in that chapter. If you run the code now, the file with the model will be loaded into R, so the model will not be fitted from scratch. This is both convenient and it ensure the code is reproducible.\n\nlibrary(brms)\n\nrt_bm_1 &lt;- brm(\n  RT ~ IsWord,\n  family = gaussian,\n  data = mald,\n  seed = 6725,\n  file = \"cache/ch-regression-cat-rt_bm_1\"\n)\n\nLet’s review the model summary.\n\nsummary(rt_bm_1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     953.14      6.12   941.17   965.17 1.00     4590     2897\nIsWordFALSE   116.34      8.80    98.74   134.14 1.00     4815     3235\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   312.47      3.11   306.44   318.60 1.00     4720     3456\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe Regression Coefficients table includes Intercept and IsWordFALSE which is what we expect from the model formula and data. We are good to go! Now we can extract the MCMC draws from the model using the as_draws_df() function. This function from the posterior package returns a tibble with values obtained in each draw of the MCMC algorithm. Since we fitted the model with 4 MCMC chains and 1000 sampling draws per chain, there is a total of 4000 drawn values (i.e. 4000 rows in the data frame).\n\nrt_bm_1_draws &lt;- as_draws_df(rt_bm_1)\nrt_bm_1_draws\n\n\n  \n\n\n\nDon’t worry about the Intercept, lprior and lp__ columns. Open the data frame in the RStudio viewer. You will see three extra column: .chain, .iteration and .draw. They indicate:\n\n.chain: The MCMC chain number (1 to 4).\n.iteration: The iteration number within chain (1 to 1000).\n.draw: The draw number across all chains (1 to 4000).\n\nMake sure that you understand these columns in light of the MCMC algorithm. The following columns contain the drawn values at each draw for three parameters of the model: b_Intercept, b_IsWordFALSE and sigma. To remind yourself what these mean, let’s have a look at the mathematical formula of the model.\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma) \\\\\n\\mu_i        & = \\beta_0 + \\beta_1 \\cdot w_i \\\\\n\\end{align}\n\\]\nSo:\n\nb_Intercept is \\(\\beta_0\\). This is the mean RT when the the word is a real word.\nb_IsWordFALSE is \\(\\beta_1\\). This is the difference in RT between nonce words and real words.\nsigma is \\(\\sigma\\). This is the overall standard deviation of the RTs.\n\nAny inference made on the basis of the model are inferences derived from the draws. One could say that the model “results” are, to put it simply, these draws and that the draws can be used to make inferences about the population one is investigating (if one is interested in inference, in the first place).",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#summary-measures-of-the-posterior-draws",
    "href": "ch-regression-draws.html#summary-measures-of-the-posterior-draws",
    "title": "23  Wrangling MCMC draws",
    "section": "23.3 Summary measures of the posterior draws",
    "text": "23.3 Summary measures of the posterior draws\nThe Regression Coefficients table from the summary() of the model reports summary measures calculated from the drawn values of b_Intercept and b_IsWordFALSE. These summary measures are the mean (Estimate), the standard deviation (Est.error) and the lower and upper limits of the 95% Credible Interval (CrI). Since we have now the draws, we can of course obtain those same measures ourselves from the data frame with the draws! We will use the quantile2() function from the posterior package to obtain 95% and 80% CrIs.\n\nlibrary(posterior)\n\nThis is posterior version 1.6.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nrt_b1_summaries &lt;- rt_bm_1_draws |&gt; \n  summarise(\n    intercept_mean = mean(b_Intercept), intercept_sd = sd(b_Intercept),\n    intercept_lo_95 = quantile2(b_Intercept, 0.025), intercept_up_95 = quantile2(b_Intercept, 0.975),\n    IsWordFALSE_mean = mean(b_IsWordFALSE), IsWordFALSE_sd = sd(b_IsWordFALSE),\n    IsWordFALSE_lo_95 = quantile2(b_IsWordFALSE, 0.025), IsWordFALSE_up_95 = quantile2(b_IsWordFALSE, 0.975),\n  ) |&gt; \n  # fancy way of mutating multiple columns\n  mutate(\n    across(everything(), ~round(., 2))\n  )\nrt_b1_summaries\n\n\n  \n\n\n\nCompare the values obtained now with the values in the model summary above. They are the same, because the summary measures in the model summary are simply summary measures of the draws. Hopefully now it is clear where the values in the Regression Coefficient table come from and how these are related to the MCMC draws.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#plotting-posterior-draws",
    "href": "ch-regression-draws.html#plotting-posterior-draws",
    "title": "23  Wrangling MCMC draws",
    "section": "23.4 Plotting posterior draws",
    "text": "23.4 Plotting posterior draws\nPlotting posterior draws is as straightforward as plotting any data. You already have all of the tools to understand plotting draws with ggplot2. To plot the reconstructed posterior probability distribution of any parameter, we plot the probability density (with geom_density()) of the draws of that parameter. Let’s plot b_IsWordFALSE. This will be the posterior probability density of the difference in RTs between nonce (IsWord is FALSE) and real words (IsWord is TRUE).\n\nrt_bm_1_draws |&gt;\n  ggplot(aes(b_IsWordFALSE)) +\n  geom_density() +\n  geom_rug(alpha = 0.2)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-draws.html#calculate-posterior-predictions-from-the-draws",
    "href": "ch-regression-draws.html#calculate-posterior-predictions-from-the-draws",
    "title": "23  Wrangling MCMC draws",
    "section": "23.5 Calculate posterior predictions from the draws",
    "text": "23.5 Calculate posterior predictions from the draws\nIn Chapter 26, you learned how to use indexing of categorical predictors to estimate the mean of the outcome variable for each level in the categorical predictor. You might be wondering: with contrasts you get the mean of the reference level and the difference between the second level and the reference level; with indexing you get the mean at each level; however, all of these estimands (the mean of all levels and the difference between the levels) are important for inference. So, should you fit the model twice to obtain all estimands? Of course, that is not necessary because independent from the way you code categorical predictors, the model will have all the information needed to calculate all of the estimands of interest. This information is in the MCMC draws. In this section, you will learn how to obtain the estimated mean RT when the word is a nonce word from the model that used contrasts.\nLet’s revise the formula of the mean \\(\\mu\\) from the model formula above: \\(\\mu_i = \\beta_0 + \\beta_1 \\cdot w_i\\). The mean depends on the value of \\(w\\) (that’s what the subscript \\(i\\) is for). We can substitute \\(w_i\\) with 0 for IsWord = TRUE and with 1 for IsWord = FALSE. So we get \\(\\mu_T = \\beta_0\\) for TRUE and \\(\\mu_F = \\beta_0 + \\beta_1\\). Look again at rt_bm_1_draws. Recall that b_Intercept is \\(\\beta_0\\) and b_IsWordFALSE is \\(\\beta_1\\). Can you figure out what you need to do to get the posterior probability distribution of \\(\\mu_F\\) (i.e. the mean RT when IsWord is FALSE)? You sum the columns b_Intercept and b_IsWordFALSE! It goes without saying that the posterior probability distribution of the mean RT when IsWord is TRUE is the probability density of b_Intercept.\n\nrt_bm_1_draws &lt;- rt_bm_1_draws |&gt; \n  mutate(\n    # predicted RT for real words\n    real = b_Intercept,\n    # predicted HNR for polite attitude\n    nonce = b_Intercept + b_IsWordFALSE\n  )\n\nrt_bm_1_draws |&gt; select(real, nonce)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n\n  \n\n\n\nThe sum operation for nonce is applied row-wise, to each draw (remember, each row in the draw tibble is the draw from one iteration of the MCMC chains). So, for nonce, the value of b_Intercept at draw 1 is added to the value of b_IsWordFALSE at draw 1, and so on. You end up with a list of sums that has the same length as the initial draws (here, 4000, i.e. 1000 per chain!). Then you can summarise and plot this new list as you did with the b_ coefficients earlier. To make further processing of the draws easier, we can reshape the tibble so that instead of having two columns real and nonce with the drawn values in them, we end up with a tibble with one column IsWord (where we store the value real or nonce) and a column named value with the drawn value. Reshaping tibbles is called pivoting in tidyverse parlance. We won’t go into the details of all pivoting operations, so I strongly recommend you to look through the vignette on pivoting (a vignette is a short tutorial that some R packages provide for users to learn basic operations with the functions in the package). The operation we need to get the shape we want is pivoting from a “wider” format to a “longer” format: this is achieved with pivot_longer() from tidyr. Make sure you read the section on pivot_longer() in the vignette to understand what the following code does.\n\nrt_bm_1_draws_long &lt;- rt_bm_1_draws |&gt; \n  select(.chain, .iteration, .draw, real, nonce) |&gt; \n  pivot_longer(real:nonce, names_to = \"IsWord\", values_to = \"value\")\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\nrt_bm_1_draws_long\n\n\n  \n\n\n\nCheck the resulting rt_bm_1_draws_long tibble. The first two rows are draw 1 of chain 1: one row is for the case when the word is a real word, and the other row is for the case when the word is a nonce word. There are a total of 8000 rows: 4000 draws (1000 draws times 4 chains) times 2 (two levels of IsWord). Now let’s make a violin plot of the predicted HNR in informal and polite attitude!\n\n\n\n\n\n\n\n\nFigure 23.1: Density plot of predicted mean RT for real and nonce words.\n\n\n\n\n\nWe can use the ggdist package to plot something a bit fancier, for example a “half eye” geometry (which can be obtained with the “halfeye” statistic, stat_halfeye()). Check the ?stat_halfeye documentation to learn about it.\n\nlibrary(ggdist)\n\n\nAttaching package: 'ggdist'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nrt_bm_1_draws_long |&gt; \n  ggplot(aes(value, IsWord)) +\n  stat_halfeye() +\n  labs(x = \"Predicted RT (ms)\", y = \"Word type\")\n\n\n\n\n\n\n\nFigure 23.2: Half-eye plot of predicted mean RT for real and nonce words.\n\n\n\n\n\nAnother useful ggplot2 statistic is stat_interval().\n\nrt_bm_1_draws_long |&gt; \n  ggplot(aes(IsWord, value)) +\n  stat_interval() +\n  labs(\n    x = \"Word type\", y = \"Predicted RT (ms)\"\n  )\n\n\n\n\n\n\n\nFigure 23.3: Credible Intervals of predicted mean RT for real and nonce words.\n\n\n\n\n\nWhen reporting the results, it is recommended to report the CrIs of the predicted outcome for each level in the categorical predictor. It is useful to include tables like Table 23.1. To make the generation of the table more straightforward, we can use a couple of tricks when creating the rt_bm_cris below. The glue() function from the tidyverse package glue allows you to specify strings based on the output of R code computation. Code is included between curly braces {}.\n\nlibrary(glue)\n\nrt_bm_1_cris &lt;- rt_bm_1_draws_long |&gt; \n  mutate(IsWord = factor(IsWord, levels = c(\"real\", \"nonce\"))) |&gt; \n  group_by(IsWord) |&gt; \n  summarise(\n    mean = mean(value), sd = sd(value),\n    ci_90 = glue(\"[{round(quantile2(value, 0.05))}, {round(quantile2(value, 0.95))}]\"),\n    ci_80 = glue(\"[{round(quantile2(value, 0.1))}, {round(quantile2(value, 0.9))}]\"),\n    ci_60 = glue(\"[{round(quantile2(value, 0.2))}, {round(quantile2(value, 0.8))}]\"),\n  )\n\n\nknitr::kable(rt_bm_1_cris, col.names = c(\"Type\", \"Mean RT\", \"SD\", \"90% CrI\", \"80% CrI\", \"60% CrI\"), digits = 0)\n\n\n\nTable 23.1: Summary measures of predicted RTs by word type (in milliseconds).\n\n\n\n\n\n\nType\nMean RT\nSD\n90% CrI\n80% CrI\n60% CrI\n\n\n\n\nreal\n953\n6\n[943, 963]\n[945, 961]\n[948, 958]\n\n\nnonce\n1069\n6\n[1059, 1080]\n[1062, 1077]\n[1064, 1075]\n\n\n\n\n\n\n\n\nIn the main text, you could report the results like so:\n\nAccording to the model, the predicted RTs when the word is real word is between 1059 and 1080 ms at 90% confidence (\\(\\beta\\) = 1069, SD = 6). When the word is a nonce word, the predicted RTs are between 943 and 963 ms (\\(\\beta\\) = 953, SD = 6).",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wrangling MCMC draws</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html",
    "href": "ch-regression-cat.html",
    "title": "25  Regression models: categorical predictors",
    "section": "",
    "text": "25.1 Revisiting reaction times\nIn Chapter 22 you learned how to fit regression models of the following form in R using the brms package.\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 \\cdot x\\\\\n\\end{align}\n\\]\nIn these models, \\(x\\) was a numeric predictor, like speech rate. Numeric predictors are not the only type of predictors that a regression model can handle. Regression predictors can also be categorical: gender, age group, place of articulation, mono- vs bi-lingual, etc. However, regression model cannot handle categorical predictors directly: think about it, what would it mean to multiply \\(\\beta_1\\) by “female” or by “old”. Categorical predictors have to be re-coded as numbers.\nIn this chapter and the next we will revisit the MALD reaction times (RTs) data from Chapter 20, this time modelling RTs depending on the lexical status of the target work (real vs nonce word). This chapter will teach you the default way of including categorical predictors in regression models: numerical coding with treatment contrasts; while in Chapter 26 you will learn about an alternative way: indexing.\nLet’s read the MALD data (Tucker et al. 2019).\nlibrary(tidyverse)\n\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\nmald\nThe relevant columns are RT with the RTs in milliseconds and IsWord, the type of target word: it tells if the target word is a real English word (TRUE) or not (a nonce word, FALSE). Figure 25.1 shows the density plot of RTs, grouped by whether the target word is real or not. We can notice that the distribution of RTs with nonce (non-real) words is somewhat shifted towards higher RTs, indicating that more time is needed to process nonce words than real words.\nCode\n# Set the light theme for plots\ntheme_set(theme_light())\n\nmald |&gt; \n  ggplot(aes(RT, fill = IsWord)) +\n  geom_density(alpha = 0.8) +\n  geom_rug(alpha = 0.1) +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nFigure 25.1: Density plot of reaction times from the MALD data (Tucker et al. 2019).\nYou might also notice that the “tails” of the distributions (the left and right sides) are not symmetric: the right tail is heavier that the left tail. This is a very common characteristics of RT values and of any variable that can only be positive (like phonetic durations). These variables are “bounded” to only positive numbers. You will learn later on that the values in these variables are generated by a log-normal distribution, rather than by a Gaussian distribution (which is “unbounded”). For the time being though, we will model the data as if they were generated by a Gaussian distribution, for pedagogical reasons.\nAnother way to present a numeric variable like RTs depending on categorical variables is to use a jitter plot, like Figure 25.2. A jitter plot places dots corresponding to the values in the data on “strips”. The strips are created by randomly jittering dots horizontally, so that they don’t all fall on a straight line. The width of the strips, aka the jitter, can be adjusted with the width argument. It’s subtle, but you can see how in the range 1 to 2 seconds there are a bit more dots in nonce words (right, orange) than in real words (left, green). In other words, the density of dots in that range is greater in nonce words than real words. If you compare again the densities in Figure 25.1 above, you will notice that the orange density in the 1-2 seconds range is higher in nonce words. These are just two ways of visualising the same thing.\nmald |&gt;\n  ggplot(aes(IsWord, RT, colour = IsWord)) +\n  geom_jitter(alpha = 0.15, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nFigure 25.2: Jitter plot of reactions times for real and nonce words.\nWe can even overlay density plots on jitter plots using “violins”, like in Figure 25.3. The violins are simply mirrored density plots, placed vertically on top of the jittered strips. The width of the violins can be adjusted with the width argument, like with the jitter.\nmald |&gt;\n  ggplot(aes(IsWord, RT, fill = IsWord)) +\n  geom_jitter(alpha = 0.15, width = 0.1) +\n  geom_violin(width = 0.2) +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nFigure 25.3: Jitter and violin plot of reactions times for real and nonce words.\nNow we can obtain a few summary measures. The following code groups the data by IsWord and then calculates the mean, median and standard deviation of RTs.\nmald_summ &lt;- mald |&gt; \n  group_by(IsWord) |&gt; \n  summarise(\n    mean(RT), median(RT), sd(RT)\n  )\n\nmald_summ\nThe mean and median RTs for nonce words (IsWord = FALSE) are about 100 ms higher than the mean and median of real words. We could stop here and call it a day, but we would make the mistake of not considering uncertainty and variability: this is just one (admittedly large) sample of all the RT values that could be produced by the entire English speaking population. So we should apply inference from the sample to the population to obtain an estimate of the difference in RTs that accounts for that uncertainty and variability. The inference method we are learning is using regression modelling. The next sections will teach you how to model the RTs with regression models in brms.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression models: categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#revisiting-reaction-times",
    "href": "ch-regression-cat.html#revisiting-reaction-times",
    "title": "25  Regression models: categorical predictors",
    "section": "",
    "text": "R Note: Fill and colour aesthetics\n\n\n\n\n\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Note: Tables with kable()\n\n\n\n\n\nWith Quarto, you can output the summaries as a table, using knitr::kable(). You can learn more about this here.\n\nknitr::kable(\n  mald_summ,\n  digits = 0,\n  col.names = c(\"Is word?\", \"mean\", \"median\", \"SD\")\n)\n\n\n\nTable 25.1: Mean, median and standard deviation of RTs for real and nonce words.\n\n\n\n\n\n\nIs word?\nmean\nmedian\nSD\n\n\n\n\nTRUE\n953\n888\n291\n\n\nFALSE\n1069\n994\n333",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression models: categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#treatment-contrasts",
    "href": "ch-regression-cat.html#treatment-contrasts",
    "title": "25  Regression models: categorical predictors",
    "section": "25.2 Treatment contrasts",
    "text": "25.2 Treatment contrasts\nWe can model RTs with a Gaussian distribution (although as mentioned above, this family of distribution is generally not appropriate for variables liker RTs) with a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). This time though we want to model a different mean \\(\\mu\\) depending on the word type in IsWord. Each observation (RT value) is either of a real word or a nonce word. This is what the subscript \\(i\\) is for in the equation below and since there are 5000 observations, \\(i = [1..5000]\\) :\n\\[\nRT_i \\sim Gaussian(\\mu_i, \\sigma)\n\\]\nThe mean \\(\\mu_i\\) will depend on the specific observation \\(RT_i\\). We will see why in a second.\nNow, how do we make the model estimate a different mean depending on IsWord? There are several ways of setting this up. The default method is to use so-called treatment contrasts which involves numerical coding of categorical predictors (the coding takes the same naming as the contrasts, so the coding for treatment contrasts is treatment coding). Let’s work by example with IsWord. This is a categorical predictor with two levels: TRUE and FALSE. With treatment contrasts, one level is chosen as the reference level and the other is compared to the reference level. The reference level is automatically set as the first level in the predictor in alphabetical order. This would mean that FALSE would be the reference level, because “F” comes before “T”.\nHowever, in the mald data, the column IsWord is a factor column and the levels have been ordered so that TRUE is the first level and FALSE the second. You can see this in the Environment panel, if you click on the arrow next to mald and look next to IsWord: it will say Factor w/ 2 levels \"TRUE\",\"FALSE\". You can also check the order of the levels of a factor with the levels() function.\n\nlevels(mald$IsWord)\n\n[1] \"TRUE\"  \"FALSE\"\n\n\nYou can set the order of the levels with the factor() function. If you don’t specify the order of the levels, the alphabetical order will be used. For example:\n\nfac &lt;- tibble(\n  fac = c(\"a\", \"a\", \"b\", \"b\", \"a\"),\n  fac_1 = factor(fac),\n  fac_2 = factor(fac, levels = c(\"b\", \"a\"))\n)\n\nlevels(fac$fac_1)\n\n[1] \"a\" \"b\"\n\nlevels(fac$fac_2)\n\n[1] \"b\" \"a\"\n\n\nWe will use IsWord with the order TRUE and FALSE. This means that TRUE will be the reference level and the RTs when IsWord is FALSE will be compared to the RTs of when IsWord is TRUE. In other words, now the mean \\(\\mu_i\\) varies depending on the level of IsWord. Now we need to numerically code IsWord (i.e. use numbers to refer to the levels of the categorical predictor). With treatment contrasts, treatment coding is used to numerically code categorical predictors. Treatment coding uses indicator variables which take the values 0 or 1. Let’s make an indicator variable \\(w_i\\) that says if IsWord is TRUE, in which case \\(w = 0\\), or FALSE, in which case \\(w = 1\\). We only need one indicator variable because there are only two levels and they can be coded with a 0 and a 1 respectively. This way of setting up indicator variables is called dummy coding. See Table 25.2 for the correspondence between the predictor IsWord and the value of the indicator variable \\(w\\).\n\n\n\nTable 25.2: Treatment contrasts coding of the categorical predictor IsWord.\n\n\n\n\n\nIsWord\n\\(w\\)\n\n\n\n\nIsWord = TRUE\n0\n\n\nIsWord = FALSE\n1\n\n\n\n\n\n\nWe can now write the equation of \\(\\mu_i\\) as in the following:\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot w_i\\\\\n\\end{align}\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are two “regression coefficients”, estimated from the data by the regression model. You can think of \\(\\beta_0\\) as the model’s intercept and of \\(\\beta_1\\) as the model’s slope. Can you figure out why?\nRecall that a regression model with a numeric predictor like the one we saw in Chapter 22 is basically the formula of a line. We modelled vowel duration as a function of speech rate. The intercept of the regression line estimated by the model indicates where the line crosses the y-axis when the x value is 0: in our case, that is the vowel duration when speech rate is 0. In the model above, with RTs as a function of word type, the numeric predictor is the indicator variable \\(w\\) (which stands for the categorical predictor IsWord). The formula of a line really works just with numbers so for the formula to make sense, categorical predictors have to be made into numbers and treatment contrasts is such one way of doing so.\nNow, \\(\\beta_0\\) is the model’s intercept because that is the mean RT when \\(w\\) is 0 (like with vowel duration when speech rate is 0; can you see the parallel?). And \\(\\beta_1\\) is the model’s slope because \\(\\beta_1\\) is the change in RT for a unit increase of \\(w\\), which is when \\(w\\) goes from 0 to 1. This in turn corresponds to the change in level in the categorical predictor. Do you see the connection? It’s a bit contorted, but once you get this it should explain several aspects of regression models with categorical predictors and treatment contrasts.\nSo \\(\\mu_i\\) is the sum of \\(\\beta_0\\) and \\(\\beta_1 \\cdot w_i\\). \\(w_i\\) is 0 (when the word is real) or 1 (the the word is a nonce word). That is why we write \\(RT_i\\): the subscript \\(i\\) allows us to pick the correct value of \\(w_i\\) for each specific observation. The result of plugging in the value of \\(w_i\\) is laid out in the following formulae.\n\\[\n\\begin{align}\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot w_i\\\\\n\\mu_{\\text{T}} & = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\\\\n\\mu_{\\text{F}} & = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\n\\end{align}\n\\]\n\nWhen IsWord is TRUE, the mean RT is equal to \\(\\beta_0\\).\nWhen IsWord is FALSE, the mean RT is equal to \\(\\beta_0 + \\beta_1\\).\n\nIf \\(\\beta_0\\) is the mean RT when IsWord is TRUE, what is \\(\\beta_1\\) by itself? Simple.\n\\[\n\\begin{align}\n\\beta_1 & = \\mu_\\text{F} - \\mu_\\text{T}\\\\\n& = (\\beta_0 + \\beta_1) - \\beta_0\n\\end{align}\n\\]\n\\(\\beta_0\\) is the difference between the mean RT when IsWord is TRUE and the mean RT when IsWord is FALSE. As mentioned above, with treatment contrasts you are comparing the second level to the first. So one regression coefficient will be the mean of the reference level, but the other coefficient will be the difference between the mean of the two levels. I know this is not particularly user-friendly, but this is the default in R when using categorical predictors.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression models: categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat.html#model-rts-by-word-type",
    "href": "ch-regression-cat.html#model-rts-by-word-type",
    "title": "25  Regression models: categorical predictors",
    "section": "25.3 Model RTs by word type",
    "text": "25.3 Model RTs by word type\nFitting a regression model with a categorical predictor like IsWord is as simple as including the predictor in the model formula, to the right of the tilde ~. From now on we will stop including 1 + since an intercept term is included by default.\n\nlibrary(brms)\n\nrt_bm_1 &lt;- brm(\n  # equivalent: RT ~ 1 + IsWord\n  RT ~ IsWord,\n  family = gaussian,\n  data = mald,\n  seed = 6725,\n  file = \"cache/ch-regression-cat-rt_bm_1\"\n)\n\nTreatment contrasts are applied by default: you do not need to create an indicator variable yourself or tell brms to use that coding (which is both a blessing and a curse). The following code chunk returns the summary of the model rt_bm_1.\n\nsummary(rt_bm_1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     953.14      6.12   941.17   965.17 1.00     4590     2897\nIsWordFALSE   116.34      8.80    98.74   134.14 1.00     4815     3235\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   312.47      3.11   306.44   318.60 1.00     4720     3456\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe first few lines should be familiar: they report information about the model, like the distribution family, the formula and so on. Then we have the Regression Coefficients table. The estimates of the coefficients and the 95% Credible Intervals (CrIs) are reported in Table 25.3.\n\n\nCode\nfixef(rt_bm_1) |&gt; knitr::kable()\n\n\n\n\nTable 25.3: Regression coefficients from a model of RTs by word type.\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nIntercept\n953.1412\n6.120423\n941.16925\n965.1697\n\n\nIsWordFALSE\n116.3413\n8.800872\n98.73942\n134.1424\n\n\n\n\n\n\n\n\nThe Estimate column is the mean of the posterior distribution of the regression coefficients, and Est.Error is the standard deviation of the posterior distribution of the regression coefficients. Q2.5 and Q97.5 are the lower and upper limit of the 95% CrI of the posterior distribution of the regression coefficients. These are summary measures of (posterior) probability distributions. We can quickly plot these with mcmc_dens() from the bayesplot package:\n\nlibrary(bayesplot)\n\nmcmc_dens(rt_bm_1, pars = vars(starts_with(\"b_\")))\n\n\n\n\n\n\n\nFigure 25.4: Density plots of the posterior probability distributions of the regression coefficients of model rt_bm_1.\n\n\n\n\n\n\nb_Intercept or Intercept in the model summary is the mean RT when IsWord is TRUE. This is \\(\\beta_0\\) in the model’s mathematical formula.\nb_IsWordFALSE or IsWordFALSE in the model summary is the difference in mean between nonce and real words. This is \\(\\beta_1\\) in the model’s mathematical formula.\n\nThe posterior distributions of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) capture the uncertainty in the estimated values of those coefficients. The 95% CrIs indicate the range of values of the posterior that cover 95% of the area under the curve of the posterior, i.e. 95% of the density mass. This means that at 95% probability, the value of the coefficient is within that range.\n\nThe 95% CrI of Intercept \\(\\beta_0\\) is [941, 965] ms: this means that there is a 95% probability that the mean RT when IsWord is TRUE is between 941 and 965 ms.\nThe 95% CrI of IsWordFALSE \\(\\beta_0\\) is [99, 134] ms: this means that there is a 95% probability that the difference in mean RT between nonce and real words is between 99 and 134 ms.\n\nSo here we have our answer: at 95% confidence (another way of saying at 95% probability), based on the model and data, RTs for nonce words are 99 to 134 ms longer than RTs for real words. Now is 99-134 ms a linguistically meaningful question? Statistics cannot help with that: only a well developed linguistic mathematical model of lexical retrieval and related neurocognitive processes allows us to make any statement regarding the linguistic relevance of a particular result. This aspect applies to any statistical approach, whether frequentist of Bayesian. Within a frequentist framework, “statistical significance” is not “linguistic significance”. A difference between two groups can be statistically significant and not be linguistically meaningful and vice versa.\nAfter having obtained estimates from the model, always ask yourself: what do those estimates mean, based on our current understanding of the linguistic phenomenon we are investigating?\n\n\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression models: categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-index.html",
    "href": "ch-regression-index.html",
    "title": "26  Indexing categorical predictors",
    "section": "",
    "text": "26.1 Indexing categorical predictors\nChapter 25 showed you how to fit a regression model with a categorical predictor. We modelled reaction times as a function of word type (real or nonce). The categorical predictor IsWord (word type) was coded with so-called treatment contrasts using dummy coding: the model’s intercept is the mean of the first level and the “slope” is the difference between the second level and the first.\nAnother way to include categorical predictors in a regression model is to use indexing: with indexing, separate regression coefficients are estimated for each level in the categorical predictor. This involves so-called “one-hot encoding” of categorical predictors into numbers and no contrasts (i.e. the coefficients do not represent differences between levels).\nIn this chapter we revisit the regression model from Chapter 25 using indexing.\nWhen using treatment coding and contrasts, the model coefficients are set up this way:\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot w_i\\\\\n\\end{align}\n\\]\nwhere \\(w\\) is the indicator variable for IsWord. \\(\\beta_0\\) is the mean RT with real words and \\(\\beta_1\\) is the difference between the mean RT with nonce words and that with real words.\nWith indexing, the model mathematical specification is the following:\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_1 \\cdot W_{\\text{T}[i]} + \\beta_2 \\cdot W_{\\text{F}[i]}\\\\\n\\end{align}\n\\]\nIn this formula, \\(\\text{W}_\\text{T}\\) and \\(\\text{W}_\\text{T}\\) are indicator variables. The system these indicator variables use is known as “one-hot encoding”: each level of the categorical predictor gets an indicator variable, which is \\(1\\) if the level of the observation \\(i\\) is the level encoded by that indicator variable, otherwise 0. Table 26.1 shows the correspondence between levels of IsWord and the values of the indicator variables.\nThe regression formula has two coefficients to be estimated: \\(\\beta_1\\) and \\(\\beta_2\\). \\(\\beta_1\\) is the mean RT when IsWord is TRUE (\\(\\mu_\\text{T}\\)) and \\(\\beta_2\\) is the mean RT when IsWord is FALSE (\\(\\mu_\\text{F}\\)), as shown below by substituting the indicator variables with the values \\(0\\) or \\(1\\) depending on the level of IsWord.\n\\[\n\\begin{align}\n\\mu_\\text{T} & = \\beta_1 \\cdot 1 + \\beta_2 \\cdot 0\\\\\n& = \\beta_1\\\\\n\\mu_\\text{F} & = \\beta_1 \\cdot 0 + \\beta_2 \\cdot 1\\\\\n& = \\beta_2\\\\\n\\end{align}\n\\]\nSince treatment contrasts are the default in R, the formula RT ~ IsWord gives us treatment contrasts. So how do you instruct R to use indexing instead?\nThe syntax for indexing is not particularly intuitive: RT ~ 0 + IsWord. Why 0 +? That is the way to tell R to remove the intercept term: remember that the R formula includes a 1 + by default (even when not explicitly written out) and that 1 is the constant or intercept term? Removing the intercept term changes the implied mathematical formula for \\(\\mu\\) from \\(\\beta_0 + \\beta_1 \\cdot w_i\\) to \\(\\beta_1 \\cdot W_{\\text{T}[i]} + \\beta_2 \\cdot W_{\\text{F}[i]}\\).",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Indexing categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-index.html#indexing-categorical-predictors",
    "href": "ch-regression-index.html#indexing-categorical-predictors",
    "title": "26  Indexing categorical predictors",
    "section": "",
    "text": "Table 26.1: Indicator variables of the categorical predictor IsWord when using indexing instead of contrasts.\n\n\n\n\n\nIsWord\n\\(W_\\text{T}\\)\n\\(W_\\text{F}\\)\n\n\n\n\nIsWord = TRUE\n1\n0\n\n\nIsWord = FALSE\n0\n1",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Indexing categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-index.html#a-model-of-rts-with-indexing",
    "href": "ch-regression-index.html#a-model-of-rts-with-indexing",
    "title": "26  Indexing categorical predictors",
    "section": "26.2 A model of RTs with indexing",
    "text": "26.2 A model of RTs with indexing\nLet’s read the MALD data.\n\nlibrary(tidyverse)\n\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nNow let’s fit a Gaussian regression model of RTs depending on the lexical status of the target word (real, IsWord = TRUE, or nonce, IsWord = FALSE). As usual, we set a seed and save the model output to a file for reproducibility.\n\nlibrary(brms)\n\nrt_idx &lt;- brm(\n  # Remove the intercept term with `0 +`\n  RT ~ 0 + IsWord,\n  family = gaussian,\n  data = mald,\n  seed = 6725,\n  file = \"cache/ch-regression-index-rt_idx\"\n)\n\nNow we can inspect the model summary. Pay particular attention to the Regression Coefficients.\n\nsummary(rt_idx)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 0 + IsWord \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIsWordTRUE    953.23      6.06   940.91   964.80 1.00     4479     3039\nIsWordFALSE  1069.41      6.05  1057.81  1081.16 1.00     3968     2825\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   312.51      3.06   306.59   318.36 1.00     5565     3216\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe regression coefficients are also reported in Table 26.1.\n\n\nCode\nrt_idx_ef |&gt; knitr::kable(digits = 0)\n\n\n\n\nTable 26.2: Regression coefficients of the rt_idx model.\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nIsWordTRUE\n953\n6\n941\n965\n\n\nIsWordFALSE\n1069\n6\n1058\n1081\n\n\n\n\n\n\n\n\n\nIsWordTRUE is \\(\\beta_1\\), in other words \\(\\mu_\\text{T}\\), or the mean RT with real words. According to our data and model, there is 95% probability that the mean RT with real words is between 941 and 965 ms.\nIsWordFALSE is \\(\\beta_2\\), in other words \\(\\mu_\\text{F}\\), or the mean RT with nonce words. According to the data and model, we can be 95% confident that the mean RT with nonce words is between 1058 and 1081 ms.\n\nWhile in the model we fitted in Chapter 25 one of the regression coefficients gave us the difference in mean RT between the two word conditions, in the model we just fitted we don’t get this information from the summary. Instead, you will have to calculate the difference between the two levels of IsWord by wrangling the MCMC draws of the model. This will be covered in Chapter 23, so for now focus on understanding the concepts and application of treatment contrasts vs indexing.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Indexing categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-index.html#why-indexing",
    "href": "ch-regression-index.html#why-indexing",
    "title": "26  Indexing categorical predictors",
    "section": "26.3 Why indexing?",
    "text": "26.3 Why indexing?\nYou might be asking yourself: if I am interested in the difference between real and nonce words, why should I use indexing instead of treatment contrasts?\nIn the simple case of a model with one categorical predictor with two levels, it might seem a overhead to have to further process the model to get the difference of interest when with treatment contrasts the difference is directly estimated by the model. But in most cases you will have more than two levels and more than one predictor! In those models, it is not possible to get all of the logical “contrasts” be estimated by the model and you will still have to process the model’s MCMC draws to get the comparisons you need. When the model estimates the means of different levels, it is more straightforward to process the draws. You will see examples in later chapters.\nAnother advantage of using indexing is related to specifying prior probability distributions. So far, we have used the default priors set by brms and we will continue to do so. However, in real research contexts it is important to think about priors and to specify custom priors, tailored for the specific context. It is much easier to think about prior probability distributions of means than of differences between means: with treatment contrasts, you would have to specify a prior probability distribution for the mean of the first level and a prior probability distribution for the difference between the second and first level.\nThe next chapter, Chapter 27, will illustrate models with one categorical predictor that has three levels. The chapter will show you both a model that uses treatment contrasts and the same model but with indexing of the categorical predictor. It is important to be familiar with both ways of including categorical predictors, but we will drop treatment contrasts after that and stick with indexing.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Indexing categorical predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html",
    "href": "ch-regression-more.html",
    "title": "27  More than two levels",
    "section": "",
    "text": "27.1 Mixean Basque VOT\nChapter 25 and Chapter 26 showed you how to fit a regression model with a categorical predictor. We modelled reaction times as a function of word type (real or nonce) from the MALD dataset (Tucker et al. 2019). The categorical predictor IsWord (word type: real or nonce) was included with treatment contrasts in Chapter 25 and using indexing in Chapter 26: in the former case, the model’s intercept is the mean of the first level and the “slope” is the difference between the second level and the first; in the latter case, the model estimated the mean of the two levels of IsWord.\nIn this chapter we will look at new data, Voice Onset Time (VOT) of Mixean Basque (Egurtzegi and Carignan 2020), to illustrate a categorical predictor with three levels.\nThe data egurtzegi2020/eu_vot.csv contains measurements of VOT from 10 speakers of Mixean Basque (Egurtzegi and Carignan 2020). Mixean Basque contrasts voiceless unaspirated, voiceless aspirated and voiced stops. Let’s read the data.\nlibrary(tidyverse)\n\neu_vot &lt;- read_csv(\"data/egurtzegi2020/eu_vot.csv\")\neu_vot\nThe VOT should increase from voiced to voiceless unaspirated to voiceless aspirated stops. We can use a Gaussian regression model to assess whether the data is compatible with our expectations. The eu_vot data has a voicing column that tells only if the stop is voiceless or voiced, but we need a column that further differentiates between unaspirated and aspirated voiceless stops. We can create a new column, phonation depending on the phone, using the case_when() function inside mutate().\ncase_when() works like an extended ifelse() function: while ifelse() is restricted to two conditions (i.e. when something is TRUE or FALSE), case when allows you to specify many conditions. The general syntax for the conditions in case_when() is condition ~ replacement where condition is a matching statement and replacement is the value that should be returned when there is a match. In the following code, we use case_when() to match specific phones in the phone column and based on that we return voiceless, voiced or aspirated. These values are saved in the new column phonation. We also convert the VOT values from seconds to milliseconds by multiply the VOT by 1000 in a new column VOT_ms.\neu_vot &lt;- eu_vot |&gt; \n  mutate(\n    phonation = case_when(\n      phone %in% c(\"p\", \"t\", \"k\") ~ \"voiceless\",\n      phone %in% c(\"b\", \"d\", \"g\") ~ \"voiced\",\n      phone %in% c(\"ph\", \"th\", \"kh\") ~ \"aspirated\"\n    ),\n    # convert to milliseconds\n    VOT_ms = VOT * 1000\n  )\nFigure 27.1 shows the densities of the VOT values for voiced, voiceless (unaspirated) and (voiceless) aspirated stops separately. Do the densities match our expectations about VOT?\nCode\neu_vot |&gt; \n  drop_na(phonation) |&gt; \n  ggplot(aes(VOT_ms, fill = phonation)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\nFigure 27.1",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#mixean-basque-vot",
    "href": "ch-regression-more.html#mixean-basque-vot",
    "title": "27  More than two levels",
    "section": "",
    "text": "Exercise 1\n\n\n\nRecreate the following plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe fill legend is not really needed, since the x-axis already separates the different phonations types, but different fill colours can help with the overall legibility of the violins since they highlight the area covered by the violin shape.\nTo remove the legend, you should use the theme() function. Check the documentation of ?theme and search online for the argument value that hides the legend.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHave you tried legend.position in theme()?\n\n\n\n\n\n\nShow me\n\n\n\n\n\n\neu_vot |&gt; \n  drop_na(phonation) |&gt; \n  ggplot(aes(phonation, VOT_ms, fill = phonation)) +\n  geom_jitter(alpha = 0.1, width = 0.2) +\n  geom_violin(alpha = 0.8, width = 0.2) +\n  labs(x = \"Phonation\", y = \"VOT (ms)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nCalculate appropriate measures of central tendency and dispersion of VOT depending on the phonation type.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#treatment-contrasts-with-three-levels",
    "href": "ch-regression-more.html#treatment-contrasts-with-three-levels",
    "title": "27  More than two levels",
    "section": "27.2 Treatment contrasts with three levels",
    "text": "27.2 Treatment contrasts with three levels\nLet’s proceed with modelling VOT. We will assume that VOT values follow a Gaussian distribution: as with reaction times, this is just a pedagogical step for you to get familiar with fitting models with categorical predictors, but other distribution families might be more appropriate. While for reaction times there are some recommended distributions (which you will learn about in XXX), there are really no recommendations for VOT, so a Gaussian distribution will have to do for now.\nBefore fitting the model, it is important to go through the model’s mathematical formula and to pay particular attention to how phonation type is coded using treatment contrasts. The phonation predictor has three levels: aspirated, voiced and voiceless. The order of the levels follows the alphabetical order. You will remember from Chapter 25 that the mean of the first level of a categorical predictor ends up being the intercept of the model while the difference of the second level relative to the first is the slope. With a third level, the model estimates another slope, which is the difference between the third level and the first. With treatment contrasts, the second and higher levels of a categorical predictor are compared (or contrasted) with the first level. With the default alphabetical order, this means that the intercept of the model will tell us the mean VOT of aspirated stops, and the mean of voiced and voiceless stops will be compared to that of aspirated stops.\nAn important aspect of treatment coding of categorical predictors that we haven’t discussed is the number of indicator variables needed: the number of indicator variables is always the number of the levels of the predictor minus one (\\(N - 1\\), where \\(N\\) is the number of levels). It follows that a predictor with three levels needs two indicator variables (\\(N = 3\\), \\(3 - 1 = 2\\)). This is illustrated in Table 27.1.\n\n\n\nTable 27.1: Treatment contrasts coding of the categorical predictor phonation.\n\n\n\n\n\nphonation\n\\(ph_\\text{VD}\\)\n\\(ph_\\text{VL}\\)\n\n\n\n\nphonation = aspirated\n0\n0\n\n\nphonation = voiced\n1\n0\n\n\nphonation = voiceless\n0\n1\n\n\n\n\n\n\nNow that we know how phonation is coded, we can look at the model formula.\n\\[\n\\begin{align}\n\\text{VOT}_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot ph_{\\text{VD}[i]} + \\beta_2 \\cdot ph_{\\text{VL}[i]}\\\\\n\\end{align}\n\\]\nThe formula states that each observation of VOT come from a Gaussian distribution with a mean and standard deviation and that the mean depends on the value of the indicator variables \\(ph_\\text{VD}\\) and \\(ph_\\text{VL}\\): that is what the subscript \\(i\\) is for.\n\n\n\n\n\n\nExercise 3\n\n\n\nWork out the formula of the mean VOT for each level of phonation by substituting the correct value for \\(ph_\\text{VD}\\) and \\(ph_\\text{VL}\\).\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor example, for phonation = aspirated:\n\\[\n\\begin{align}\n\\mu_i & = \\beta_0 + \\beta_1 \\cdot ph_{\\text{VD}[i]} + \\beta_2 \\cdot ph_{\\text{VL}[i]}\\\\\n& = \\beta_0 + \\beta_1 \\cdot 0 + \\beta_2 \\cdot 0\\\\\n& = \\beta_0\n\\end{align}\n\\]\nSo the mean VOT with aspirated stops is \\(\\beta_0\\).\n\n\n\nThe code below fits a Gaussian regression model, with VOT (in milliseconds) as the outcome variable and phonation type as the (categorical) predictor. Phonation type (phonation) is coded with treatment contrasts. Before fitting the model, answer the question in Quiz 1 below.\n\nlibrary(brms)\n\nvot_bm &lt;- brm(\n  VOT_ms ~ phonation,\n  family = gaussian,\n  data = eu_vot,\n  seed = 6725,\n  file = \"cache/ch-regression-more-vot_bm\"\n)\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nHow many regression coefficients will there be in the summary of the model below?\n\n 2 3 4\n\n\n\nWhen you run the model you will see this message: Warning: Rows containing NAs were excluded from the model.. This is really nothing to worry about: it just warns you that rows that have NAs were dropped before fitting the model. Of course, you could also drop them yourself in the data and feed the filtered data to the model. This is probably a better practice because it gives you the opportunity to explicitly find out which rows have NAs (and why).\n\n\n\n\n\n\nQuiz 1\n\n\n\nBased on the density plots of VOT you made above, which of the following sets of expectations makes sense?\n\n \\(\\beta_0, \\beta_1, \\beta_2\\) should have negative values. \\(\\beta_0\\) should have positive values and \\(\\beta_1, \\beta_2\\) should have negative values. \\(\\beta_0, \\beta_1\\)$ should have positive values, $\\(\\beta_2\\) should have negative values.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can just check the summary of the model. Does the summary meet your expectations?\nsummary(vot_bm)\n\n\n\nCarefully look through the Regression Coefficients of the model and make sure you understand what each raw corresponds to. It should be clear by now that while the first coefficient, the “intercept”, is the mean VOT with aspirated stops, the second and third coefficients are the difference between the mean VOT of aspirated stops and the mean VOT of voiced and voiceless stops respectively. This falls out of the formulae you worked out in Exercise 3.\nThe following paragraph shows you how you could write up the model and results in a paper.\n\nWe fitted a Bayesian regression model to Voice Onset Time (VOT) of Mixean Basque stops. We used a Gaussian distribution for the outcome and phonation (aspirated, voiced, voiceless) as the only predictor. Phonation was coded using the default treatment coding.\nAccording to the model, the mean VOT of aspirated stops is between 30 and 36 ms, at 95% probability (\\(\\beta\\) = 33, SD = 1.5). At 95% confidence, the VOT of voiced stops is 73-81 ms shorter than that of aspirated stops (\\(\\beta\\) = -77, SD = 1.86), while the VOT of voiceless stops is 10-16 ms shorter than that of aspirated stops (\\(\\beta\\) = -13, SD = 1.58).",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-regression-more.html#indexing-with-three-levels",
    "href": "ch-regression-more.html#indexing-with-three-levels",
    "title": "27  More than two levels",
    "section": "27.3 Indexing with three levels",
    "text": "27.3 Indexing with three levels\nIn Chapter 26, you learned about including categorical predictors in regression models using indexing, rather than the default treatment coding. With indexing, the regression coefficients indicate the predicted mean outcome at each level of the categorical predictor. There is nothing special about predictors with three levels: you get one coefficient per level, so three in total.\nHere is the mathematical formula of the model.\n\\[\n\\begin{align}\n\\text{VOT}_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_1 \\cdot P_{\\text{A}[i]} + \\beta_2 \\cdot P_{\\text{VD}[i]} + \\beta_3 \\cdot P_{\\text{VL}[i]}\\\\\n\\end{align}\n\\] The coefficients \\(\\beta_1, \\beta_2, \\beta_3\\) are the mean VOT for each level of phonation.\n\n\n\n\n\n\nExercise 4\n\n\n\nFigure out the coding table for the levels of phonation like we did in Chapter 26 for IsWord.\n\n\nTo fit a model of VOT by phonation using indexing, simply use the 0 + syntax, like shown in the following code.\n\nvot_bm_i &lt;- brm(\n  VOT_ms ~ 0 + phonation,\n  family = gaussian,\n  data = eu_vot,\n  seed = 6725,\n  file = \"cache/ch-regression-more-vot_bm_i\"\n)\n\nRun the model and look at the regression coefficients. Do they match your expectations? It is always good to also plot the predicted values.\n\nconditional_effects(vot_bm_i)\n\n\n\n\n\n\n\nFigure 27.2: Predicted VOT by consonant phonation in Mixean Basque based on vot_bm_i.\n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nWrite a short report of the model. Include information on the model specification, including coding of categorical predictors, and the regression coefficients. Do these results suggest three different VOT categories?\n\n\n\n\n\n\nEgurtzegi, Ander, and Christopher Carignan. 2020. “An Acoustic Description of Mixean Basque.” The Journal of the Acoustical Society of America 147 (4): 27912802. https://doi.org/10.1121/10.0000996.\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley, Filip Nenadić, and Michelle Sims. 2019. “The Massive Auditory Lexical Decision (MALD) Database.” Behavior Research Methods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>More than two levels</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html",
    "href": "ch-null-ritual.html",
    "title": "28  Frequentist statistics, the Null Ritual and p-values",
    "section": "",
    "text": "28.1 The p-value\nIn Chapter 18, you were introduced to two approaches to inference: frequentist and Bayesian statistics. You have encountered the p-value. It is very likely that you had heard of p-values before, especially when reading research literature. You will have also heard of “statistical significance”, which is based on the p-value obtained in a statistical test performed on data. This section will explain what p-values are, how they are often misinterpreted and misused (Cassidy et al. 2019; Gigerenzer 2004) , and how the “Null Ritual”, a degenerate form of statistics derived from different frequentist frameworks (yes, you can frequentist statistics in different ways!) that has become the standard in research, despite not being a coherent way of doing frequentist statistics (Gigerenzer, Krauss, and Vitouch 2004).\nTo illustrate p-values, we will compare simulated durations of vowels when followed by voiceless consonants vs voiced consonants. It is a well-known phenomenon that vowels followed by voiced consonants tend to be longer than vowels followed by voiceless ones (see review in Coretta 2019). Let’s simulate some vowel duration observations: we do so with the rnorm() function, which takes three arguments: the number of observations to sample and the mean and standard deviation of the Gaussian distribution to sample observations from. We use a \\(Gaussian(80, 10)\\) for durations before voiceless consonants, and a \\(Gaussian(90, 10)\\) for durations before voiced consonants. In other word, there is a difference of 10 milliseconds between the two means. Since the rnorm() function randomly samples from the given distribution, we have set a “seed” so that the code will return the same numbers every time it is run, for reproducibility.\nset.seed(2953)\n# Vowel duration before voiceless consonants\nvdur_vls &lt;- rnorm(15, mean = 80, sd = 10)\nvdur_voi &lt;- rnorm(15, mean = 90, sd = 10)\nNormally, you don’t know what the underlying means are, we do here because we set them. So let’s get the sample mean of vowel duration in the two conditions, and take the difference.\nmean_vls &lt;- mean(vdur_vls)\nmean_voi &lt;- mean(vdur_voi)\n\ndiff &lt;- mean_voi - mean_vls\ndiff\n\n[1] 7.43991\nThe difference in the sample means is about 7.4. Now, a frequentist statistician would define the following two hypotheses:\n\\(H_0\\) simply states that there is no difference between the mean of vowel duration when followed by voiced or voiceless consonants. This is the “null” hypothesis. For \\(H_1\\), we decide to go with “greater than 0” because we know about the trend of longer vowels before voiced consonants, so the difference should be positive. This is called the “alternative” hypothesis.\nHere is where things get tricky: if \\(H_0\\) is correct, then we should observe a difference as close to 0 as possible. Why not exactly 0? Because it is impossible for two samples (even if they come exactly from the same distribution) to have exactly the same mean for the difference to be 0. But how do we define “as close as possible”? The frequentist solution is to define a probability distribution of the difference between means centred around 0. This means that 0 has the highest probability, but that negative and positive differences around 0 are also possible.\nWilliam Sealy Gosset, a statistician, chemist and brewer who worked for Guinness, the brewery, has proposed the t-distribution as an appropriate probability distribution for differences between means. The t-distribution is similar to a Gaussian distribution, but the probability on either side of the mean declines more gently than with the Gaussian. As the Gaussian, the t-distribution has a mean and a standard deviation. It has an extra parameter: the degrees of freedom, or \\(df\\). The \\(df\\) affect how quickly the probability declines moving away from zero: the higher the \\(df\\) the more quickly the probability gets lower. This is illustrated in Figure 28.1. The figure shows four different t-distributions: what they all have in common is that the mean is 0 and the standard deviation is 1. These are called “standard” t-distributions. Where they differ is in their degrees of freedom. When the degrees of freedom are infinite (Inf), the t-distribution is equivalent to a Gaussian distribution.\n# Degrees of freedom to compare\ndfs &lt;- c(1, 2, 5, Inf)\n\n# Create data\ndata &lt;- tibble(df = dfs) |&gt;\n  mutate(data = map(df, ~ {\n    tibble(\n      x = seq(-4, 4, length.out = 500),\n      y = if (is.infinite(.x)) dnorm(seq(-4, 4, length.out = 500))\n          else dt(seq(-4, 4, length.out = 500), df = .x)\n    )\n  })) |&gt;\n  unnest(data)\n\n# Plot\nggplot(data, aes(x = x, y = y, color = as.character(df))) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"t-Distributions\",\n    x = \"t-statistic\", y = \"Density\",\n    color = \"DFs\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n  \n\n\n\n\n\n\n\nFigure 28.1: Example t-distributions with different degrees of freedom and fixed mean and standard deviation (mean = 0, sd = 1).\nWhy mean 0 and standard deviation 1? Because we can standardise the difference between the two means and always use the same standard t-distribution, so that the scale of the difference doesn’t matter: we could be comparing milliseconds, or Hertz, or kilometres. To standardise the difference between two means, we calculate the t-statistic. The t-statistic is a standardised difference. Here’s the formula:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nWe have the means of vowel duration before voiced and voiceless consonants and we know the sample size (15 observations per group), so we just need to calculate the variance.\nvar_vls &lt;- sd(vdur_vls)^2 # also var(vdur_vls)\nvar_voi &lt;- sd(vdur_voi)^2 # also var(vdur_voi)\n\ntstat &lt;- (mean_voi - mean_vls) / sqrt((var_voi / 15) + (var_vls / 15))\n\ntstat\n\n[1] 2.437442\nSo the t-statistic for our calculated difference is 2.4374424. The frequentist statistician would then ask: what is the probability of finding a t-statistic (and a difference) this large or larger, assuming that the t-statistic (and the difference) is 0. This probability is the p-value. You should note two things:\nThe next step is thus to obtain the probability of \\(t \\geq\\) 2.4374424 (t being equal or greater than 2.4374424), given a standard t-distribution. Before we can do this we need to pick the degrees of freedom of the distribution, because of course these affect the probability. The degrees of freedom are calculated based on the data with the following, admittedly complex, formula:\n\\[\n\\nu = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}}\n\\]\ndf &lt;- ( (var_voi/15 + var_vls/15)^2 ) / \n      ( ((var_voi/15)^2 / (15 - 1)) + ((var_vls/15)^2 / (15 - 1)) )\nThe degrees of freedom for our data are approximately 28. Figure 28.2 shows a t-distribution with those degrees of freedom and a dashed line where our t-statistic falls. The shaded purple area marks the area under the probability curve with t values equal or greater than the obtained t-statistic. The size of this area is the probability that we get a t value equal or greater than the obtained t-statistic. This probability is the p-value!\n# Create data\ndata &lt;- tibble(df = df) |&gt;\n  mutate(data = map(df, ~ {\n    tibble(\n      x = seq(-4, 4, length.out = 500),\n      y = if (is.infinite(.x)) dnorm(seq(-4, 4, length.out = 500))\n          else dt(seq(-4, 4, length.out = 500), df = .x)\n    )\n  })) |&gt;\n  unnest(data)\n\n# Plot\nggplot(data, aes(x = x, y = y)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = tstat, linetype = \"dashed\") +\n  geom_area(\n    data = subset(data, x &gt;= tstat),\n    aes(x = x, y = y),\n    fill = \"purple\", alpha = 0.3\n  ) +\n  labs(\n    title = glue::glue(\"Standard t-Distribution with df = {round(df)}\"),\n    x = \"t-statistic\", y = \"Density\",\n  )\n\n\n\n\n\n\n\nFigure 28.2: Standard t-distribution and obtained t-statistic.\nYou can get the p-value in R using the pt() function. You need the t-value and the degrees of freedom. These are saved in tstat and df from previous code. We also need to set another argument, lower.tail: this argument states whether we want the probability of getting a t value that is equal or less than the specified t value, but we want the probability of getting a t value that is equal or greater than the specified t value, so we set lower.tail to FALSE.\npvalue &lt;- pt(tstat, df, lower.tail = FALSE)\npvalue\n\n[1] 0.01075221\nIn other words, assuming \\(H_0\\) is true and there is not difference between the two groups of vowel duration, the probability of obtaining a t-statistic greater or equal to 2.4374424 is approximately 0.01. In other words, there is approximately a 1% probability that the difference between durations of vowels followed by voiced or voiceless consonants is 7.4399103 or larger. Of course, we want the p-value to be as small as possible: if \\(H_0\\) is true and the true difference is 0, finding a large difference should be very unlikely (think about the t-distribution: values away from 0 are less likely than 0 and values closer to 0).\nBut how do we decide how small is small enough? Is 1% small enough? What about 0.5%? 5%?, maybe 10%? This is the so-called \\(\\alpha\\)-level (read “alpha level”, from the Greek letter \\(\\alpha\\)). We will get back to the issue of setting an \\(\\alpha\\)-level in the next section, but for now now that in social research it has become standard to set it to 0.05. In other words, if the p-value is lower than \\(\\alpha = 0.05\\) then we take the p-value to be small enough, otherwise we don’t. When the p-value is smaller than 0.05, we say we found a statistically significant difference, when it is equal or greater than 0.05, we say we found a statistically non-significant difference. When we find a statistical significant difference, the frequentist story goes, we say that we reject the null hypothesis \\(H_0\\). In our simulated example of vowel duration, the p-value is smaller than 0.05, so we say the mean vowel durations before voiced vs voiceless consonants are (statistically) significantly different from each other.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html#the-p-value",
    "href": "ch-null-ritual.html#the-p-value",
    "title": "28  Frequentist statistics, the Null Ritual and p-values",
    "section": "",
    "text": "\\(H_0\\): the difference between means is 0.\n\\(H1\\): the difference between means is greater than 0.\n\n\n\n\n\n\n\n\n\\(t\\) is the t-statistic.\n\\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means of the first and second group (the order doesn’t really matter).\n\\(s^2_1\\) and \\(s^2_2\\) are the variance of the first and second group. The variance is simply the square of the standard deviation (expressed with \\(s\\) here).\n\\(n_1\\) and \\(n_2\\) are the number of observations for the first and second group (sample size).\n\n\n\n\n\nFirst, the part about the real difference being 0. This is \\(H_0\\) from above, our null hypothesis that the difference is 0. For a p-value to work, we must assume that \\(H_0\\) is true. Otherwise, the frequentist machinery does not work.\nAnother important aspect is the “difference this large or larger”: due to how probability density function works (i.e. functions to obtain probabilities of continuous variables), we cannot obtain the probability of a specific value, but only the probability of an interval of values. The frequentist story goes that, if \\(H_0\\) is true, you should not get very large differences, let alone larger differences than the one found.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-null-ritual.html#the-null-ritual",
    "href": "ch-null-ritual.html#the-null-ritual",
    "title": "28  Frequentist statistics, the Null Ritual and p-values",
    "section": "28.2 The “Null Ritual”",
    "text": "28.2 The “Null Ritual”\n\n\n\n\nCassidy, Scott A., Ralitza Dimova, Benjamin Giguère, Jeffrey R. Spence, and David J. Stanley. 2019. “Failing Grade: 89 Per-Cent of Introduction to Psychology Textbooks That Define/Explain Statistical Significance Do so Incorrectly.” Advances in Methods and Practices in Psychological Science. https://doi.org/10.1177/2515245919858072.\n\n\nCoretta, Stefano. 2019. “An Exploratory Study of Voicing-Related Differences in Vowel Duration as Compensatory Temporal Adjustment in Italian and Polish.” Glossa: A Journal of General Linguistics 4 (1): 1–25. https://doi.org/10.5334/gjgl.869.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The Journal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual. What You Always Wanted to Know about Significance Testing but Were Afraid to Ask.” In, 391408.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Frequentist statistics, the Null Ritual and *p*-values</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html",
    "href": "ch-regression-bernoulli.html",
    "title": "29  Binary outcomes",
    "section": "",
    "text": "29.1 Binary outcomes and the Bernoulli family\nBinary outcome variables are very common in linguistics. These are categorical variable that have two levels, e.g.:\nSo far you have been fitting regression models in which the outcome variable was numeric and continuous. However, a lot of studies use binary outcome variables and it thus important to learn how to deal with those. This is what this post is about.\nWhen modelling binary outcomes, what the researcher is usually interested in is the probability of obtaining one of the two levels. For example, in a lexical decision task one might want to know the probability that real words were recognised as such (in other words, we are interested in accuracy: incorrect or correct response).\nLet’s say there is an 80% probability of responding correctly. So (\\(p()\\) stands for “probability of”):\nYou see that if you know the probability of one level (correct) you automatically know the probability of the other level, since there are only two levels and the total probability has to sum to 1.\nThe distribution family for binary probabilities is the Bernoulli family. The Bernoulli family has only one parameter, \\(p\\), which is the probability of obtaining one of the two levels (one generally picks which level).\nWith our lexical decision task example, we can write:\n\\[\n\\begin{align}\n\\text{resp}_{correct} & \\sim Bernoulli(p) \\\\\np & = 0.8\n\\end{align}\n\\]\nYou can read it as:\nIf you randomly sampled from \\(Bernoulli(0.8)\\) you would get “correct” 80% of the times and “incorrect” 20% of the times.\nNow, what we are trying to do when modelling binary outcome variables is to estimate the probability \\(p\\) from the data. But there is a catch: probabilities are bounded between 0 and 1 and regression models don’t work with bounded variables out of the box!\nBounded probabilities are transformed into an unbounded numeric variable. The following section explains how.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#binary-outcomes-and-the-bernoulli-family",
    "href": "ch-regression-bernoulli.html#binary-outcomes-and-the-bernoulli-family",
    "title": "29  Binary outcomes",
    "section": "",
    "text": "yes / no\ngrammatical / ungrammatical\nSpanish / English\ndirect object (gave the girl the book) / prepositional phrase (gave the book to the girl)\ncorrect / incorrect\n\n\n\n\n\n\\(p(\\text{correct}) = 0.8\\)\n\\(p(\\text{incorrect}) = 1 - p(\\text{correct}) = 0.2\\)\n\n\n\n\n\n\n\nThe probability of getting a correct response follows a Bernoulli distribution with \\(p\\) = 0.8.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#probability-and-log-odds",
    "href": "ch-regression-bernoulli.html#probability-and-log-odds",
    "title": "29  Binary outcomes",
    "section": "29.2 Probability and log-odds",
    "text": "29.2 Probability and log-odds\nAs we have just learned probabilities are bounded between 0 and 1 but we need something that is not bounded because regression models don’t work with bounded numeric variables.\nThis is where the logit function comes in: the logit function (from “logistic unit”) is a mathematical function that transforms probabilities into log-odds.\nThe plot below shows the correspondence of probabilities (on the y-axis) and log-odds (on the x-axis), as marked by the black S-shaped line. Since probabilities can’t be smaller than 0 and greater than 1, the black line slopes in either direction and it approaches 0 and 1 on the y-axis without ever reaching them (in mathematical terms, it’s an asymptotic line). It is helpful to just memorise that probability 0.5 corresponds to log-odds 0.\n\n\n\n\n\n\n\n\n\nWhen you fit a regression model with a binary outcome and a Bernoulli family, the estimates are in log-odds. To transform log-odds back into probability, one uses the inverse logit (or logistic) function. The logit and inverse logit functions in R are applied with the qlogis() and plogis() functions respectively.\n\np &lt;- 0.3\n\n# from probability to log-odds\nLO &lt;- qlogis(p)\nLO\n\n[1] -0.8472979\n\n# from log-odds to probability\nplogis(LO)\n\n[1] 0.3\n\n\nThat is all you have to understand for now to be able to fit and interpret a Bernoulli regression!",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-bernoulli.html#fitting-a-bernoulli-model",
    "href": "ch-regression-bernoulli.html#fitting-a-bernoulli-model",
    "title": "29  Binary outcomes",
    "section": "29.3 Fitting a Bernoulli model",
    "text": "29.3 Fitting a Bernoulli model\n\n\n\n\n\n\nBernoulli, binomial and logistic regression\n\n\n\n\n\nA lot of researchers know Bernoulli models under the name “binomial” or “logistic” regression. Please, note that these are exactly equivalent: they refer to a model with a Bernoulli distribution for the outcome variable. It is just that different research traditions call them differently.\nSo if somebody asks you to run a logistic regression, or if you read a paper that reports one, what they just mean is to run a regression with a binary outcome variable and a Bernoulli distribution!\n\n\n\n\n29.3.1 The data\nTo illustrate how to fit a Bernoulli model, we will use data from Brentari 2024 on the emergent Nicaraguan Sign Language (Lengua de Señas Nicaragüense, NSL).\n\nverb_org &lt;- read_csv(\"data/brentari2024/verb_org.csv\")\n\nRows: 630 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, Object, Number, Agency, Num_Predicates\ndbl (1): Participant\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nverb_org contains information on predicates as signed by three groups (Group): home-signers (homesign), first generation NSL signers (NSL1) and second generation NSL signers (NSL2). Specifically, the data coded in Num_Predicates whether the predicates uttered by the signer were single-verb predicates (SVP, single) or a multi-verb predicates (MVP, multiple). The hypothesis of the study is that use of multi-verb predicates would increase with each generation, i.e. that NSL1 signers would use more MVPs than home-signers and that NSL2 signers would use more MVPs than home-signers and NSL1 signers. (For the linguistic reasons behind this hypothesis, check the paper linked above).\nLet’s plot the data to learn a bit more about it.\n\nverb_org |&gt; \n  ggplot(aes(Group, fill = Num_Predicates)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nWhat do you notice about the type of predicates in the three groups?\nTo assess the study hypothesis, we can fit a Bernoulli model with Num_Predicates as the outcome variable and Group as the predictor.\nBefore we move on onto fitting the model, it is useful to transform Num_Predicates into a factor and specify the order of the levels so that single is the first level and multiple is the second level.\nThis is useful because Bernoulli models estimate the probability (the parameter \\(p\\) in \\(Bernoulli(p)\\) of getting the second level in the outcome variable.\nYou can also think of this in terms of 0s and 1s: the first level is assigned to 0 and the second level is assigned to 1. Then a Bernoulli distribution with probability \\(p\\) tells you the probability of getting a 1. It doesn’t matter how you prefer to think about Bernoulli distributions, as long as you remember that the probability being estimated is the probability of the second level.\nNow let’s mutate verb_org.\n\nverb_org &lt;- verb_org |&gt; \n  mutate(\n    Num_Predicates = factor(Num_Predicates, levels = c(\"single\", \"multiple\"))\n  )\n\nIf you reproduce the plot above you will see now that the order of Num_Predicates in the legend is “single” then “multiple” and that the order of the proportions in the bar chart have flipped.\nNow we can move on onto modelling.\n\n\n29.3.2 The model\n\\[\n\\begin{align}\n\\text{Num\\_Preds}_{MVP} & \\sim Bernoulli(p_i) \\\\\nlogit(p_i) & = \\alpha_{\\text{Group}[i]} \\\\\n\\end{align}\n\\]\n\nThe probability of using an MVP follows a Bernoulli distribution with probability \\(p\\).\nThe log-odds of \\(p\\) are equal to \\(\\alpha\\) for each Group.\n\nIn other words, the model estimates \\(p\\) for each group. Here is the code. Remember that to use the indexing approach for categorical predictors (Group) we need to suppress the intercept with the 0 + syntax.\n\nmvp_bm &lt;- brm(\n  Num_Predicates ~ 0 + Group,\n  family = bernoulli,\n  data = verb_org,\n  cores = 4,\n  seed = 1329,\n  file = \"cache/ch-regression-bernoulli_mvp_bm\"\n)\n\nLet’s inspect the model summary (we will get 80% CrIs).\n\nsummary(mvp_bm, prob = 0.8)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Num_Predicates ~ 0 + Group \n   Data: verb_org (Number of observations: 630) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n              Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nGrouphomesign    -0.57      0.15    -0.76    -0.38 1.00     4424     3005\nGroupNSL1        -1.46      0.17    -1.68    -1.24 1.00     3797     2995\nGroupNSL2        -0.02      0.14    -0.21     0.16 1.00     4020     2851\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBased on the model, there is an 80% probability that the log-odds of a MVP are between -0.76 and -0.38 in home-signers, between -1.68 and -1.24 in NSL1 signers and between -0.21 and 0.16 in NSL2 signers.\nIt’s easier to understand the results if we convert the log-odds to probabilities. The quickest way to do this is to get the Regression Coefficients table from the summary with fixef() and mutate the Q columns with plogis().\n\nfixef(mvp_bm, prob = c(0.1, 0.9)) |&gt;\n  # we need to convert the output of fixef() to a tibble to use mutate()\n  as_tibble() |&gt;\n  # we plogis() the Q columns and round to the second digit\n  mutate(\n    Q10 = round(plogis(Q10), 2),\n    Q90 = round(plogis(Q90), 2)\n  )\n\n\n  \n\n\n\nBased on the model, there is an 80% probability that the probability of using an MVP is between 32-41% in home-signers, between 16-22% in NSL1 signers and between 45-54% in NSL2 signers.\nWe can now see more clearly that the hypothesis of the study is not fully borne out by the data: while NSL2 signers are more likely to use an MVP than home-signers and NSL1 signers, it is not the case that NSL1 signers are more likely to use MVPs than home-signers.\nTo conclude this introduction to Bernoulli models (aka binomial/logistic regressions) we can get the predicted probabilities of use of MVPs in the three groups with conditional_effects().\n\nconditional_effects(mvp_bm)",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Binary outcomes</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat-cat.html",
    "href": "ch-regression-cat-cat.html",
    "title": "30  Regression models: multiple predictors",
    "section": "",
    "text": "30.1 Two categorical predictors\nSo far, we fitted regressions with a single predictor, like the following Gaussian model of reaction times from Chapter 26:\n\\[\n\\begin{align}\nRT_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\beta_1 \\cdot W_{\\text{T}[i]} + \\beta_2 \\cdot W_{\\text{F}[i]}\\\\\n\\end{align}\n\\]\nThe categorical predictor \\(W\\) (IsWord in the data) has two levels (TRUE and FALSE), so there are two indexing variables: \\(W_{\\text{T}}\\) and \\(W_{\\text{F}}\\). Each indexing variable gets its coefficient: \\(\\beta_1\\) and \\(\\beta_2\\). In most context, however, you will want to investigate the effects of more than one predictor.\nRegression models can be fit with multiple predictors. Traditionally, regression models with a single predictor were called “simple regression” and models with more than one “multiple regression”, but it doesn’t make sense to have a specific name: they are all regression models. In this chapter, we will discuss regression models with two categorical predictors.\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\nf0_bm &lt;- brm(\n  f0mn ~ gender + attitude,\n  family = gaussian,\n  data = polite,\n  cores = 4,\n  seed = 7123,\n  file = \"cache/ch-regression-cat-cat_f0_bm\"\n)\nsummary(f0_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ gender + attitude \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     255.14      4.48   246.47   263.81 1.00     4323     3184\ngenderM      -116.07      5.28  -126.30  -105.67 1.00     4618     2936\nattitudepol   -14.71      5.34   -25.05    -4.40 1.00     4708     2884\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.03      1.93    35.48    42.98 1.00     4604     2932\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nconditional_effects(f0_bm, \"gender\")\nconditional_effects(f0_bm, \"attitude\")\nconditional_effects(f0_bm, \"gender:attitude\")",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression models: multiple predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-cat-cat.html#is-the-effect-of-attitude-the-same-in-both-genders",
    "href": "ch-regression-cat-cat.html#is-the-effect-of-attitude-the-same-in-both-genders",
    "title": "30  Regression models: multiple predictors",
    "section": "30.2 Is the effect of attitude the same in both genders?",
    "text": "30.2 Is the effect of attitude the same in both genders?\n\npolite |&gt; \n  ggplot(aes(gender, f0mn, colour = attitude)) +\n  geom_jitter(alpha = 0.7, position = position_jitterdodge(jitter.width = 0.1, seed = 2836))\n\n\n\n\n\n\n\nFigure 30.1",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression models: multiple predictors</span>"
    ]
  },
  {
    "objectID": "ch-regression-interaction.html",
    "href": "ch-regression-interaction.html",
    "title": "31  Regression models: interactions",
    "section": "",
    "text": "polite &lt;- read_csv(\"data/winter2012/polite.csv\")\n\n\nf0_bm_int &lt;- brm(\n  f0mn ~ gender + attitude + gender:attitude,\n  family = gaussian,\n  data = polite,\n  cores = 4,\n  seed = 7123,\n  file = \"cache/ch-regression-interaction_f0_bm_int\"\n)\n\n\nsummary(f0_bm_int)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ gender + attitude + gender:attitude \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             256.56      5.20   246.38   267.27 1.00     2632     2950\ngenderM              -119.38      7.66  -134.08  -104.50 1.00     2532     2665\nattitudepol           -17.48      7.28   -31.76    -3.18 1.00     2573     2806\ngenderM:attitudepol     6.65     10.67   -14.20    27.47 1.00     2191     2693\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.10      1.94    35.52    43.14 1.00     3834     2779\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(f0_bm_int, \"gender:attitude\")\n\n\n\n\n\n\n\n\n\nf0_bm_int_draws &lt;- as_draws_df(f0_bm_int)\n\n\nf0_bm_int_draws &lt;- f0_bm_int_draws |&gt; \n  mutate(\n    f_inf = b_Intercept,\n    f_pol = b_Intercept + b_attitudepol,\n    m_inf = b_Intercept + b_genderM,\n    m_pol = b_Intercept + b_genderM + b_attitudepol + `b_genderM:attitudepol`\n  )\n\n\nlibrary(posterior)\n\nThis is posterior version 1.6.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nf0_bm_int_draws |&gt; \n  mutate(\n    m_pol_inf = m_pol - m_inf\n  ) |&gt; \n  summarise(\n    mean_diff = mean(m_pol_inf), sd_diff = sd(m_pol_inf),\n    lo_diff = quantile2(m_pol_inf, probs = 0.025), hi_diff = quantile2(m_pol_inf, probs = 0.975)\n  ) |&gt;\n  round()",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Regression models: interactions</span>"
    ]
  },
  {
    "objectID": "ch-regression-interaction-index.html",
    "href": "ch-regression-interaction-index.html",
    "title": "32  Regression models: interactions using indexing",
    "section": "",
    "text": "polite &lt;- read_csv(\"data/winter2012/polite.csv\")\n\n\nf0_bm_int2 &lt;- brm(\n  f0mn ~ 0 + gender:attitude,\n  family = gaussian,\n  data = polite,\n  cores = 4,\n  seed = 7123,\n  file = \"cache/ch-regression-interaction-index_f0_bm_int2\"\n)\n\n\nsummary(f0_bm_int2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ 0 + gender:attitude \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngenderF:attitudeinf   256.61      5.16   246.73   266.76 1.00     5037     2982\ngenderM:attitudeinf   137.23      5.87   125.37   148.66 1.00     5491     2936\ngenderF:attitudepol   238.93      5.08   229.06   248.85 1.00     5191     2912\ngenderM:attitudepol   126.28      5.82   114.88   137.71 1.00     5455     3018\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.13      1.94    35.51    43.18 1.00     4975     3159\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(f0_bm_int2, \"gender:attitude\")\n\n\n\n\n\n\n\n\n\nf0_bm_int2_draws &lt;- as_draws_df(f0_bm_int2)\n\n\nf0_bm_int2_draws &lt;- f0_bm_int2_draws |&gt; \n  mutate(\n    f_pol_inf = `b_genderF:attitudepol` - `b_genderF:attitudeinf`,\n    m_pol_inf = `b_genderM:attitudepol` - `b_genderM:attitudeinf`,\n  )\n\n\nlibrary(posterior)\n\nThis is posterior version 1.6.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nf0_bm_int2_draws |&gt;\n  summarise(\n    mean_diff = mean(m_pol_inf), sd_diff = sd(m_pol_inf),\n    lo_diff = quantile2(m_pol_inf, probs = 0.025), hi_diff = quantile2(m_pol_inf, probs = 0.975)\n  ) |&gt;\n  round()",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Regression models: interactions using indexing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bezeau, Scott, and Roger Graves. 2001. “Statistical Power and\nEffect Sizes of Clinical Neuropsychology Research.” Journal\nof Clinical and Experimental Neuropsychology 23 (3): 399–406. https://doi.org/10.1076/jcen.23.3.399.1181.\n\n\nBochynska, Agata, Liam Keeble, Caitlin Halfacre, Joseph V. Casillas,\nIrys-Amélie Champagne, Kaidi Chen, Melanie Röthlisberger, Erin M.\nBuchanan, and Timo B. Roettger. 2023. “Reproducible Research\nPractices and Transparency Across Linguistics.” Glossa\nPsycholinguistics 2 (1). https://doi.org/10.5070/g6011239.\n\n\nBrugger, Peter. 2001. “From Haunted Brain to Haunted Science: A\nCognitive Neuroscience View of Paranormal and Pseudoscientific\nThought.” Hauntings and Poltergeists: Multidisciplinary\nPerspectives, 195213.\n\n\nCameron-Faulkner, Thea, Nivedita Malik, Circle Steele, Stefano Coretta,\nLudovica Serratrice, and Elena Lieven. 2020. “A Cross-Cultural\nAnalysis of Early Prelinguistic Gesture Development and Its Relationship\nto Language Development.” Child Development 92 (1):\n273290. https://doi.org/10.1111/cdev.13406.\n\n\nCassidy, Scott A., Ralitza Dimova, Benjamin Giguère, Jeffrey R. Spence,\nand David J. Stanley. 2019. “Failing Grade: 89 Per-Cent of\nIntroduction to Psychology Textbooks That Define/Explain Statistical\nSignificance Do so Incorrectly.” Advances in Methods and\nPractices in Psychological Science. https://doi.org/10.1177/2515245919858072.\n\n\nCharles, Sarah J., James E. Bartlett, Kyle J. Messick, Thomas J.\nColeman, and Alex Uzdavines. 2019. “Researcher Degrees of Freedom\nin the Psychology of Religion.” The International Journal for\nthe Psychology of Religion 29 (4): 230245.\n\n\nCohen, Jacob. 1962. “The Statistical Power of Abnormal-Social\nPsychological Research: A Review.” The Journal of Abnormal\nand Social Psychology 65 (3): 145–53. https://doi.org/10.1037/h0045186.\n\n\nCoretta, Stefano. 2019. “An Exploratory Study of Voicing-Related\nDifferences in Vowel Duration as Compensatory Temporal Adjustment in\nItalian and Polish.” Glossa: A Journal of General\nLinguistics 4 (1): 1–25. https://doi.org/10.5334/gjgl.869.\n\n\n———. 2020. “Open Science in Phonetics and Phonology.” https://doi.org/10.31219/osf.io/4dz5t.\n\n\n———. n.d. “Vowel Duration, Voicing Duration, and Vowel Height:\nAcoustic and Articulatory Data from Italian [Research\nCompendium].” https://doi.org/10.17605/OSF.IO/XDGFZ.\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke,\nByron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, et al. 2023.\n“Multidimensional Signals and Analytic Flexibility: Estimating\nDegrees of Freedom in Human-Speech Analyses.” Advances in\nMethods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nCoretta, Stefano, Josiane Riverin-Coutlée, Enkeleida Kapia, and Stephen\nNichols. 2022. “Northern Tosk Albanian.” Journal of the\nInternational Phonetic Association, 123. https://doi.org/10.1017/s0025100322000044.\n\n\nCumming, Geoff. 2013. “The New Statistics: Why and How.”\nPsychological Science 25 (1): 729. https://doi.org/10.1177/0956797613504966.\n\n\nDarwin Holmes, Andrew Gary. 2020. “Researcher Positionality: A\nConsideration of Its Influence and Place in Qualitative\nResearcha New Researcher Guide.” Shanlax\nInternational Journal of Education 8 (4): 110. https://doi.org/10.34293/education.v8i4.3232.\n\n\nDevezer, Berna, Danielle J. Navarro, Joachim Vandekerckhove, and Erkan\nOzge Buzbas. 2021. “The Case for Formal Methodology in Scientific\nReform.” Royal Society Open Science 8 (3): rsos.200805,\n200805. https://doi.org/10.1098/rsos.200805.\n\n\nDienes, Zoltan. 2008. Understanding Psychology as a Science: An\nIntroduction to Scientific and Statistical Inference. Macmillan\nInternational Higher Education.\n\n\nDryer, Matthew S. 2008. “Descriptive Theories, Explanatory\nTheories, and Basic Linguistic Theory.” In Catching Language:\nThe Standing Challenge of Grammar Writing, edited by Felix K.\nAmeka, Alan Charles Dench, and Nicholas Evans. Vol. 167. Trends in\nLinguistics Studies and Monographs. Mouton De Gruyter.\n\n\nEgurtzegi, Ander, and Christopher Carignan. 2020. “An Acoustic\nDescription of Mixean Basque.” The Journal of the Acoustical\nSociety of America 147 (4): 27912802. https://doi.org/10.1121/10.0000996.\n\n\nEllis, J. Timothy, and Yair Levy. 2008. “Framework of\nProblem-Based Research: A Guide for Novice Researchers on the\nDevelopment of a Research-Worthy Problem.” Informing Science:\nThe International Journal of an Emerging Transdiscipline 11: 1733.\nhttps://doi.org/10.28945/438.\n\n\nFanelli, Daniele. 2010. “Do Pressures to Publish Increase\nScientists’ Bias? An Empirical Support from US States Data.”\nEdited by Enrico Scalas. PLoS ONE 5 (4): e10271. https://doi.org/10.1371/journal.pone.0010271.\n\n\n———. 2012. “Negative Results Are Disappearing from Most\nDisciplines and Countries.” Scientometrics 90 (3):\n891–904. https://doi.org/10.1007/s11192-011-0494-7.\n\n\nFischhoff, Baruch. 1975. “Hindsight Is Not Equal to Foresight: The\nEffect of Outcome Knowledge on Judgment Under Uncertainty.”\nJournal of Experimental Psychology: Human Perception and\nPerformance 1 (3): 288.\n\n\nFlake, Jessica Kay, and Eiko I. Fried. 2020. “Measurement\nSchmeasurement: Questionable Measurement Practices and How to Avoid\nThem.” Advances in Methods and Practices in Psychological\nScience 3 (4): 456465. https://doi.org/10.1177/2515245920952393.\n\n\nGaeta, Laura, and Christopher R. Brydges. 2020. “An Examination of\nEffect Sizes and Statistical Power in Speech, Language, and Hearing\nResearch.” Journal of Speech, Language, and Hearing\nResearch 63 (5): 15721580. https://doi.org/10.1044/2020_jslhr-19-00299.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in\nHereditary Stature.” The Journal of the Anthropological\nInstitute of Great Britain and Ireland 15: 246. https://doi.org/10.2307/2841583.\n\n\n———. 1980. “Kinship and Correlation.” The North\nAmerican Review 150 (401): 419431.\n\n\nGelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and\nObjective in Statistics.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 180 (4): 9671033. https://doi.org/10.1111/rssa.12276.\n\n\nGelman, Andrew, Daniel Lakeland, Brian Haig, Christian Hennig, Art Owen,\nRobert Cousins, Stan Young, et al. 2019. “Many Perspectives on\nDeborah Mayo’s “Statistical Inference as Severe Testing:\nHow to Get Beyond the Statistics Wars”.” https://doi.org/10.48550/arXiv.1905.08876.\n\n\nGelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in\nScience: Data-Dependent Analysis. A “Garden of Forking\nPaths”explains Why Many Statistically\nSignificant Comparisons Don’t Hold Up.” American\nScientist 102 (6): 460466.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between\n“Significant” and “Not\nSignificant” Is Not Itself Statistically\nSignificant.” The American Statistician 60 (4): 328331.\nhttps://doi.org/10.1198/000313006X152649.\n\n\nGigerenzer, Gerd. 2004. “Mindless Statistics.” The\nJournal of Socio-Economics 33 (5): 587606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n\n———. 2018. “Statistical Rituals: The Replication Delusion and How\nWe Got There.” Advances in Methods and Practices in\nPsychological Science 1 (2): 198218. https://doi.org/10.1177/2515245918771329.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The\nNull Ritual. What You Always Wanted to Know about\nSignificance Testing but Were Afraid to Ask.” In,\n391408.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings\nAre False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1080/09332480.2019.1579573.\n\n\nJafar, Anisa J. N. 2018. “What Is Positionality and Should It Be\nExpressed in Quantitative Studies?” Emergency Medicine\nJournal. https://doi.org/10.1136/emermed-2017-207158.\n\n\nJohn, Leslie K., George Loewenstein, and Drazen Prelec. 2012.\n“Measuring the Prevalence of Questionable Research Practices with\nIncentives for Truth Telling.” Psychological Science 23\n(5): 524532. https://doi.org/10.1177/0956797611430953.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results\nAre Known.” Personality and Social Psychology Review 2\n(3): 196217. https://doi.org/10.1207/s15327957pspr0203_4.\n\n\nKobrock, Kristina, and Timo B. Roettger. 2023. “Assessing the\nReplication Landscape in Experimental Linguistics.” Glossa\nPsycholinguistics 2 (1). https://doi.org/10.5070/g6011135.\n\n\nKoole, Sander L, and Daniël Lakens. 2012. “Rewarding Replications:\nA Sure and Simple Way to Improve Psychological Science.”\nPerspectives on Psychological Science 7 (6): 608614.\n\n\nKruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New\nStatistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power\nAnalysis from a Bayesian Perspective.” Psychonomic Bulletin\n& Review 25 (1): 178206. https://doi.org/10.3758/s13423-016-1221-4.\n\n\nMakel, Matthew C., Jonathan A. Plucker, and Boyd Hegarty. 2012.\n“Replications in Psychology Research: How Often Do They Really\nOccur?” Perspectives on Psychological Science 7 (6):\n537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMayo, Deborah G. 2018. Statistical Inference as Severe Testing: How\nto Get Beyond the Statistics Wars. 1st ed. Cambridge University\nPress. https://doi.org/10.1017/9781107286184.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. Second edition. Chapman & Hall/CRC\nTexts in Statistical Science Series. Boca Raton: CRC Press.\n\n\nMorin, Olivier. 2015. “A Plea for\n“Shmeasurement” in the Social\nSciences.” Biological Theory 10 (3): 237245. https://doi.org/10.1007/s13752-015-0217-z.\n\n\nNicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025.\nIntroduction to Bayesian Data Analysis for Cognitive Science.\nhttps://bruno.nicenboim.me/bayescogsci/.\n\n\nNickerson, Raymond S. 1998. “Confirmation Bias: A Ubiquitous\nPhenomenon in Many Guises.” Review of General Psychology\n2 (2): 175220. https://doi.org/10.1037/1089-2680.2.2.175.\n\n\nNissen, Silas Boye, Tali Magidson, Kevin Gross, and Carl T. Bergstrom.\n2016. “Publication Bias and the Canonization of False\nFacts.” Elife 5: e21451. https://doi.org/10.7554/eLife.21451.\n\n\nNosek, Brian A, and Daniël Lakens. 2014. “A Method to Increase the\nCredibility of Published Results.” Social Psychology 45\n(3): 137141.\n\n\nOkasha, Samir. 2016. Philosophy of Science: Very Short\nIntroduction. Oxford: Oxford University Press. https://doi.org/10.1093/actrade/9780192802835.001.0001.\n\n\nRoettger, Timo B. 2019. “Researcher Degrees of Freedom in Phonetic\nSciences.” Laboratory Phonology: Journal of the Association\nfor Laboratory Phonology 10 (1): 127.\n\n\nRoettger, Timo B., Bodo Winter, and Harald Baayen. 2019. “Emergent\nData Analysis in Phonetic Sciences: Towards Pluralism and\nReproducibility.” Journal of Phonetics 73: 17. https://doi.org/10.1016/j.wocn.2018.12.001.\n\n\nRosenberg, Alexander, and Lee Mclntyre. 2020. Philosophy of science:\na contemporary introduction. Fourth edition. Routledge contemporary\nintroductions to philosophy. New York London: Routledge.\n\n\nScheel, Anne M. 2022. “Why Most Psychological Research Findings\nAre Not Even Wrong.” Infant and Child Development 31\n(1): e2295. https://doi.org/10.1002/icd.2295.\n\n\nScheel, Anne M., Leonid Tiokhin, Peder M. Isager, and Daniël Lakens.\n2020. “Why Hypothesis Testers Should Spend Less Time Testing\nHypotheses.” Perspectives on Psychological Science 16\n(4): 744–55. https://doi.org/10.1177/1745691620966795.\n\n\nSedlmeier, Peter, and Gerd Gigerenzer. 1992. “Do Studies of\nStatistical Power Have an Effect on the Power of Studies?” In,\n389–406. Washington: American Psychological Association. https://doi.org/10.1037/10109-032.\n\n\nSilberzahn, Raphael, Eric L. Uhlmann, Daniel P. Martin, Pasquale\nAnselmi, Frederik Aust, Eli Awtrey, Štěpán Bahník, Feng Bai, Colin\nBannard, and Evelina Bonnier. 2018. “Many Analysts, One Data Set:\nMaking Transparent How Variations in Analytic Choices Affect\nResults.” Advances in Methods and Practices in Psychological\nScience 1 (3): 337356. https://doi.org/10.1177/2515245917747646.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011.\n“False-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as\nSignificant.” Psychological Science 22 (11): 13591366.\n\n\nSlow Science Academy. 2010. “The Slow Science Manifesto.”\nhttp://slow-science.org.\n\n\nSong, Yoonsang, Youngah Do, Arthur L. Thompson, Eileen R. Waegemaekers,\nand Jongbong Lee. 2020. “Second Language Users Exhibit Shallow\nMorphological Processing.” Studies in Second Language\nAcquisition 42 (5): 11211136. https://doi.org/10.1017/s0272263120000170.\n\n\nSterling, Theodore D. 1959. “Publication Decisions and Their\nPossible Effects on Inferences Drawn from Tests of\nSignificanceor Vice Versa.” Journal of the\nAmerican Statistical Association 54 (285): 3034.\n\n\nTucker, Benjamin V, Daniel Brenner, Kyle Danielson D, Matthew C Kelley,\nFilip Nenadić, and Michelle Sims. 2019. “The Massive Auditory\nLexical Decision (MALD) Database.” Behavior Research\nMethods 51 (3): 11871204. https://doi.org/10.3758/s13428-018-1056-1.\n\n\nTukey, John W. 1969. “Analyzing Data: Sanctification or Detective\nWork?” American Psychologist 24 (2): 83–91. https://doi.org/10.1037/h0027108.\n\n\n———. 1980. “We Need Both Exploratory and Confirmatory.”\nThe American Statistician 34 (1): 23–25. https://doi.org/10.2307/2682991.\n\n\nTversky, Amos, and Daniel Kahneman. 1974. “Judgment Under\nUncertainty: Heuristics and Biases: Biases in Judgments Reveal Some\nHeuristics of Thinking Under Uncertainty.” Science 185\n(4157): 11241131. https://doi.org/10.1126/science.185.4157.1124.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. “How to Embrace\nVariation and Accept Uncertainty in Linguistic and Psycholinguistic Data\nAnalysis.” Linguistics 59 (5): 13111342. https://doi.org/10.1515/ling-2019-0051.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der\nMaas, and Rogier A. Kievit. 2012. “An Agenda for Purely\nConfirmatory Research.” Perspectives on Psychological\nScience 7 (6): 632638. https://doi.org/10.1177/1745691612463078.\n\n\nWicherts, Jelte M., Denny Borsboom, Judith Kats, and Dylan Molenaar.\n2006. “The Poor Availability of Psychological Research Data for\nReanalysis.” American Psychologist 61 (7): 726.\n\n\nWicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn,\nMarjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen.\n2016. “Degrees of Freedom in Planning, Running, Analyzing, and\nReporting Psychological Studies: A Checklist to Avoid p-Hacking.”\nFrontiers in Psychology 7. https://doi.org/10.3389/fpsyg.2016.01832.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science (2e). Second edition. https://r4ds.hadley.nz.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using\nr. Routledge.\n\n\nWinter, Bodo, and Sven Grawunder. 2012. “The Phonetic Profile of\nKorean Formal and Informal Speech Registers.” Journal of\nPhonetics 40 (6): 808–15. https://doi.org/10.1016/j.wocn.2012.08.006.\n\n\nYarkoni, Tal. 2022. “The Generalizability Crisis.”\nBehavioral and Brain Sciences 45. https://doi.org/10.1017/s0140525x20001685.",
    "crumbs": [
      "References"
    ]
  }
]