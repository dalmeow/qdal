# Gaussian models {#sec-gauss-model}

![](https://img.shields.io/badge/Area-Statistics-red) ![](https://img.shields.io/badge/Area-R-green)

## A Gaussian model of reaction times

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(webexercises)
library(glue)
```

In the context of a quantitative research study, one of the objectives is to figure out the probability distribution of the variable of interest: Voice Onset Time, number of telic verbs, informativity score, acceptability ratings, reaction times, and so on. Let's imagine we are interested in understanding more about the nature of reaction times in auditory lexical decision tasks (lexical decision tasks in which the target is presented aurally rather than in writing). We can revisit the RT data from above to try and address the following research question:

> RQ: In a typical auditory lexical decision task, what are the mean and standard deviation of reaction times (RTs)?

Now, you might wonder why the mean and the standard deviation? This is because we are assuming that reaction times (i.e the population of reaction times, rather than our specific sample) are distributed according to a Gaussian probability distribution. It is usually the onus of the researcher to assume a probability distribution family. You will learn some heuristics for picking a distribution family later, but for now the Gaussian family will be a safe assumption to make. In statistical notation, we can write: $\text{RT} \sim Gaussian(\mu, \sigma)$

which you can read as: "reaction times are distributed according to a Gaussian distribution with mean $\mu$ and standard deviation $\sigma$". So the research question above is about finding the values of $\mu$ and $\sigma$.

For illustration's sake, let's assume the sample mean and standard deviation are also the population $\mu$ and $\sigma$. @fig-mald-rt-distr shows the empirical sample probability distribution (in grey) and the theoretical sample probability distribution (in purple) based on the sample mean and SD: in other words, the purple curve is the density curve of the probability distribution $Gaussian(1010, 318)$, We have seen earlier that the sample mean and SD of the RTs from the `mald` data are biased, due to uncertainty and variability. What we are really after is the values of $\mu$ and $\sigma$ which are the mean and standard deviation of the Gaussian distribution of the *population* of RTs in auditory lexical decision tasks. In other words, we want to make inference from the sample to the population of RTs.

```{r}
#| label: fig-mald-rt-distr
#| code-fold: true

mald <- readRDS("data/tucker2019/mald_1_1.rds")

rt_mean <- mean(mald$RT)
rt_sd <- sd(mald$RT)
rt_mean_text <- glue("mean: {round(rt_mean)} ms")
rt_sd_text <- glue("SD: {round(rt_sd)} ms")
x_int <- 2000

ggplot(data = tibble(x = 0:300), aes(x)) +
  geom_density(data = mald, aes(RT), colour = "grey", fill = "grey", alpha = 0.2) +
  stat_function(fun = dnorm, n = 101, args = list(rt_mean, rt_sd), colour = "#9970ab", linewidth = 1.5) +
  scale_x_continuous(n.breaks = 5) +
  geom_vline(xintercept = rt_mean, colour = "#1b7837", linewidth = 1) +
  geom_rug(data = mald, aes(RT), alpha = 0.1) +
  annotate(
    "label", x = rt_mean + 1, y = 0.0015,
    label = rt_mean_text,
    fill = "#1b7837", colour = "white"
  ) +
  annotate(
    "label", x = x_int, y = 0.0015,
    label = rt_sd_text,
    fill = "#8c510a", colour = "white"
  ) +
  annotate(
    "label", x = x_int, y = 0.001,
    label = "theoretical sample\ndistribution",
    fill = "#9970ab", colour = "white"
  ) +
  annotate(
    "label", x = x_int, y = 0.0003,
    label = "empirical sample\ndistribution",
    fill = "grey", colour = "white"
  ) +
  labs(
    title = "Theoretical sample distribution of reaction times",
    subtitle = glue("Gaussian distribution: mean = {round(rt_mean)} ms, SD = {round(rt_sd)}"),
    x = "RT (ms)", y = "Relative probability (density)"
  )
```

A statistical tool we can use to obtain an estimate of $\mu$ and $\sigma$ is a Gaussian model. A Gaussian model is a statistical model that estimates the values of the parameters of a Gaussian distribution, i.e. $\mu$ and $\sigma$. Since the values of the parameters are uncertain, we can estimate their probability distribution from the data, rather than just their values. This is what a Bayesian Gaussian model does. You can fit a Gaussian model to data using the [brms](https://paulbuerkner.com/brms/) package (the name is an initialism of Bayesian Regression Models using Stan; Gaussian models are a special type of regression models, which will be introduced in @sec-reg-intro).

The brms package can run a variety of Bayesian (regression) models. It is a very flexible package that allows you to model a lot of different types of variables. You don't really need to understand all of the technical details to be able to effectively use the package and interpret the results, so this textbook will focus on how to use the package in the context of research. We will cover some of the technicalities, but if you are are particularly interested in the inner workings of the package, feel free to do so (you can find materials on specific aspects by searching online). One useful thing to know is that brms is a bridge between R and the statistical programming software [Stan](https://mc-stan.org). Stan is a powerful piece of software that can run any type of Bayesian models. What brms does is that it allows you to write Bayesian models in R, which are translated into Stan models and run with Stan under the hood. You can safely use brms without learning Stan, but if you are interested in the computational aspects of Stan, you can check the [Stan documentation](https://mc-stan.org).

The first thing to do is of course to attach the brms package (see code below). You can then run a regression model with the `brm()` function, short for Bayesian Regression Model (a Gaussian model is a special type of regression model). The mandatory arguments of `brm()` are a model formula, a distribution family (of the outcome variable), and the data you want to run the model with. Running a model with data is also formally known as *fitting the model to the data*. Remember the model mathematical formula from above: $\text{RT} \sim Gaussian(\mu, \sigma)$. It would be nice if `brms()` allowed you to write the formula like that.

``` r
# This would be nice, but it won't work!
brm(
  RT ~ Gaussian(mu, sigma),
  data = mald
)
```

Alas, due to historical and technical reasons of how other R packages write model formulae, you need to use a special way of specifying the model. As mentioned, you need three arguments: a model formula, the distribution family of the outcome, and the data. So the mathematical formula is split in two parts (corresponding to two arguments of the `brm()` function): `formula` and `family`.

``` r
brm(
  formula = RT ~ 1,
  family = gaussian,
  data = mald
)
```

-   `RT ~ 1` and `family = gaussian` simply tell brms to model RT using a Gaussian distribution. This means that the probability distribution of the mean and the standard deviation of the Gaussian distribution of RTs will be estimated from the data. `RT ~ 1` might look very weird to you right now, but it will become clear in the next couple of chapters why it is that way. For now, just accept that that is the way you write a Gaussian model in R.
-   We specify the data with `data = height`.

As with other R functions, we want to save the output of `brm()` to a variable, here `rt_bm`. We will then be able to inspect the output in `rt_bm`. Now run the Gaussian model of RTs (don't forget to attach the brms package, like in the following code).

``` r
library(brms)

rt_bm <- brm(
  RT ~ 1,
  family = gaussian,
  data = mald
)
```

When you run the code, text will be printed in the R Console or below the code if you are using a Quarto document (you should!). This is what it looks like.

``` r
Compiling Stan program...
Start sampling

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000156 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.56 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)

<more text omitted>
```

The messages in the text are related to Stan and the statistical algorithm used by Stan to estimate the parameters of the model (in this model, these are the mean and the standard deviation of height). `Compiling Stan program...` tells you that brms has instructed Stan to compile the model specified in R and that Stan is now compiling the model to be run on the data (don't worry if this does not make sense). `Start sampling` tells us that the statistical algorithm used for estimation has started. This algorithm is the Markov Chain Monte Carlo algorithm, or MCMC for short. The algorithm is run by default four times; in technical terms, *four MCMC chains* are run. Hence why you will see information on Chain 1, 2, 3, and 4. If you want to learn more about the MCMC algorithm, check [Finding posteriors through sampling](https://elizabethpankratz.github.io/bayes_stat/day1/mcmc.html) by Elizabeth Pankratz and the resources linked there.

```{r}
#| label: rt-bm-run
#| include: false

library(brms)

rt_bm <- brm(
  RT ~ 1,
  family = gaussian,
  data = mald,
  seed = 6725,
  file = "cache/ch-gauss-model-rt_bm"
)
```

Now that the model has finished running and that the output has been saved in `rt_bm`, we can inspect it with the `summary()` function (note that this is difference from `summarise()`!).

```{r}
#| label: rt-bm-summ

summary(rt_bm)
```

Let's break down the summary bit by bit:

-   The first few lines are a reminder of the model we fitted:

    -   `Family` is the chosen distribution for the outcome variable (here `RT`), here a Gaussian distribution.

    -   `Links` lists the link functions. You don't need to worry about these now.

    -   `Formula` is the model formula.

    -   `Data` reports the name of the data and the number of observations.

    -   Finally, `Draws` has information about the MCMC draws.

-   Then, the `Regression Coefficients` are listed as a table. They are called this way because brms fits Bayesian *regression* models and a Gaussian model is a special type of regression model (you will learn about regression models in the following chapters). In the Gaussian model we fitted now, we only have one regression coefficient, `Intercept,` which corresponds to $\mu$ from the formula above (for the reason why it is called that way, you will have to wait until regression models are introduced in the following chapter, sorry for the many promissory explanation). You will learn how to interpret the `Regression Coefficients` table below.

-   Then `Further distributional parameters` are tabled. Here we only have `sigma` which is $\sigma$ from the formula above. The table has the same columns as the `Regression Coefficients` table.

## Posterior probability distributions

The `Regression Coefficients` table is the most important part of the model summary, the part that gives you information on the estimates of the model parameters.

The main characteristics of Bayesian regressions is that they don't just provide you with a single numeric estimate for the model parameters. Rather, the model estimates a *full probability distribution* for each parameter/coefficient. These probability distributions are called **posterior probability distributions** (or posteriors for short).

They are called *posterior* because they are derived from the data (and the prior probability distributions, you will learn more about these later).

The `Regression Coefficients` table reports a few summary measures of these posterior distributions. Here, we only have the summary measures of one posterior: the posterior of the model's mean $\mu$. The table also has three diagnostic measures, which you can ignore for now.

Here's a break down of the table's columns:

-   `Estimate`, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient).

-   `Est.error`, the error of the mean estimate (i.e. the standard deviation of the posterior distribution of the coefficient).

-   `l-95% CI` and `u-95% CI`, the lower and upper limits of the 95% Bayesian Credible Interval (more on these below).

-   `Rhat`, `Bulk_ESS`, `Tail_ESS` are diagnostics of the MCMC chains.

## Plotting the posterior distributions

While the model summary reports *summaries* of the posterior distributions, it is always helpful to *plot* the posteriors.

```{r}
#| label: rt-bm-plot
plot(rt_bm, combo = c("dens", "trace"))
```

The plot above shows the posterior distribution of the estimated coefficient `b_Intercept` and `sigma`, which correspond to the $\mu$ and $\sigma$ of the formula above, respectively.

For the `b_Intercept` coefficient, the posterior probability encompasses values between 163 and 167 cm, approximately. But some values are more probable then others: the values in the centre of the distribution have a higher probability density then the values on the sides. In other words, values around 165 cm are more probable than values below 164 and above 166, for example.

However, looking at a full probability distribution like that is not super straightforward. Credible Intervals (CrIs) help summarise the posterior distributions so that interpretation is more straightforward.

## Interpreting Credible Intervals

The model summary reports the Bayesian Credible Intervals (CrIs) of the posterior distributions.

Another way of obtaining summaries of the coefficients is to use the `posterior_summary()` function. Ignore the last three lines.

```{r}
#| label: rt-bm-post-summary
posterior_summary(rt_bm)
```

The 95% CrI of the `b_Intercept` is between `{r} round(brms::fixef(rt_bm)[3])` and `{r} round(brms::fixef(rt_bm)[4])`. This means there is a 95% probability, or that we can be 95% confident, that the `Intercept` value is within that range.

For `sigma`, the 95% CrI is between `{r} round(brms::posterior_summary(rt_bm)["sigma", "Q2.5"], 1)` and `{r} round(brms::posterior_summary(rt_bm)["sigma", "Q97.5"], 1)`. So, again, there is a 95% probability that the `sigma` value is between those values.

So, to summarise, **a 95% CrI tells us that we can be 95% confident, or in other words that there is a 95% probability, that the value of the coefficient is between the values of the CrI.**

Here you have the results from the regression model. Really, the results of the model are the full posterior probabilitis, but it makes things easier to focus on the CrI.

The model has "recovered" the values we used to simulate the height data quite well: above, we used a mean of 165 and a standard deviation of 8 to simulate data. The model is suggesting that the mean and SD of `h` is between 164-166 cm and 7-9 cm respectively: quite neat!

Regression models work.

## Reporting

What about reporting the model in writing? We could report the model and the results like this.[^ch-gauss-model-1]

[^ch-gauss-model-1]: To know how to add a citation for any R package, simply run `citation("package")` in the R Console, where `"package"` is the package name between double quotes.

::: callout-tip
We fitted a Bayesian regression using the brms package (Bürkner 2017) in R (R Core Team 2024). The outcome variable was height and we did not include any predictor, to estimate the overall mean and standard deviation of height.

Based on the model results, there is a 95% probability that the mean is between `{r} round(brms::fixef(rt_bm)[3])` and `{r} round(brms::fixef(rt_bm)[4])` cm and that the standard deviation is between `{r} round(brms::posterior_summary(rt_bm)["sigma", "Q2.5"], 1)` and `{r} round(brms::posterior_summary(rt_bm)["sigma", "Q97.5"], 1)` cm.
:::
