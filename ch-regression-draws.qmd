# Binary outcomes {#sec-regression-draws}

![](https://img.shields.io/badge/Area-Statistics-red) ![](https://img.shields.io/badge/Area-R-green)

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_light())
library(brms)
```

## MCMC what?

Bayesian regression models fitted with brms/Stan use the Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate the probability distributions of the model's coefficient. You have first encountered MCMC in @ch-fit-model. The MCMC algorithm does two things: first it finds a probability area that is the most compatible with the prior probabilities and the probability of the data given the priors; then, it sample values (called *draws*) from this high-probability area to reconstruct the posterior distribution. Elizabeth Pankras has written a nice short introduction to MCMC algorithms here: [Finding posteriors through sampling](https://elizabethpankratz.github.io/bayes_stat/day1/mcmc.html). I strongly recommend that you read that before proceeding (if you want to further your understanding of MCMC, the resources linked on that page are an excellent starting point). Note that to be a proficient user of brms and Bayesian regression models, you don't need to fully understand the mathematics behind MCMC algorithms, as long as you understand them conceptually.

When you run a model with brms, the draws (i.e. the sampled values) are stored in the model object. All operations on a model, like obtaining the `summary()`, are actually operations on those draws. We normally fit regression models with four MCMC chains. The sampling algorithm within each chain runs for 2000 iterations by default. The first half (1000 iterations) are used to "warm up" the algorithm (i.e. find the high-probability area) while the second half (1000 iterations) are the ones that sample from the high-probability area to reconstruct the posterior distribution. For four chains ran with 2000 iterations of which 1000 for warm up, we end up with 4000 iterations we can use to learn details about the posterior.

The rest of the post will teach you how to extract and manipulate the model's draws.

## Extract MCMC posterior draws

Let's read the `winter2012/polite.csv` data again first.

```{r}
#| label: polite

library(tidyverse)

polite <- read_csv("data/winter2012/polite.csv")

polite
```

Now we can reload the Bayesian regression model from [Introduction to regression models (Part II)](intro-regression-categorical.qmd). We simply use the same code: but now, instead of the model being fit again, the code will just read the file `cache/hnr_bm.rds`.

```{r}
#| label: hnr-bm

library(brms)

hnr_bm <- brm(
  HNRmn ~ attitude,
  family = gaussian(),
  data = polite,
  cores = 4,
  file = "cache/hnr_bm",
  seed = 2913
)
```

It's always good to inspect the summary to make sure the model is the correct one.

```{r}
#| label: hnr-bm-summ

summary(hnr_bm, prob = 0.8)
```

The `Regression Coefficients` table includes `Intercept` and `attitudepoliteite` which is what we expect from the model formula and data. We are good to go!

Now we can extract the MCMC draws from the model using the `as_draws_df()` function.

This function returns a tibble with values obtained at each draw of the MCMC draws. Since we fitted the model with 4 chains and 1000 sampling draws per chain, there is a total of 4000 drawn values (i.e. 4000 rows).

```{r}
#| label: hnr-bm-draws

hnr_bm_draws <- as_draws_df(hnr_bm)
hnr_bm_draws
```

Don't worry about the `lprior` and `lp__` columns. The columns `.chain`, `.iteration` and `.draw` indicate:

-   The chain number (1 to 4).
-   The iteration number within chain (1 to 1000).
-   The draw number across all chains (1 to 4000).

The following columns contain the drawn values at each draw for three parameters of the model: `b_Intercept`, `b_attitude` and `sigma`. To remind yourself what these mean, let's have a look at the mathematical formula of the model.

$$
\begin{align}
\text{HNRmn} & \sim Gaussian(\mu, \sigma) \\
\mu        & = \beta_0 + \beta_1 \cdot \text{is\_polite} \\
\end{align}
$$

So:

-   `b_Intercept` is $\beta_0$. This is the mean HNR when the attitude is informal.
-   `b_attitudepolite` is $\beta_1$. This is the *difference* in HNR between polite and informal attitude.
-   `sigma` is $\sigma$. This is the overall standard deviation.

## Summaries and plotting of posterior draws

The `Regression Coefficients` table reports summaries of the distribution of the drawn values of `b_Intercept` and `b_attitudepolite`. These summary measures are the mean (`Estimate`), the standard deviation (`Est.error`) and the lower and upper limits of the credible interval.

We can of course obtain those same measures ourselves from the draws tibble!

```{r}
#| label: hnr-bm-draws-summ

library(posterior)

hnr_bm_draws |> 
  select(-sigma) |> 
  pivot_longer(b_Intercept:b_attitudepol) |> 
  group_by(name) |> 
  summarise(
    mean = mean(value), sd = sd(value),
    lo_80 = quantile2(value, 0.1), up_80 = quantile2(value, 0.9)
  ) |> 
  # fancy way of mutating multiple columns
  mutate(
    across(mean:up_80, ~round(., 2))
  )
```

Hopefully now it is clear where the values in the `Regression Coefficient` table come from and how these are related to the MCMC draws.

Next we can plot the (posterior) probability distribution of the draws for any parameter. Let's plot `b_attitudepolite`. This will be the posterior probability density of the difference in HNR between the polite and informal attitude.

```{r}
#| label: hnr-bm-draws-plot

hnr_bm_draws |>
  ggplot(aes(b_attitudepol)) +
  geom_density() +
  geom_rug(alpha = 0.2)
```

## Calculate predictions based on the draws

The question we ended up with in [Introduction to regression models (Part II)](intro-regression-categorical.qmd) was: what about the posterior probability of the predicted HNR *when the attitude is polite?*.

So far, we have seen the predicted HNR when attitude is informal and the mean *difference* between polite and informal.

To calculate the predicted HNR when attitude is polite, we should look back at the equation of $\mu$ from the model mathematical formulae.

$$
\begin{align}
\mu        & = \beta_0 + \beta_1 \cdot \text{is\_polite} \\
\end{align}
$$

When `is_polite` is 0, attitude is informal and we have $\mu = \beta_0$. When `is_polite` is 1, attitude is polite and we have $\mu = \beta_0 + \beta_1$.

So, to get the predicted HNR when attitude is polite, we just need to sum `b_Intercept` ($\beta_0$) and `b_attitudepolite` ($\beta_1$).

```{r}
#| label: hnr-bm-draws-pred

hnr_bm_draws_pred <- hnr_bm_draws |> 
  mutate(
    # predicted HNR for informal attitude
    informal = b_Intercept,
    # predicted HNR for polite attitude
    polite = b_Intercept + b_attitudepol
  )

hnr_bm_draws_pred
```

The sum operation is applied row-wise, to each draw. So, for `polite`, the value of `b_Intercept` at draw 1 is added to the value of `b_attitudepolite` at draw 1, and so on. You end up with a list of sums that has the same length as the initial draws (here, 4000, i.e. 1000 per chain!). Then you can summarise and plot this new list as you did with the `b_` coefficients.

To make that easier, let's pivot longer.

```{r}
#| label: hnr-bm-draws-pred-long

hnr_bm_draws_pred_long <- hnr_bm_draws_pred |> 
  select(.chain, .iteration, .draw, informal, polite) |> 
  pivot_longer(informal:polite, names_to = "attitude", values_to = "pred")

hnr_bm_draws_pred_long
```

## Plot and summarise posterior predictions

Now let's make a violin plot of the predicted HNR in informal and polite attitude!

```{r}
#| label: hnr-bm-draws-pred-long-violin

hnr_bm_draws_pred_long |> 
  ggplot(aes(attitude, pred)) +
  geom_violin(width = 0.2) +
  labs(
    x = "Attitude", y = "Predicted HNR"
  )
```

We can use the ggdist package to plot something a bit fancier, like a half eye. Check the `?stat_halfeye` documentation to learn what those lines are.

```{r}
#| label: hnr-bm-draws-pred-long-halfeye

library(ggdist)

hnr_bm_draws_pred_long |> 
  ggplot(aes(pred, attitude)) +
  stat_halfeye() +
  labs(x = "Predicted HNR", y = "Attitude")
```

Another useful stat is `stat_interval()`.

```{r}
#| label: hnr-bm-draws-pred-long-interval

hnr_bm_draws_pred_long |> 
  ggplot(aes(attitude, pred)) +
  stat_interval() +
  labs(
    x = "Attitude", y = "Predicted HNR"
  )
```

And finally, let's summarise the predictions.

```{r}
#| label: hnr-bm-draws-pred-long-summ

hnr_bm_draws_pred_long |> 
  group_by(attitude) |> 
  summarise(
    mean = mean(pred), sd = sd(pred),
    lo_80 = quantile2(pred, 0.1), up_80 = quantile2(pred, 0.9)
  ) |> 
  # fancy way of mutating multiple columns
  mutate(
    across(mean:up_80, ~round(., 2))
  )
```

You could report the summary like so:

> According to the model, the predicted HNR when attitude is informal is between 16 and 16.48 at 80% confidence ($\beta$ = 16.27, SD = 0.16). When attitude is polite, the predicted HNR is between 17.31 and 17.73 ($\beta$ = 17.53, SD = 0.16).

## Get the difference of differences

As usual, we need to obtain the posterior draws.

```{r}
#| label: shallo-bm-draws
shallow_bm_draws <- as_draws_df(shallow_bm)
```

Now, to get the difference between constituent and non-constituent/unrelated:

1.  We must first get the mean of the non-constituent and unrelated relation types.
2.  Then we can take the difference between the exponentiated posterior draws of the constituent type and the exponentiated posterior draws of the mean non-constituent/unrelated type. The exponentiation transforms the draws from logged ms to ms and then we take the difference.
3.  Finally, we take the difference between the calculated differences in ms.

Run the code and inspect the `shallow_bm_diff` tibble to understand what is going on.

```{r}
#| label: shallow-bm-diff
shallow_bm_diff <- shallow_bm_draws |> 
  mutate(
    # 1. Mean non-constituent/unrelated for L1 and L2
    L1_NCU = mean(c(`b_Relation_typeNonConstituent:GroupL1`, `b_Relation_typeUnrelated:GroupL1`)),
    L2_NCU = mean(c(`b_Relation_typeNonConstituent:GroupL2`, `b_Relation_typeUnrelated:GroupL2`)),
    
    # 2. Difference between non-constituent/unrelated and constituent
    L1_diff = exp(L1_NCU) - exp(`b_Relation_typeConstituent:GroupL1`),
    L2_diff = exp(L2_NCU) - exp(`b_Relation_typeConstituent:GroupL2`),
    
    # 3. Difference of difference
    L2_L1_diff = L2_diff - L1_diff
  )
shallow_bm_diff |> 
  select(L1_NCU:L2_L1_diff)
```

The column `L2_L1_diff` contains the posterior draws of the difference between the constituent and non-constituent/unrelated type in the two groups. We can proceed as usual, like with any other posterior, and calculate the CrI of this difference of differences.

```{r}
#| label: shallow-bm-diff-post
library(posterior)

shallow_bm_diff |> 
  summarise(
    lo_80 = quantile2(L2_L1_diff, prob = 0.1),
    hi_80 = quantile2(L2_L1_diff, prob = 0.9),
    lo_80 = round(lo_80), hi_80 = round(hi_80),
  )
```

Here, at 80% confidence, the difference between constituent and non-constituent/unrelated types in the L2 group is 6 to 53 ms smaller than the difference in the L1 group.
