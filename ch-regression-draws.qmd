# Wrangling MCMC draws {#sec-regression-draws}

![](https://img.shields.io/badge/Area-Statistics-red) ![](https://img.shields.io/badge/Area-R-green)

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_light())
library(brms)
```

## MCMC what?

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(brms)

```

<!--# TODO: Split this chapter: take out difference of difference because for that you need interactions which we haven't done yet -->

Bayesian regression models fitted with brms/Stan use the Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate the probability distributions of the model's parameters. You have first encountered MCMC in @sec-fit-model: the text printed when running `brm()` is about the MCMC. Bayesian models reconstruct the posterior probability distribution of a set of parameters. More precisely, they reconstruct (i.e. estimate) the *joint* probability distribution of the parameters. In other words, all parameters are estimated at the same time: think of this as a multidimensional space of probability, with one dimension per paramter. With two parameters, like intercept and slope, this is a 3-dimensional space, something we can imagine: think of a landscape with hills and valleys, where the *x*-axis are values for the intercept, the *y*-axis are values for the slope and the *z*-axis (the vertical axis) are probability densities. The hills represent high probability areas and valleys represent low probability areas. This landscape of hills and valleys is determined by the two components of the Bayes' Theorem: the prior probability distribution $P(h)$ and the probability of the data given the prior, $P(d | h)$.

Constructing the landscape from the prior and data analytically (i.e. solving the mathematical equation of Bayes' Theorem) is very often very hard or even impossible. The MCMC algorithm allows us to sample points in the landscape without actually mathematically reconstruct the landscape. In the Stan software, which brms uses to fit Bayesian models, the MCMC algorithm uses a specific implementation called Hamiltonian Monte Carlo (HMC). The HMC version of MCMC simulates physical particles rolling through the posterior landscape, using equations from physical mechanics. At each iteration, the algorithm "flicks" a particle (like a pinball ball) and then stops it in its tracks: the place on the posterior landscape where the particle stops is taken as a "draw". In the 3-dimensional landscape of intercept and slope, the intercept and slope values corresponding to the spot the particle was stopped are recorded. So the draw from one iteration holds one value for the intercept and one for the slope. The algorithm proceeds for several iterations, thus creating a list of draws. These draws can be used to plot and summaries the posterior distributions of the parameters. The draws are called **posterior draws**, because they come from the posterior probability distribution.

This is a bit abstract and if you want to learn more about MCMC, I recommend @mcelreath2020, Ch 9 and @nicenboim2025, Ch 3. Note that to be a proficient user of brms and Bayesian regression models, you don't need to fully understand the mathematics behind MCMC algorithms, as long as you understand them conceptually.

When you run a model with brms, the draws (i.e. the sampled values) are stored in the model object. All operations on a model, like obtaining the `summary()`, are actually operations on those draws. We normally fit regression models with four MCMC chains. The sampling algorithm within each chain runs for 2000 iterations by default. The first half (1000 iterations) are used to "warm up" the algorithm (i.e. tune certain parameters related to the mechanics equations that make the particles move) while the second half (1000 iterations) are the ones included in the actual posterior draws. Since four chains run with 2000 iterations of which 1000 are kept as posterior draws, we end up with 4000 draws we can use to learn details about the posterior. The rest of this chapter will teach you how to extract and manipulate the model's draws. We will do so by revisiting the model fitted in @sec-regression.

## Reproducible model fit

Before we move to the model, it is worth making a couple practical considerations. Fitting simple models with brms is relatively quick. However, more complex model using larger data sets can take some time for the MCMC to efficiently sample the posterior distribution (sometimes even hours!). It is useful to save the model fit to a file so that, once the model is fit once, you don't have to fit it again. This can be done by specifying a file path in the `file` argument in the `brm()` function. I suggest developing the habit of having a dedicated `cache/` in your Quarto project to save all of the brms model objects in. Go ahead and create a `cache/` folder in you project. Then, in your `week-05.qmd` document, rewrite the model from the previous chapter like so:

```{r}
#| label: ita-egg
#| include: false

load("data/coretta2018a/ita_egg.rda")
```

```{r}
#| label: vow-bm-file

vow_bm <- brm(
  # `1 +` can be omitted.
  v1_duration ~ speech_rate,
  family = gaussian,
  data = ita_egg_clean,
  cores = 4,
  seed = 20912,
  file = "cache/vow_bm"
)
```

The `file` argument tells brms to save the model output to the `cache/` folder in a file called `vow_bm.rds`. The extension `.rds` is appended automatically (this is the same file type you encountered where reading data, like `glot_status.rds`). What about `cores` and `seed`? When the model is fit, four MCMC chains are run. By default, these are run sequentially: the first chain, then the second, then the third and finally the fourth. But we can speed things up a bit by running them in parallel on separate computer cores. Virtually all computers today have at least four cores, so we can run the four chains using four cores. This is what `cores = 4` does: it tells brms to run each chain on one core so they are run in parallel. Since the MCMC algorithm contains a random component (physical particles are randomly flicked across the landscape), every time you refit the model, a different set of draws are drawn (because the particles stop at random places in the landscape). One way to make the model reproducible (meaning, obtaining exactly the same draws every time) is to set a "seed". In computing, a seed is a number used for random number generation: when set, the same list of "random" numbers is produced. The MCMC algorithm uses random number generation to run itself, so by setting the seed we are in fact "fixing" the randomness of the algorithm. The seed number can be any number: here I set it to `20912`.

Now run the model. The model will be fitted and the model object will be saved in `cache/` with the file name `vow_bm.rds`. If you now re-run the same code again, you will notice that `brm()` does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works!). This saves time: you fit the model once but you can read the output multiple times. This is also good for reproducibility: an independent researcher with access to your code and the cache folder can run your code and get exactly the same results as yours.

::: callout-important
When you save the model fit to a file, R does not keep track of changes in the model specification (like changes in formula or data and so on), so if you make changes to model, you need to **delete the saved model file** before re-running the code for the changes to have effect!
:::

## Extract MCMC posterior draws

There are different ways to extract the MCMC posterior draws from a fitted model. In this book, we will use the `as_draws_df()` function from the [posterior](https://mc-stan.org/posterior/) package. The function extracts the draws from a Bayesian regression model and outputs them as a data frame. Let's take the `rt_bm_1` model from @sec-regression-cat and extract the draws. Let's first read the `tucker2019/mald_1_1.rds` data again.

```{r}
#| label: mald

library(tidyverse)

mald <- readRDS("data/tucker2019/mald_1_1.rds")
```

Now we reload the Bayesian regression model from @sec-regression-cat. We simply use the same code we used in that chapter. If you run the code now, the file with the model will be loaded into R, so the model will not be fitted from scratch. This is both convenient and it ensure the code is reproducible.

```{r}
#| label: rt-bm-1

library(brms)

rt_bm_1 <- brm(
  RT ~ IsWord,
  family = gaussian,
  data = mald,
  seed = 6725,
  file = "cache/ch-regression-cat-rt_bm_1"
)
```

Let's review the model summary.

```{r}
#| label: rt-bm-1-summ

summary(rt_bm_1)
```

The `Regression Coefficients` table includes `Intercept` and `IsWordFALSE` which is what we expect from the model formula and data. We are good to go! Now we can extract the MCMC draws from the model using the `as_draws_df()` function. This function from the posterior package returns a tibble with values obtained in each draw of the MCMC algorithm. Since we fitted the model with 4 MCMC chains and 1000 sampling draws per chain, there is a total of 4000 drawn values (i.e. 4000 rows in the data frame).

```{r}
#| label: rt-bm-1-draws

rt_bm_1_draws <- as_draws_df(rt_bm_1)
rt_bm_1_draws
```

Don't worry about the `Intercept`, `lprior` and `lp__` columns. Open the data frame in the RStudio viewer. You will see three extra column: `.chain`, `.iteration` and `.draw`. They indicate:

-   `.chain`: The MCMC chain number (1 to 4).
-   `.iteration`: The iteration number within chain (1 to 1000).
-   `.draw`: The draw number across all chains (1 to 4000).

Make sure that you understand these columns in light of the MCMC algorithm. The following columns contain the drawn values at each draw for three parameters of the model: `b_Intercept`, `b_IsWordFALSE` and `sigma`. To remind yourself what these mean, let's have a look at the mathematical formula of the model.

$$
\begin{align}
RT_i & \sim Gaussian(\mu_i, \sigma) \\
\mu_i        & = \beta_0 + \beta_1 \cdot w_i \\
\end{align}
$$

So:

-   `b_Intercept` is $\beta_0$. This is the mean RT when the the word is a real word.
-   `b_IsWordFALSE` is $\beta_1$. This is the *difference* in RT between nonce words and real words.
-   `sigma` is $\sigma$. This is the overall standard deviation of the RTs.

Any inference made on the basis of the model are inferences derived from the draws. One could say that the model "results" are, to put it simply, these draws and that the draws can be used to make inferences about the population one is investigating (if one is interested in inference, in the first place).

## Summary measures of the posterior draws

The `Regression Coefficients` table from the `summary()` of the model reports summary measures calculated from the drawn values of `b_Intercept` and `b_IsWordFALSE`. These summary measures are the mean (`Estimate`), the standard deviation (`Est.error`) and the lower and upper limits of the 95% Credible Interval (CrI). Since we have now the draws, we can of course obtain those same measures ourselves from the data frame with the draws! We will use the `quantile2()` function from the posterior package to obtain 95% and 80% CrIs.

```{r}
#| label: rt-bm-1-draws-summ

library(posterior)

rt_b1_summaries <- rt_bm_1_draws |> 
  summarise(
    intercept_mean = mean(b_Intercept), intercept_sd = sd(b_Intercept),
    intercept_lo_95 = quantile2(b_Intercept, 0.025), intercept_up_95 = quantile2(b_Intercept, 0.975),
    IsWordFALSE_mean = mean(b_IsWordFALSE), IsWordFALSE_sd = sd(b_IsWordFALSE),
    IsWordFALSE_lo_95 = quantile2(b_IsWordFALSE, 0.025), IsWordFALSE_up_95 = quantile2(b_IsWordFALSE, 0.975),
  ) |> 
  # fancy way of mutating multiple columns
  mutate(
    across(everything(), ~round(., 2))
  )
rt_b1_summaries
```

Compare the values obtained now with the values in the model summary above. They are the same, because the summary measures in the model summary are simply summary measures of the draws. Hopefully now it is clear where the values in the `Regression Coefficient` table come from and how these are related to the MCMC draws.

## Plotting posterior draws

Plotting posterior draws is as straightforward as plotting any data. You already have all of the tools to understand plotting draws with ggplot2. To plot the reconstructed posterior probability distribution of any parameter, we plot the probability density (with `geom_density()`) of the draws of that parameter. Let's plot `b_IsWordFALSE`. This will be the posterior probability density of the difference in RTs between nonce (`IsWord` is `FALSE`) and real words (`IsWord` is `TRUE`).

```{r}
#| label: rt-bm-1-draws-plot

rt_bm_1_draws |>
  ggplot(aes(b_IsWordFALSE)) +
  geom_density() +
  geom_rug(alpha = 0.2)
```
