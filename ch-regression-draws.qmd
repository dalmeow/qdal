# Wrangling MCMC draws {#sec-regression-draws}

![](https://img.shields.io/badge/Area-Statistics-red) ![](https://img.shields.io/badge/Area-R-green)

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_light())
library(brms)
```

## MCMC what?

<!--# Use mald reaction times from chapter 23 -->

<!--# Split this chapter: take out difference of difference because for that you need interactions which we haven't done yet -->

Bayesian regression models fitted with brms/Stan use the Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate the probability distributions of the model's coefficient. You have first encountered MCMC in @ch-fit-model. The MCMC algorithm does two things: first it finds a probability area that is the most compatible with the prior probabilities and the probability of the data given the priors; then, it sample values (called *draws*) from this high-probability area to reconstruct the posterior distribution. Since the high-probability area is assumed to be representative of the posterior distribution (that's why we sample from it), the MCMC draws are also called *posterior draws*. Elizabeth Pankratz has written a nice short introduction to MCMC algorithms here: [Finding posteriors through sampling](https://elizabethpankratz.github.io/bayes_stat/day1/mcmc.html). I strongly recommend that you read that before proceeding (if you want to further your understanding of MCMC, the resources linked on that page are an excellent starting point). Note that to be a proficient user of brms and Bayesian regression models, you don't need to fully understand the mathematics behind MCMC algorithms, as long as you understand them conceptually.

When you run a model with brms, the draws (i.e. the sampled values) are stored in the model object. All operations on a model, like obtaining the `summary()`, are actually operations on those draws. We normally fit regression models with four MCMC chains. The sampling algorithm within each chain runs for 2000 iterations by default. The first half (1000 iterations) are used to "warm up" the algorithm (i.e. find the high-probability area) while the second half (1000 iterations) are the ones that sample from the high-probability area to reconstruct the posterior distribution. For four chains ran with 2000 iterations of which 1000 for warm up, we end up with 4000 iterations we can use to learn details about the posterior.

The rest of the post will teach you how to extract and manipulate the model's draws. We will first revisit the model fitted in @sec-regression-cat.

## Extract MCMC posterior draws

There are different ways to extract the MCMC posterior draws from a fitted model. In this book, we will use the `as_draws_df()` function from the posterior package. The function extracts the draws from a Bayesian regression model and outputs them as a data frame. Let's take the `rt_bm_1` model from @sec-regression-cat and extract the draws. Let's first read the `tucker2019/mald_1_1.rds` data again.

```{r}
#| label: mald

library(tidyverse)

mald <- readRDS("data/tucker2019/mald_1_1.rds")
```

Now we reload the Bayesian regression model from @sec-regression-cat. We simply use the same code we used in that chapter. If you run the code now, the file with the model will be loaded into R, so the model will not be fitted from scratch. This is both convenient and it ensure the code is reproducible.

```{r}
#| label: rt-bm-1

library(brms)

rt_bm_1 <- brm(
  RT ~ IsWord,
  family = gaussian,
  data = mald,
  seed = 6725,
  file = "cache/ch-regression-cat-rt_bm_1"
)
```

Let's review the model summary.

```{r}
#| label: rt-bm-1-summ

summary(rt_bm_1)
```

The `Regression Coefficients` table includes `Intercept` and `IsWordFALSE` which is what we expect from the model formula and data. We are good to go! Now we can extract the MCMC draws from the model using the `as_draws_df()` function. This function from the posterior package returns a tibble with values obtained in each draw of the MCMC algorithm. Since we fitted the model with 4 MCMC chains and 1000 sampling draws per chain, there is a total of 4000 drawn values (i.e. 4000 rows in the data frame).

```{r}
#| label: rt-bm-1-draws

rt_bm_1_draws <- as_draws_df(rt_bm_1)
rt_bm_1_draws
```

Don't worry about the `Intercept`, `lprior` and `lp__` columns. Open the data frame in the RStudio viewer. You will see three extra column: `.chain`, `.iteration` and `.draw`. They indicate:

-   `.chain`: The MCMC chain number (1 to 4).
-   `.iteration`: The iteration number within chain (1 to 1000).
-   `.draw`: The draw number across all chains (1 to 4000).

Make sure that you understand these columns in light of the MCMC algorithm. The following columns contain the drawn values at each draw for three parameters of the model: `b_Intercept`, `b_IsWordFALSE` and `sigma`. To remind yourself what these mean, let's have a look at the mathematical formula of the model.

$$
\begin{align}
RT_i & \sim Gaussian(\mu_i, \sigma) \\
\mu_i        & = \beta_0 + \beta_1 \cdot w_i \\
\end{align}
$$

So:

-   `b_Intercept` is $\beta_0$. This is the mean RT when the the word is a real word.
-   `b_IsWordFALSE` is $\beta_1$. This is the *difference* in RT between nonce words and real words.
-   `sigma` is $\sigma$. This is the overall standard deviation of the RTs.

Any inference made on the basis of the model are inferences derived from the draws. One could say that the model "results" are, to put it simply, these draws and that the draws can be used to make inferences about the population one is investigating (if one is interested in inference, in the first place).

## Summary measures of the posterior draws

The `Regression Coefficients` table from the `summary()` of the model reports summary measures calculated from the drawn values of `b_Intercept` and `b_IsWordFALSE`. These summary measures are the mean (`Estimate`), the standard deviation (`Est.error`) and the lower and upper limits of the 95% Credible Interval (CrI). Since we have now the draws, we can of course obtain those same measures ourselves from the data frame with the draws! We will use the `quantile2()` function from the posterior package to obtain 95% and 80% CrIs.

```{r}
#| label: rt-bm-1-draws-summ

library(posterior)

rt_b1_summaries <- rt_bm_1_draws |> 
  summarise(
    intercept_mean = mean(b_Intercept), intercept_sd = sd(b_Intercept),
    intercept_lo_95 = quantile2(b_Intercept, 0.025), intercept_up_95 = quantile2(b_Intercept, 0.975),
    IsWordFALSE_mean = mean(b_IsWordFALSE), IsWordFALSE_sd = sd(b_IsWordFALSE),
    IsWordFALSE_lo_95 = quantile2(b_IsWordFALSE, 0.025), IsWordFALSE_up_95 = quantile2(b_IsWordFALSE, 0.975),
  ) |> 
  # fancy way of mutating multiple columns
  mutate(
    across(everything(), ~round(., 2))
  )
rt_b1_summaries
```

Compare the values obtained now with the values in the model summary above. They are the same, because the summary measures in the model summary are simply summary measures of the draws. Hopefully now it is clear where the values in the `Regression Coefficient` table come from and how these are related to the MCMC draws.

## Plotting posterior draws

Plotting posterior draws is as straightforward as plotting any data. You already have all of the tools to understand plotting draws with ggplo2. To plot the reconstructed posterior probability distribution os any parameter, we plot the probabilty density (with `geom_density()`) of the draws of that parameter. Let's plot `b_IsWordFALSE`. This will be the posterior probability density of the difference in RTs between nonce (`IsWord` is `FALSE`) and real words (`IsWord` is `TRUE`).

```{r}
#| label: rt-bm-1-draws-plot

rt_bm_1_draws |>
  ggplot(aes(b_IsWordFALSE)) +
  geom_density() +
  geom_rug(alpha = 0.2)
```

## Calculate posterior predictions from the draws

The question we ended up with in [Introduction to regression models (Part II)](intro-regression-categorical.qmd) was: what about the posterior probability of the predicted HNR *when the attitude is polite?*.

So far, we have seen the predicted HNR when attitude is informal and the mean *difference* between polite and informal.

To calculate the predicted HNR when attitude is polite, we should look back at the equation of $\mu$ from the model mathematical formulae.

$$
\begin{align}
\mu        & = \beta_0 + \beta_1 \cdot \text{is\_polite} \\
\end{align}
$$

When `is_polite` is 0, attitude is informal and we have $\mu = \beta_0$. When `is_polite` is 1, attitude is polite and we have $\mu = \beta_0 + \beta_1$.

So, to get the predicted HNR when attitude is polite, we just need to sum `b_Intercept` ($\beta_0$) and `b_attitudepolite` ($\beta_1$).

```{r}
#| label: hnr-bm-draws-pred

hnr_bm_draws_pred <- hnr_bm_draws |> 
  mutate(
    # predicted HNR for informal attitude
    informal = b_Intercept,
    # predicted HNR for polite attitude
    polite = b_Intercept + b_attitudepol
  )

hnr_bm_draws_pred
```

The sum operation is applied row-wise, to each draw. So, for `polite`, the value of `b_Intercept` at draw 1 is added to the value of `b_attitudepolite` at draw 1, and so on. You end up with a list of sums that has the same length as the initial draws (here, 4000, i.e. 1000 per chain!). Then you can summarise and plot this new list as you did with the `b_` coefficients.

To make that easier, let's pivot longer.

```{r}
#| label: hnr-bm-draws-pred-long

hnr_bm_draws_pred_long <- hnr_bm_draws_pred |> 
  select(.chain, .iteration, .draw, informal, polite) |> 
  pivot_longer(informal:polite, names_to = "attitude", values_to = "pred")

hnr_bm_draws_pred_long
```

## Plot and summarise posterior predictions

Now let's make a violin plot of the predicted HNR in informal and polite attitude!

```{r}
#| label: hnr-bm-draws-pred-long-violin

hnr_bm_draws_pred_long |> 
  ggplot(aes(attitude, pred)) +
  geom_violin(width = 0.2) +
  labs(
    x = "Attitude", y = "Predicted HNR"
  )
```

We can use the ggdist package to plot something a bit fancier, like a half eye. Check the `?stat_halfeye` documentation to learn what those lines are.

```{r}
#| label: hnr-bm-draws-pred-long-halfeye

library(ggdist)

hnr_bm_draws_pred_long |> 
  ggplot(aes(pred, attitude)) +
  stat_halfeye() +
  labs(x = "Predicted HNR", y = "Attitude")
```

Another useful stat is `stat_interval()`.

```{r}
#| label: hnr-bm-draws-pred-long-interval

hnr_bm_draws_pred_long |> 
  ggplot(aes(attitude, pred)) +
  stat_interval() +
  labs(
    x = "Attitude", y = "Predicted HNR"
  )
```

And finally, let's summarise the predictions.

```{r}
#| label: hnr-bm-draws-pred-long-summ

hnr_bm_draws_pred_long |> 
  group_by(attitude) |> 
  summarise(
    mean = mean(pred), sd = sd(pred),
    lo_80 = quantile2(pred, 0.1), up_80 = quantile2(pred, 0.9)
  ) |> 
  # fancy way of mutating multiple columns
  mutate(
    across(mean:up_80, ~round(., 2))
  )
```

You could report the summary like so:

> According to the model, the predicted HNR when attitude is informal is between 16 and 16.48 at 80% confidence ($\beta$ = 16.27, SD = 0.16). When attitude is polite, the predicted HNR is between 17.31 and 17.73 ($\beta$ = 17.53, SD = 0.16).
