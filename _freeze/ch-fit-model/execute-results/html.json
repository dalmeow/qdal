{
  "hash": "ec5630c8ab069626968ddbd8560098e1",
  "result": {
    "engine": "knitr",
    "markdown": "# Fitting Gaussian models with brms {#sec-fit-model}\n\n![](https://img.shields.io/badge/Area-Statistics-red) ![](https://img.shields.io/badge/Area-R-green)\n\n\n\nIn the previous chapter, I have introduced the theory behind Bayesian Gaussian models. In this chapter, you will learn how to fit Gaussian models in R. You can fit a Gaussian model to data in R using the [brms](https://paulbuerkner.com/brms/) package (the name is an initialism of \"Bayesian Regression Models using Stan\"; Gaussian models are a special type of regression models, which will be introduced in @sec-reg-intro).\n\nThe brms package can run a variety of Bayesian (regression) models. It is a very flexible package that allows you to model a lot of different types of variables. You don't really need to understand all of the technical details to be able to effectively use the package and interpret the results, so this textbook will focus on how to use the package in the context of research. We will cover some of the technicalities, but if you are are particularly interested in the inner workings of the package, feel free to find materials on specific aspects by searching online. One useful thing to know is that brms is a bridge between R and the statistical programming software [Stan](https://mc-stan.org). Stan is a powerful piece of software that can run any type of Bayesian model, not just regressions. What brms does is that it allows you to write Bayesian models in R, which are translated into Stan models and run with Stan under the hood. You can safely use brms without learning Stan, but if you are interested, you can check [Ch 8-10](https://bruno.nicenboim.me/bayescogsci/ch-introstan.html#ch-introstan) of @nicenboim2025 and the [Stan documentation](https://mc-stan.org).\n\nYou can run a Bayesian Gaussian model with the `brm()` function, short for \"Bayesian Regression Model\" (a Gaussian model is a special type of regression model). The mandatory arguments of `brm()` are a model formula, a distribution family (of the outcome variable), and the data you want to run the model with. Running a model with data is also formally known as *fitting the model to the data*. We want to fit a Gaussian model to reaction times from @tucker2019. Let's revisit the mathematical formula of the model from @sec-gauss-model (let's discard the priors; see the R Note box below):\n\n$$\nRT \\sim Gaussian(\\mu, \\sigma)\n$$\n\nIt would be nice if `brms()` allowed you to write the formula like that.\n\n``` r\n# This would be nice, but it won't work!\nbrm(\n  RT ~ Gaussian(mu, sigma),\n  data = mald\n)\n```\n\nAlas, due to historical and technical reasons of how other R packages write model formulae, you need to use a special way of specifying the model. As mentioned, you need three arguments: a model formula, the distribution family of the outcome, and the data. So the mathematical formula is split in two parts (corresponding to two arguments of the `brm()` function): `formula` and `family`.\n\n``` r\nbrm(\n  formula = RT ~ 1,\n  family = gaussian,\n  data = mald\n)\n```\n\n-   `RT ~ 1` and `family = gaussian` simply tell brms to model RT using a Gaussian distribution. This means that the probability distribution of the mean and the standard deviation of the Gaussian distribution of RTs will be estimated from the data. `RT ~ 1` might look very weird to you right now, but it will become clear in the next couple of chapters why it is that way. For now, just accept that that is the way you write a Gaussian model in R.\n-   We specify the data with `data = height`.\n\nAs with other R functions, we want to assign the output of `brm()` to a variable, here `rt_bm`. We will then be able to inspect the output in `rt_bm`. Now run the Gaussian model of RTs (don't forget to attach the brms package and read the data, like in the following code).\n\n``` r\nlibrary(brms)\n\nmald <- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nrt_bm <- brm(\n  RT ~ 1,\n  family = gaussian,\n  data = mald\n)\n```\n\nWhen you run the code, some text will be printed below the code in your Quarto document. This is what it looks like.\n\n``` r\nCompiling Stan program...\nStart sampling\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000156 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.56 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n\n<more text omitted>\n```\n\nThe messages in the text are related to Stan and the statistical algorithm used by Stan to estimate the parameters of the model (in this model, these are the mean and the standard deviation of RTs). `Compiling Stan program...` tells you that brms has instructed Stan to compile the model specified in R and that Stan is now compiling the model to be run on the data (don't worry if this does not make sense). `Start sampling` tells us that the statistical algorithm used for estimation has started. This algorithm is the Markov Chain Monte Carlo algorithm, or MCMC for short. The algorithm is run by default four times; in technical terms, *four MCMC chains* are run. This is why information on Chain 1, 2, 3, and 4 is printed. We will treat MCMC in more details in @sec-regression-draws.\n\n\n\nNow that the model has finished running and that the output has been saved in `rt_bm`, we can inspect it with the `summary()` function (note that this is different from `summarise()`!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rt_bm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 1 \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept  1010.50      4.45  1001.66  1019.26 1.00     3628     2476\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   317.88      3.17   311.74   324.17 1.00     4064     2571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nLet's break down the summary bit by bit:\n\n-   The first few lines are a reminder of the model we fitted:\n\n    -   `Family` is the chosen distribution for the outcome variable (`RT` in our model), here a Gaussian distribution.\n\n    -   `Links` lists the link functions. You don't need to worry about these now.\n\n    -   `Formula` is the model formula.\n\n    -   `Data` reports the name of the data and the number of observations.\n\n    -   Finally, `Draws` has information about the MCMC algorithm. Again, you can discard those for now.\n\n-   Then, the `Regression Coefficients` are listed as a table. They are called regression coefficients because brms fits Bayesian *regression* models and a Gaussian model is a special type of regression model (you will learn about regression models in later chapters). In the Gaussian model we fitted now, we only have one regression coefficient, `Intercept`, which corresponds to $\\mu$ from the formula above (for the reason why it is called that way, you will have to wait until regression models are introduced in the following chapter, I apologise for the many promissory explanations...). You will learn how to interpret the `Regression Coefficients` table below.\n\n-   Then `Further distributional parameters` are also in the form of a table. Here we only have `sigma` which is $\\sigma$ from the formula above. The table has the same columns as the `Regression Coefficients` table.\n\n## Posterior probability distributions\n\nThe main characteristic of Bayesian models is that they don't just provide you with a single numeric estimate for the model parameters. As mentioned in @sec-gauss-model, the model estimates a *full probability distribution* for each parameter/coefficient. These probability distributions are called **posterior probability distributions** (or posteriors for short). They are called *posterior* because they are derived from the data and the prior probability distributions. The model we fitted has two parameters: the mean $\\mu$ and the standard deviation $\\sigma$. For reasons that will become clear in @sec-reg-intro, the $\\mu$ parameter is called `Intercept` in the summary: you can find it in the `Regression Coefficients` table. The standard deviation $\\sigma$ is in the `Further distributional parameters` table.\n\n::: callout-tip\n### Posterior probability distribution\n\nA **posterior probability distribution** is a probability distribution of a model parameter estimated by a Bayesian model from the prior probability distribution and the data.\n:::\n\nThe `Regression Coefficients` table reports, for each estimated coefficient, a few summary measures of the posterior distributions of the estimated coefficients. Here, we only have the summary measures of one posterior: the posterior of the model's mean $\\mu$. The table also has three diagnostic measures, which you can ignore for now.\n\nHere's a breakdown of the table's columns:\n\n-   `Estimate`, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient). In our model, this is the mean of the posterior distribution of the mean $\\mu$ (`Intercept`). Yes, you read correctly: the mean of the mean! Remember, Bayesian models estimate a full posterior probability distribution and the posterior can be summarised by a mean and a standard deviation. In this model, the posterior mean (short for mean of the posterior probability distribution) of $\\mu$ is 1010.5 ms.\n\n-   `Est.error`, the error of the mean estimate, or *est*imate *error*. The estimate error is the standard deviation of the posterior distribution of the coefficient (here the mean $\\mu$). Yes, the standard deviation of the mean! Again, since we are estimating a full probability distribution for $\\mu$, we can summarise it with a mean and SD as we do for any Gaussian distribution. In this model, the posterior SD of $\\mu$ is 4.45 ms. Be careful: this SD is *not* $\\sigma$. It is the standard deviation of the posterior distribution of the mean $\\mu$.\n\n-   `l-95% CI` and `u-95% CI`, the lower and upper limits of the 95% Bayesian Credible Interval (more on these below).\n\n-   Finally, `Rhat`, `Bulk_ESS`, `Tail_ESS` are diagnostics of the MCMC chains, which you can ignore for now.\n\nThe `Further distributional parameters` table has the same structure. In this model, the posterior mean of $\\sigma$ is 317.88 ms and the posterior SD of $\\sigma$ is 3.17 ms.\n\nPutting all this together in mathematical notation:\n\n$$\n\\begin{align}\nRT & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & \\sim P(1010.5, 4.45)\\\\\n\\sigma & \\sim P(317.88, 3.17)\n\\end{align}\n$$\n\nIn other words, according to the model and data, the mean or RTs is a value from the distribution $P(1010.5, 4.45)$ and the SD of RTs is a value from the distribution $P(317.88, 3.17)$. Here, $P()$ stands for a generic posterior probability distribution. The model has quantified the uncertainty around the value of the $\\mu$ and $\\sigma$ parameters and this uncertainty is reflected by the fact that we get a full posterior probability distribution (summarised by its mean and SD) for each of the parameters. We don't know *exactly* the values of the parameters of the Gaussian distribution assumed to have generated the sampled RTs.\n\n::: {.callout-important collapse=\"true\"}\n### R Note: Default priors in brms\n\nOne major aspect of Bayesian modelling is the combination of prior knowledge with evidence from the observed data. As introduced in @sec-gauss-model, choosing priors is an important step in conducting Bayesian analyses. However, in this textbook we will not deal with prior specification given the quite tight schedule of the course.\n\nIf prior specification is a necessary step, how come we are not doing that? When fitting Bayesian models with brms, you are in fact using brms **default priors**. These are generic priors that work in most circumstances and they are equivalent to prior probability distributions that are almost flat (like in the first prior of @fig-prior-update). In other words, they are so generic that they have very little influence on the posterior distribution, but still they help with the computational aspects of model fitting (which you will learn more about in @sec-regression-draws).\n\nIf you want to inspect brms default priors, you can use the `get_prior()` function. Let's do this for the model we fitted above. The function requires the model formula, the family and the data, much like the `brm()` function. It returns a data frame with one prior per row. The columns of interest are `prior` and `class`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  RT ~ 1,\n  family = gaussian,\n  data = mald\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"prior\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"class\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"coef\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"group\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"resp\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dpar\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"nlpar\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"lb\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ub\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"source\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"student_t(3, 935.5, 220.2)\",\"2\":\"Intercept\",\"3\":\"\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"student_t(3, 0, 220.2)\",\"2\":\"sigma\",\"3\":\"\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"0\",\"9\":\"\",\"10\":\"default\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThere are two priors, one for the intercept ($\\mu$) and one for $\\sigma$ (see `class` column). For both $\\mu$ and $\\sigma$, brms sets a Student-*t* prior probability distribution. The Student-*t* distribution is a continuous probability distribution with three parameters: the degrees of freedom, the mean and the standard deviation. You will encounter the Student-*t* distribution in @sec-null-ritual, where you will learn about frequentist *p*-values. For now, it will suffice to say that a Student-*t* distribution (also simply called a *t*-distribution) is similar to a Gaussian distribution (hence the two parameters, mean and SD).\n\nNormally, you would choose priors *before* collecting/seeing the data. Here, brms actually uses the data to come up with generic priors that cover a wide range of values without including very unlikely values. Yet, the priors set by brms are so generic that, as said above, they bear very little effect on the posterior. So they are safe to use, even if they are based on the data itself. Just remember, thought, that if you do want to specify your own priors, you must do so independent of the data (ideally, even before you have access to the data, whether you collect it yourself or whether it is pre-existing).\n\nFor this model and data, brms sets a *t-*distribution for the prior of the mean $\\mu$, with mean 935.5 and SD 220.2, and a *t*-distribution for the SD $\\sigma$, with mean 0 and SD 220.2. You will notice that the SDs of both priors are the same. The following figures illustrate the density curve of the priors.\n\n\n::: {#fig-default-priors .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nx_mu <- seq(0, 2000)\n\nggplot() +\n  aes(x = x_mu, y = extraDistr::dlst(x_mu, 3, 935.5, 220.2)) +\n  geom_path(linewidth = 1) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n\nx_sigma <- seq(0, 1500)\n\nggplot() +\n  aes(x = x_sigma, y = extraDistr::dlst(x_sigma, 3, 0, 220.2)) +\n  geom_path(linewidth = 1) +\n  labs(\n    x = element_blank(), y = \"Density\"\n  )\n```\n\n::: {.cell-output-display}\n![Prior probability distribution for the mean.](ch-fit-model_files/figure-html/fig-default-priors-1.png){#fig-default-priors-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Prior probability distribution for the SD.](ch-fit-model_files/figure-html/fig-default-priors-2.png){#fig-default-priors-2 width=672}\n:::\n\nDefault brms priors for this chapter's model and data.\n:::\n\n:::\n\n## Plotting the posterior distributions\n\nWhile the model summary reports *summaries* of the posterior distributions, it is always helpful to *plot* the posteriors. We can easily do so with the base R `plot()` function, like in @fig-rt-bm. The density plots of the posteriors distributions of the two parameters estimated in the model are shown on the left of the figure: `b_Intercept` which corresponds to `Intercept` from the summary and `sigma` (the reason for why it's `b_Intercept` will become clear in @sec-reg-intro).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rt_bm, combo = c(\"dens\", \"trace\"))\n```\n\n::: {.cell-output-display}\n![Posterior plots of the rt_bm Gaussian model](ch-fit-model_files/figure-html/fig-rt-bm-1.png){#fig-rt-bm width=672}\n:::\n:::\n\n\nFor the `b_Intercept` coefficient, i.e. the mean $\\mu$, the posterior probability encompasses values between 1000 and 1020 ms, approximately. But some values are more probable than others: the values in the centre of the distribution have a higher probability density then the values on the sides. The mean of that posterior probability distribution is the `Estimate` value in the model summary: 1010.5 ms. Its standard deviation is the `Est.error`: 4.45 ms. The mean indicates the value with the highest probability density, which corresponds to the value on the horizontal axis of the density plot below the highest peak of the density curve. Based on these properties, values around 1010.5 ms are more probable than values further away from it.\n\nFor `sigma`, i.e. $\\sigma$, the posterior probability covers values between 310 and 325. What is the mean of the posterior probability of $\\sigma$? The answer is in the summary, in `Further distributional parameters`. There you will also find the standard deviation of the posterior of $\\sigma$. That's a standard deviation of a standard deviation! As before, this is because we are not estimating a simple value for $\\sigma$ but a full (posterior) probability distribution and we can summarise this distribution with a mean and a standard deviation. Again, the highest peak in the distribution corresponds to the `Estimate` value.\n\nNow, looking at a full probability distribution like that is not very straightforward and summary measures can be even less straightforward. Credible Intervals (CrIs) help summarise the posterior distributions so that interpretation is more straightforward.\n\n## Interpreting Credible Intervals\n\nThe model summary reports the Bayesian Credible Intervals (CrIs) of the posterior distributions. Another way of returning summaries of the coefficients is to use the `posterior_summary()` function which returns a table (technically, a matrix, another type of R objects; we print only the first two rows with `[1:2,]` because we can ignore the other ones).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(rt_bm)[1:2,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Estimate Est.Error      Q2.5     Q97.5\nb_Intercept 1010.5015  4.452312 1001.6555 1019.2559\nsigma        317.8845  3.171714  311.7382  324.1688\n```\n\n\n:::\n:::\n\n\nA Bayesian CrI is simply the central probability interval of a posterior probability distribution. You have encountered intervals in @sec-gaussian. By default, brms prints the 95% CrIs: these are the 95% central probability intervals of the posterior probability of each coefficient. The 95% CrI of the `b_Intercept` is between 1002 and 1019. This means that there is a 95% probability, or (equivalently) that we can be 95% confident, that the `Intercept`, i.e. $\\mu$, is within that range, given the model and data. For `sigma`, i.e. $\\sigma$, the 95% CrI is between 312 and 324. So, again, there is a 95% probability that the `sigma` value is between those values, given the model and data. So, to summarise, a 95% CrI tells us that we can be 95% confident, or in other words that there is a 95% probability, that the value of the coefficient is between the values of the CrI.\n\nThere is nothing special about 95% CrI and in fact it is recommended to calculate and report a few of them. Personally, I use 90, 80, 70 and 60% CrIs. You can get any CrI with the `summary()` and the `posterior_summary()` functions, but you will also learn an alternative and more succinct way in @sec-regression. Here is how to get an 80% CrI with `summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rt_bm, prob = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 1 \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept  1010.50      4.45  1004.74  1016.14 1.00     3628     2476\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma   317.88      3.17   313.84   321.89 1.00     4064     2571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nFor `posterior_summary()`, you specify the percentiles rather than the probability level.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(rt_bm, probs = c(0.1, 0.9))[1:2,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Estimate Est.Error       Q10       Q90\nb_Intercept 1010.5015  4.452312 1004.7400 1016.1351\nsigma        317.8845  3.171714  313.8399  321.8941\n```\n\n\n:::\n:::\n\n\nHere you have the results from the regression model. Really, the results of the model are the full posterior probabilities, but it makes things easier to focus on the CrIs. If you are used to frequentist statistics (either from learning how to run them or from reading frequentist analyses in academic papers) this might seem a bit underwhelming. But this is the core thinking of Bayesian statistics: inference is based on full probability distributions for the model's parameters, rather than on a single value (like a *p*-value).\n\n## Reporting\n\n<!--# TODO: Link to chapter where they learn how to report multiple CrIs once you have written that. -->\n\nWhat about reporting the model in writing? We could report the model and the results like this (for simplicity, we report only the 95% CrIs here. You will learn how to report multiple CrIs in tables later).[^ch-fit-model-1]\n\n[^ch-fit-model-1]: To know how to add a citation for any R package, simply run `citation(\"package\")` in the R Console, where `\"package\"` is the package name between double quotes.\n\n> We fitted a Bayesian Gaussian model of reaction times (RTs) using the brms package (Bürkner 2017) in R (R Core Team 2024). The model estimates the mean and standard deviation of RTs.\n>\n> Based on the model results, there is a 95% probability that the mean is between 1002 and 1019 ms (mean = 1011, SD = 4) and that the standard deviation is between 312 and 324 ms (mean = 318, SD = 3).\n\nThe example used in this chapter is quite trivial: we are just estimating a mean and SD from the data, but in order to understand how to answer more involved questions it is fundamental that you understand what was covered in this chapter. So make sure to have grasped the basics of Gaussian models before moving onto regressions in @sec-reg-intro. On the other hand, if you will ever be asked what we can expect RTs in a auditory lexical decision task to look like you might be able to say that, based on the model and data in this chapter, we can be quite confident that they will have a mean of about 1010 ms and an SD of about 320 ms. You might think this was a lot of work to just learn about what we knew directly from the sample, and it is, but be reassured that in normal research contexts things are always much more complex than this, so it is indeed worth the effort.\n",
    "supporting": [
      "ch-fit-model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}