# Log-normal regression {#sec-regression-lognorm}

<!--# TODO: lognormal -->

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_light())
library(brms)
library(posterior)
library(bayesplot)
library(ggdist)
```

In @sec-probability, we talked about skewness of distributions in relation to the density plots of reaction times. In @sec-gauss-model, we further explained that it is the onus of the researcher to decide on a distribution family when fitting regression models and we said that, in the absence of more specific knowledge, the Gaussian distribution is a safe assumption to make. Note that the choice of distribution family should be based as much as possible on theoretical grounds (as opposed to empirical). In other words, you shouldn't plot the variable to check with distribution it might follow (more on this in the Important box below).

There are heuristics one can follow to pick a theoretically grounded distribution. The major ones are listed in @sec-outcome, so you can refer to that section in the appendix, but in this chapter we will focus on one type of variables and the default distribution choice: i.e. variables that can only take on values that are positive numbers. These variables, in the absence of more specific knowledge, can be assumed to be from a log-normal distribution.

## Log-normal distribution

The log-normal distribution is a continuous probability distribution of variables that can only be positive and not zero. It has two parameters: the mean $\mu$ and the standard deviation $\sigma$. These are the same parameters of the Gaussian distribution, but the parameters of the log-normal distribution are in logged units. The name log-normal comes from the fact that variables that are log-normal approximate a Gaussian (aka normal) distribution when we take the logarithm (log) of the values. In other words, the variable is assumed to be Gaussian on the log scale, rather than on the natural scale. Mathematically, we represent the log-normal distribution with $LogNormal(\mu, \sigma)$.

::: callout-tip
## Log-normal distribution

$$
LogNormal(\mu, \sigma)
$$

The **log-normal** distribution is a continuous probability distribution with a mean $\mu$ and standard deviation $\sigma$, measured on the log scale.

Continuous variables that can only be positive (and not zero) tend to be log-normally distributed.
:::

Typical examples of continuous variables that can only be positive and not zero are:

-   Phonetic durations (segments, words, sentences, pauses, ...).

-   Frequencies like f0 and formants. Speech rate.

-   Reaction times.

I will illustrate the nature of log-normal variables using reaction times (RT) from @tucker2019. Let's read the data and plot RTs depending on the word type. Recall we ran a Gaussian regression model of the data in @sec-regression-cat.

```{r}
#| label: mald
#| message: false

mald <- readRDS("data/tucker2019/mald_1_1.rds")
```

```{r}
#| label: fig-rt-lognorm
#| fig-cap: "Density plot of reaction times, by word type (real and nonce)."
#| code-fold: true

mald |> 
  ggplot(aes(RT, fill = IsWord)) +
  geom_density(alpha = 0.8) +
  geom_rug(alpha = 0.1) +
  scale_fill_brewer(palette = "Dark2")
```

We have already observed in @sec-regression-cat that the distribution of RTs is right-skewed: which is, there are more extreme values to the right of the distribution than what would be expected if this were a Gaussian variable. This is because RTs are naturally bounded to positive numbers only, while Gaussian variables are unbounded. A common procedure, which you will likely encounter in the literature, is to take the logarithm of RTs (and other log-normal variables), or simply to log them: the logarithm of a log-normal variable transforms the variable so that it approximates a Gaussian distribution. In R, the logarithm function is simply applied with `log()` (this uses the natural logarithm, which is the logarithm with base $e$; if you need a refresher, see [Introduction to logarithms](https://www.mathsisfun.com/algebra/logarithms.html)).

```{r}
#| label: fig-rt-gaussian-log
#| fig-cap: "Density plot of logged RTs by word type."
#| code-fold: true

mald |> 
  ggplot(aes(log(RT), fill = IsWord)) +
  geom_density(alpha = 0.8) +
  geom_rug(alpha = 0.5) +
  scale_fill_brewer(palette = "Dark2")
```

@fig-rt-gaussian-log shows the densities of logged RTs for real and nonce words. Note how the density curves are less skewed now compared to @fig-rt-lognorm. You will also note that the some lower RTs values now look more extreme than in the first figure: logging a variable compresses the scale more at higher values and spreads the scale more at lower values, which results in the reduction of right-skew, but also in making very low values look more extreme. Before moving onto discussing what to do with outliers, an important clarification is due.

::: callout-important
**Looking at a density plot is not a safe way to decide if you need to log a variable or if a variable is log-normal.**
:::

There are cases where the density plot is a combination of multiple underlying distributions with different means and SDs that makes it look as if it is skewed. For example, the following code creates a mixture of three Gaussian distributions of the same variable, but coming from three different groups, each with a slightly higher mean and SD than the first group. @fig-gauss-mix shows a single density curve for the data (@fig-gauss-mix-1), and the density plots for the individual groups (@fig-gauss-mix-2). In the single density plot, we might be given the impression that this is a log-normal variable because of the right skew, but in fact, when plotting the density curves of the individual groups we can see that the distribution in each group is quite symmetric (not skewed) and quite Gaussian-like.

```{r}
#| label: gauss-mix

set.seed(123)

# Sample sizes
n <- 2000

# Three different normal distributions
x1 <- rnorm(n, mean = 20, sd = 0.5)
x2 <- rnorm(n, mean = 21, sd = 1.5)
x3 <- rnorm(n, mean = 22, sd = 3)

dat <- tibble(
  x = c(x1, x2, x3),
  gr = rep(c("a", "b", "c"), each = n)
)
```

```{r}
#| label: fig-gauss-mix
#| fig-cap: "A Gaussian variable from three groups with different mean/SDs might look like a log-normal distribution (right-skewed)."
#| fig-subcap: 
#|   - "Single density curve."
#|   - "Separate density curves."
#| layout-ncol: 2
#| echo: false


dat |> 
  ggplot(aes(x)) +
  geom_density() +
  geom_rug()

dat |> 
  ggplot(aes(x, fill = gr)) +
  geom_density(alpha = 0.5) +
  geom_rug() +
  theme(legend.position = "none")
```

Deciding if a variable is log-normal should be based on theoretical considerations rather than on the empirical distribution. Ask yourself, before seeing the data: is this variable continuous and can it only take on positive values? If the answer is yes, then assuming a log-normal distribution is safe.

## Dealing with outliers

Very extreme values are generally called **outliers**: an outlier is a value that is more extreme than what the distribution would indicate. The definition of outlier is quite vague and there are different ways of operationalising "outlierness" (i.e. to determine if a value is an outlier). There are also different camps as to what to do with outliers, particularly in regard to inclusion/exclusion criteria. I side with the camp that suggests not to exclude outliers, if they are real outliers. In most cases, thinking about errors is a much more useful way of deciding which values to include or exclude. For example, in our RT data there is one observation of 34 ms. The second lowest RT is 200 ms. Given the task participants had to complete, a lexical decision task, it is unlikely that they could thoughfully answer after only 34 ms (that is a very short time, a vowel is usually longer than that!). So we could argue that that was an error: the participant mistakenly pressed the button before thinking about the answer.

We have a good theoretical reason to exclude that observation. We would not call this an outlier, because it is an error. You should reserve the word outlier only for extreme observations that are not the result of an error, misunderstanding or the like. It also very often depends on the specific task at hand: for certain task, an RT of 5 seconds might still be acceptable, but for others it probably means the participant was distracted. This type of observations do not represent the process one is interested in, so they are best left out. But if there are observations that are extreme, but not so extreme to believe that they come from errors or other causes, then it is theoretically more sound to keep them.

Since we have established that this very low RT observation of 34 ms must be an error, let's drop it from the data before moving on onto modelling.

```{r}
#| label: mald-filt

mald_filt <- mald |> 
  filter(RT > 34)

```

## Modelling RTs with a log-normal regression

The problems arising from assuming a Gaussian distribution for RTs is visually obvious when comparing the empirical distribution of the observed RTs with the predicted distribution from a Gaussian model. Let's reload the Gaussian model from @sec-regression-cat.

```{r}
#| label: rt-bm-1

rt_bm_1 <- brm(
  RT ~ IsWord,
  family = gaussian,
  data = mald,
  seed = 6725,
  file = "cache/ch-regression-cat-rt_bm_1"
)
```

We can plot the empirical and predicted distribution using the `pp_check()` function from the brms package. The function name stands for posterior predictive check: in other words, we are checking how the predicted joint posterior distribution of the data looks like when compared with the empirical distribution. The joint posterior probability distribution is simply the probability of the outcome variable as predicted by the posterior probability distributions of the parameters of the model. The Gaussian regression above correspond to the following mathematical equations:

$$
\begin{align}
RT_i & \sim Gaussian(\mu_i, \sigma)\\
\mu_i & = \beta_0 + \beta_1 \cdot w_i\\
\end{align}
$$

The joint posterior distribution of the outcome $RT$ is the $Gaussian(\mu, \sigma)$ distribution in the model. This is based on the posterior probability of the regression coefficients $\beta_0, \beta_1$ and the overall standard deviation $\sigma$. Remember that inference in the context of Bayesian regression models using MCMC is based on the MCMC draws. Each draw has sampled a value for the model parameters. So for each draw we can reconstruct one joint posterior distribution based on the specific parameter values of that draw. It is useful to plot the joint posterior based on several draws, but since this computation is expensive, it is usually best to use just some and not all the draws. There isn't a specific number and by default `pp_check()` uses 10 draws. In most cases these suffice. In the following code I set the number of draws to 50 just to illustrate how to use the `ndraws` argument.

@fig-rt-bm-1-pp shows the output of the `pp_check()` function. The first argument of the function is simply the model object, `rt_bm_1`. We set `ndraws = 50` to use 50 random draws from the MCMC draws to reconstruct joint posteriors. These are in light blue in the figure. The dark blue, thicker line is the empirical density of the data, the same density you would get with `geom_density()`. It is quite obvious that the reconstructed posterior densities do not match the empirical density. Values below 500 ms are over-estimated by the model (in other words, the model over-predicts the presence of lower RT values) and similarly values between 1000 and 1500 ms are over-estimated. The empirical density of the data is much more compact around the peak of the distribution, compare to the posteriors from the model.

```{r}
#| label: fig-rt-bm-1-pp
#| fig-cap: "Posterior predictive checks for a Gaussian regression model of RTs."

pp_check(rt_bm_1, ndraws = 50)
```

A common reason for the failure of the posterior probability to correctly reconstruct the empirical distribution is the incorrect choice of the family distribution (another notable reason is not including important predictors in the model, like in the three Gaussian groups from the example above: a Gaussian model of that data without group as a predictor will incorrectly estimate values). We have learned above that RT values can be assumed to be log-normal, rather than Gaussian, because they are continuous and can only be positive.

We will proceed then with modelling RTs using a log-normal regression model. This is just a regression model with a log-normal family as the distribution family for the outcome variable, here RTs. Here are the model formulae:

$$
\begin{align}
RT_i & \sim LogNormal(\mu_i, \sigma)\\
\mu_i & = \beta_0 + \beta_1 \cdot sr_i\\
\end{align}
$$

```{r}
#| label: rt-bm-2

rt_bm_2 <- brm(
  RT ~ IsWord,
  family = lognormal,
  data = mald_filt,
  cores = 4,
  seed = 6725,
  file = "cache/ch-regression-lognormal-rt_bm_2"
)
```

```{r}
#| label: rt-bm-2-pp

pp_check(rt_bm_2, ndraws = 50)
```

```{r}
#| label: rt-bm-2-cond

conditional_effects(rt_bm_2)
```

```{r}
#| label: rt-bm-2-draws

rt_bm_2_draws <- as_draws_df(rt_bm_2)

rt_bm_2_draws <- rt_bm_2_draws |>
  mutate(
    real_log = b_Intercept,
    nonce_log = b_Intercept + b_IsWordFALSE,
    real = exp(real_log),
    nonce_log = exp(nonce_log)
  )
```
